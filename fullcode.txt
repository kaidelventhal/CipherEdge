kamikaze_komodo/main.py:
<code>
# FILE: kamikaze_komodo/main.py
import asyncio
import os
import pandas as pd
from datetime import datetime, timedelta, timezone
from typing import Dict, Any, Type, Optional, Union, List

from kamikaze_komodo.app_logger import get_logger, logger as root_logger
from kamikaze_komodo.config.settings import settings

# Core components
from kamikaze_komodo.data_handling.data_handler import DataHandler
from kamikaze_komodo.backtesting_engine.optimizer import StrategyOptimizer
from kamikaze_komodo.orchestration.scheduler import TaskScheduler
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData

# Strategies for the Gauntlet
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.strategy_framework.strategies.ewmac import EWMACStrategy
from kamikaze_komodo.strategy_framework.strategies.bollinger_band_breakout_strategy import BollingerBandBreakoutStrategy
from kamikaze_komodo.strategy_framework.strategies.volatility_squeeze_breakout_strategy import VolatilitySqueezeBreakoutStrategy
from kamikaze_komodo.strategy_framework.strategies.funding_rate_strategy import FundingRateStrategy
from kamikaze_komodo.strategy_framework.strategies.ensemble_ml_strategy import EnsembleMLStrategy
from kamikaze_komodo.strategy_framework.strategies.regime_switching_strategy import RegimeSwitchingStrategy
from kamikaze_komodo.strategy_framework.strategies.bollinger_band_mean_reversion_strategy import BollingerBandMeanReversionStrategy
from kamikaze_komodo.strategy_framework.strategies.ehlers_instantaneous_trendline import EhlersInstantaneousTrendlineStrategy


# ML Models and Training Pipelines
from kamikaze_komodo.ml_models.training_pipelines.lightgbm_pipeline import LightGBMTrainingPipeline
from kamikaze_komodo.ml_models.training_pipelines.xgboost_classifier_pipeline import XGBoostClassifierTrainingPipeline
from kamikaze_komodo.ml_models.training_pipelines.lstm_pipeline import LSTMTrainingPipeline
from kamikaze_komodo.ml_models.training_pipelines.kmeans_regime_pipeline import KMeansRegimeTrainingPipeline
from kamikaze_komodo.ml_models.regime_detection.kmeans_regime_model import KMeansRegimeModel
from kamikaze_komodo.config.settings import PROJECT_ROOT


logger = get_logger(__name__)

# Define a simple concrete class for the HOLD action in the Regime Switcher
class HoldStrategy(BaseStrategy):
    """A simple strategy that always returns a HOLD signal."""
    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        # No indicators needed for this strategy
        return data

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        return SignalType.HOLD


async def train_all_models_if_needed():
    """
    Checks for the existence of required ML models and trains them if they are missing.
    """
    root_logger.info("Checking for pre-trained ML models...")
    if not settings:
        root_logger.critical("Settings not loaded, cannot check or train models.")
        return

    symbol = settings.default_symbol
    timeframe = settings.default_timeframe

    models_to_train = {
        "LightGBM": {
            "pipeline": LightGBMTrainingPipeline,
            "config_section": "LightGBM_Forecaster",
            "default_filename": f"lgbm_{symbol.replace('/', '_').lower()}_{timeframe}.joblib"
        },
        "XGBoost": {
            "pipeline": XGBoostClassifierTrainingPipeline,
            "config_section": "XGBoost_Classifier_Forecaster",
            "default_filename": f"xgb_classifier_{symbol.replace('/', '_').lower()}_{timeframe}.joblib"
        },
        "LSTM": {
            "pipeline": LSTMTrainingPipeline,
            "config_section": "LSTM_Forecaster",
            "default_filename": f"lstm_{symbol.replace('/', '_').lower()}_{timeframe}.pth"
        },
        "KMeansRegime": {
            "pipeline": KMeansRegimeTrainingPipeline,
            "config_section": "KMeans_Regime_Model",
            "default_filename": f"kmeans_regime_{symbol.replace('/', '_').lower()}_{timeframe}.joblib"
        }
    }

    for model_name, config in models_to_train.items():
        model_params = settings.get_strategy_params(config["config_section"])
        model_base_path = model_params.get('modelsavepath', 'ml_models/trained_models')
        model_filename = model_params.get('modelfilename', config["default_filename"])
        
        full_model_path = os.path.join(PROJECT_ROOT, model_base_path, model_filename)

        if os.path.exists(full_model_path):
            logger.info(f"{model_name} model found at {full_model_path}. Skipping training.")
        else:
            logger.warning(f"{model_name} model not found at {full_model_path}. Starting training process...")
            try:
                pipeline = config["pipeline"](symbol=symbol, timeframe=timeframe)
                await pipeline.run_training()
                logger.info(f"Successfully trained and saved {model_name} model.")
            except Exception as e:
                logger.error(f"An error occurred during {model_name} model training: {e}", exc_info=True)


async def run_phase1_gauntlet():
    root_logger.info("Starting Kamikaze Komodo - Phase 1: Backtesting & Core Strategy Refinement Gauntlet")
    if not settings:
        root_logger.critical("Settings failed to load.")
        return

    data_handler = DataHandler()
    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    hist_days = settings.historical_data_days
    start_date = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_date = datetime.now(timezone.utc)

    # --- Gauntlet Configuration ---
    SLIPPAGE_MODEL_TO_TEST = 'volume_volatility_based' # Options: 'fixed', 'volume_volatility_based'
    
    # --- Prepare Regime Switching Sub-strategies ---
    regime_params = settings.get_strategy_params("RegimeSwitchingStrategy")
    trending_params = settings.get_strategy_params(regime_params.get('trending_strategy_section', 'EhlersInstantaneousTrendlineStrategy'))
    ranging_params = settings.get_strategy_params(regime_params.get('ranging_strategy_section', 'BollingerBandMeanReversionStrategy'))
    
    trending_strategy_instance = EhlersInstantaneousTrendlineStrategy(symbol, timeframe, trending_params)
    ranging_strategy_instance = BollingerBandMeanReversionStrategy(symbol, timeframe, ranging_params)

    strategy_map = {
        0: ranging_strategy_instance,
        1: trending_strategy_instance,
        2: HoldStrategy(symbol, timeframe)
    }

    strategies_to_test = {
        "EWMACStrategy": {
            "class": EWMACStrategy,
            "param_grid": { 'shortwindow': [12], 'longwindow': [26] }, # Reduced for speed
        },
        "BollingerBandBreakoutStrategy": {
            "class": BollingerBandBreakoutStrategy,
            "param_grid": { 'bb_period': [20], 'bb_std_dev': [2.0] }, # Reduced for speed
        },
        "VolatilitySqueezeBreakoutStrategy": {
            "class": VolatilitySqueezeBreakoutStrategy,
            "param_grid": { 'kc_atr_multiplier': [1.5], 'bb_std_dev': [2.0] }, # Reduced for speed
        },
        "FundingRateStrategy": {
            "class": FundingRateStrategy,
            "param_grid": { 'short_threshold': [0.0005], 'long_threshold': [-0.0005] }, # Reduced for speed
        },
        "EnsembleMLStrategy": {
            "class": EnsembleMLStrategy,
            "param_grid": { 'ensemble_method': ['majority_vote', 'weighted_average'] },
        },
        "RegimeSwitchingStrategy": {
            "class": RegimeSwitchingStrategy,
            "param_grid": { 'regime_confirmation_period': [2, 3] },
            "init_kwargs": {"strategy_mapping": strategy_map}
        },
    }
    
    # --- Prepare Data with Market Regimes ---
    root_logger.info("Preparing main data feed with market regimes for the gauntlet...")
    main_data_df = await data_handler.get_prepared_data(
        symbol, timeframe, start_date, end_date,
        needs_funding_rate=True, needs_sentiment=True
    )
    
    kmeans_params = settings.get_strategy_params("KMeans_Regime_Model")
    kmeans_config = settings.get_strategy_params("KMeans_Regime_Model")
    kmeans_path = os.path.join(PROJECT_ROOT, kmeans_config.get('modelsavepath', 'ml_models/trained_models'), f"kmeans_regime_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
    kmeans_model = KMeansRegimeModel(model_path=kmeans_path, params=kmeans_params)

    if kmeans_model.model:
        main_data_df['market_regime'] = kmeans_model.predict_regimes_for_dataframe(main_data_df)
        main_data_df['market_regime'] = main_data_df['market_regime'].ffill()
    else:
        logger.warning("KMeans model not loaded, 'market_regime' column will be empty.")
        main_data_df['market_regime'] = None
    
    if main_data_df.empty:
        logger.error("Failed to prepare main data feed. Aborting gauntlet.")
        await data_handler.close()
        return

    # --- WFO Settings ---
    total_days = (end_date - start_date).days
    train_days = int(total_days * 0.4)
    test_days = int(total_days * 0.1)
    tf_hours = 4
    if 'h' in timeframe:
        try: tf_hours = int(timeframe.replace('h', ''))
        except: pass
    elif 'd' in timeframe:
        try: tf_hours = int(timeframe.replace('d', '')) * 24
        except: pass
    bars_per_day = 24 / tf_hours if tf_hours > 0 else 1
    train_bars = int(train_days * bars_per_day)
    test_bars = int(test_days * bars_per_day)
    step_bars = test_bars
    
    gauntlet_summary = []

    for name, config in strategies_to_test.items():
        root_logger.info(f"\n{'='*25} GAUNTLET: {name} {'='*25}")
        
        if len(main_data_df) < (train_bars + test_bars):
            logger.error(f"Not enough data for {name} ({len(main_data_df)} bars). Required ~{train_bars + test_bars}. Skipping.")
            continue
            
        optimizer = StrategyOptimizer(
            strategy_class=config["class"],
            data_feed_df=main_data_df,
            param_grid=config["param_grid"],
            optimization_metric='sortino_ratio',
            initial_capital=10000.0,
            commission_bps=settings.commission_bps,
            slippage_bps=settings.slippage_bps,
            slippage_model_type=SLIPPAGE_MODEL_TO_TEST,
            symbol=symbol,
            timeframe=timeframe,
            warmup_period=200,
            strategy_init_kwargs=config.get("init_kwargs", {})
        )

        wfo_results = optimizer.walk_forward_optimization(
            training_period_bars=train_bars,
            testing_period_bars=test_bars,
            step_size_bars=step_bars
        )
        
        if wfo_results:
            results_df = pd.DataFrame(wfo_results)
            avg_sortino = results_df[f'test_sortino_ratio'].mean()
            avg_calmar = results_df[f'test_calmar_ratio'].mean() if f'test_calmar_ratio' in results_df.columns else float('nan')
            
            logger.info(f"--- WFO Results Summary for {name} ---")
            print(results_df[['train_end_date', 'test_end_date', 'best_params', 'test_sortino_ratio']].to_string())
            logger.info(f"Average Out-of-Sample Sortino Ratio: {avg_sortino:.4f}")
            logger.info(f"Average Out-of-Sample Calmar Ratio: {avg_calmar:.4f}")

            gauntlet_summary.append({
                "Strategy": name,
                "Avg Out-of-Sample Sortino": avg_sortino,
                "Avg Out-of-Sample Calmar": avg_calmar
            })
        else:
            logger.warning(f"WFO for {name} produced no results.")

    root_logger.info(f"\n{'='*25} GAUNTLET FINAL SUMMARY {'='*25}")
    if gauntlet_summary:
        summary_df = pd.DataFrame(gauntlet_summary).sort_values(by="Avg Out-of-Sample Sortino", ascending=False)
        print(summary_df.to_string())
    else:
        logger.info("No strategies completed the gauntlet.")
    
    await data_handler.close()
    root_logger.info("Phase 1 Gauntlet completed.")


async def main():
    root_logger.info("Kamikaze Komodo Program Starting...")
    if not settings:
        root_logger.critical("Settings failed to load. Application cannot start.")
        return

    await train_all_models_if_needed()
    await run_phase1_gauntlet()

    root_logger.info("Kamikaze Komodo Program Finished.")

if __name__ == "__main__":
    try:
        if settings and settings.sentiment_llm_provider == "VertexAI" and not settings.vertex_ai_project_id:
            root_logger.warning("Vertex AI is selected, but Project ID is not set in config.ini. AI features may fail.")
        
        if not os.path.exists("logs"):
            os.makedirs("logs")

        asyncio.run(main())
    except KeyboardInterrupt:
        root_logger.info("Kamikaze Komodo program terminated by user.")
    except Exception as e:
        root_logger.critical(f"Critical error in main execution: {e}", exc_info=True)
</code>

kamikaze_komodo/__init__.py:
<code>
# kamikaze_komodo/__init__.py
# This file makes the 'root' directory a Python package.
</code>

kamikaze_komodo/app_logger.py:
<code>
# kamikaze_komodo/app_logger.py
import logging
import os
from logging.handlers import RotatingFileHandler

LOG_DIR = "logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

log_file_path = os.path.join(LOG_DIR, "kamikaze_komodo.log")

# Configure logging
logger = logging.getLogger("KamikazeKomodo")
logger.setLevel(logging.DEBUG)  # Set to DEBUG to capture all levels of messages

# Create handlers
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)  # Console logs info and above

file_handler = RotatingFileHandler(
    log_file_path, maxBytes=10*1024*1024, backupCount=5  # 10MB per file, 5 backups
)
file_handler.setLevel(logging.DEBUG)  # File logs debug and above

# Create formatters and add it to handlers
log_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s.%(funcName)s - %(message)s')
console_handler.setFormatter(log_format)
file_handler.setFormatter(log_format)

# Add handlers to the logger
if not logger.handlers:
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

def get_logger(module_name: str) -> logging.Logger:
    """
    Returns a logger instance for a specific module.
    """
    return logging.getLogger(f"KamikazeKomodo.{module_name}")
</code>

kamikaze_komodo/orchestration/scheduler.py:
<code>
# kamikaze_komodo/orchestration/scheduler.py

from typing import Optional
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.executors.pool import ThreadPoolExecutor, ProcessPoolExecutor
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings
import os

logger = get_logger(__name__)

class TaskScheduler:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(TaskScheduler, cls).__new__(cls, *args, **kwargs)
        return cls._instance

    def __init__(self, db_path: Optional[str] = "logs/scheduler_jobs.sqlite"):
        if hasattr(self, '_initialized') and self._initialized: # Ensure __init__ runs only once for singleton
            return
        
        if not settings:
            logger.critical("Settings not loaded. TaskScheduler cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.db_path = db_path
        if self.db_path and not os.path.isabs(self.db_path):
            # Get project root based on this file's location: kamikaze_komodo/orchestration/scheduler.py
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            self.db_path = os.path.join(project_root, self.db_path)
        
        db_dir = os.path.dirname(self.db_path)
        if not os.path.exists(db_dir):
            os.makedirs(db_dir, exist_ok=True)
            logger.info(f"Created directory for scheduler database: {db_dir}")

        jobstores = {
            'default': SQLAlchemyJobStore(url=f'sqlite:///{self.db_path}')
        }
        executors = {
            'default': ThreadPoolExecutor(10), # For I/O bound tasks
            'processpool': ProcessPoolExecutor(3) # For CPU bound tasks
        }
        job_defaults = {
            'coalesce': False, # Run missed jobs if scheduler was down (be careful with this)
            'max_instances': 3 # Max parallel instances of the same job
        }
        
        self.scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            executors=executors,
            job_defaults=job_defaults,
            timezone='UTC' # Explicitly set timezone
        )
        self._initialized = True
        logger.info(f"TaskScheduler initialized with SQLite job store at: {self.db_path}")

    def start(self):
        if not self.scheduler.running:
            try:
                self.scheduler.start()
                logger.info("APScheduler started.")
            except Exception as e:
                logger.error(f"Failed to start APScheduler: {e}", exc_info=True)
        else:
            logger.info("APScheduler is already running.")

    def shutdown(self, wait: bool = True):
        if self.scheduler.running:
            try:
                self.scheduler.shutdown(wait=wait)
                logger.info("APScheduler shut down.")
            except Exception as e:
                logger.error(f"Error shutting down APScheduler: {e}", exc_info=True)

    def add_job(self, func, trigger: str = 'interval', **kwargs):
        """
        Adds a job to the scheduler.
        Args:
            func: The function to execute.
            trigger: The trigger type (e.g., 'interval', 'cron', 'date').
            **kwargs: Arguments for the trigger and job (e.g., minutes=1, id='my_job').
        """
        try:
            job = self.scheduler.add_job(func, trigger, **kwargs)
            logger.info(f"Job '{kwargs.get('id', func.__name__)}' added with trigger: {trigger}, params: {kwargs}")
            return job
        except Exception as e:
            logger.error(f"Failed to add job '{kwargs.get('id', func.__name__)}': {e}", exc_info=True)
            return None

    def remove_job(self, job_id: str):
        try:
            self.scheduler.remove_job(job_id)
            logger.info(f"Job '{job_id}' removed.")
        except Exception as e: # Specific exception: JobLookupError
            logger.warning(f"Failed to remove job '{job_id}': {e}")
</code>

kamikaze_komodo/orchestration/__init__.py:
<code>
# kamikaze_komodo/orchestration/__init__.py
# This file makes the 'orchestration' directory a Python package.
</code>

kamikaze_komodo/core/utils.py:
<code>
from datetime import datetime, timezone
from kamikaze_komodo.core.models import BarData
def format_timestamp(ts: datetime, fmt: str = "%Y-%m-%d %H:%M:%S %Z") -> str:
    """
    Formats a datetime object into a string.
    Ensures timezone awareness, defaulting to UTC if naive.
    """
    if ts.tzinfo is None:
        ts = ts.replace(tzinfo=timezone.utc)
    return ts.strftime(fmt)
def current_timestamp_ms() -> int:
    """
    Returns the current UTC timestamp in milliseconds.
    """
    return int(datetime.now(timezone.utc).timestamp() * 1000)
def ohlcv_to_bardata(ohlcv: list, symbol: str, timeframe: str) -> BarData:
    """
    Converts a CCXT OHLCV list [timestamp_ms, open, high, low, close, volume]
    to a BarData object.
    """
    from kamikaze_komodo.core.models import BarData # Local import to avoid circular dependency
    
    if len(ohlcv) != 6:
        raise ValueError("OHLCV list must contain 6 elements: timestamp, open, high, low, close, volume")
    dt_object = datetime.fromtimestamp(ohlcv[0] / 1000, tz=timezone.utc)
    return BarData(
        timestamp=dt_object,
        open=float(ohlcv[1]),
        high=float(ohlcv[2]),
        low=float(ohlcv[3]),
        close=float(ohlcv[4]),
        volume=float(ohlcv[5]),
        symbol=symbol,
        timeframe=timeframe
    )
# Add other utility functions as needed, e.g.,
# - Mathematical helpers not in TA-Lib
# - Data validation functions
# - etc.
</code>

kamikaze_komodo/core/models.py:
<code>
# kamikaze_komodo/core/models.py
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field, ConfigDict
from datetime import datetime, timezone
import pydantic
from kamikaze_komodo.core.enums import OrderType, OrderSide, SignalType, TradeResult

class BarData(BaseModel):
    """
    Represents OHLCV market data for a specific time interval.
    Used for data interchange, primarily from the DataFetcher.
    Phase 1: Made model flexible to hold arbitrary indicator data from backtest DataFrames.
    """
    model_config = ConfigDict(extra='allow', frozen=False)

    timestamp: datetime = Field(..., description="The start time of the candle, expected to be timezone-aware (UTC)")
    open: float = Field(..., gt=0, description="Opening price")
    high: float = Field(..., gt=0, description="Highest price")
    low: float = Field(..., gt=0, description="Lowest price")
    close: float = Field(..., gt=0, description="Closing price")
    volume: float = Field(..., ge=0, description="Trading volume")
    symbol: Optional[str] = Field(None, description="Trading symbol, e.g., BTC/USD")
    timeframe: Optional[str] = Field(None, description="Candle timeframe, e.g., 1h")
    funding_rate: Optional[float] = Field(None, description="Funding rate for perpetual futures")
    sentiment_score: Optional[float] = Field(None, description="Sentiment score associated with this bar's timestamp")
    market_regime: Optional[int] = Field(None, description="Market regime identified by a model (e.g., 0, 1, 2)")

class Order(BaseModel):
    id: str = Field(..., description="Unique order identifier (from exchange or internal)")
    symbol: str = Field(..., description="Trading symbol, e.g., BTC/USD")
    type: OrderType = Field(..., description="Type of order (market, limit, etc.)")
    side: OrderSide = Field(..., description="Order side (buy or sell)")
    amount: float = Field(..., gt=0, description="Quantity of the asset to trade")
    price: Optional[float] = Field(None, gt=0, description="Price for limit or stop orders")
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description="Time the order was created")
    status: str = Field("open", description="Current status of the order (e.g., open, filled, canceled)")
    filled_amount: float = Field(0.0, ge=0, description="Amount of the order that has been filled")
    average_fill_price: Optional[float] = Field(None, description="Average price at which the order was filled")
    exchange_id: Optional[str] = Field(None, description="Order ID from the exchange")

class Trade(BaseModel):
    id: str = Field(..., description="Unique trade identifier")
    symbol: str = Field(..., description="Trading symbol, e.g., BTC/USD")
    entry_order_id: str = Field(..., description="ID of the order that opened the trade")
    exit_order_id: Optional[str] = Field(None, description="ID of the order that closed the trade")
    side: OrderSide = Field(..., description="Trade side (buy/long or sell/short)")
    entry_price: float = Field(..., gt=0, description="Price at which the trade was entered")
    exit_price: Optional[float] = Field(None, description="Price at which the trade was exited (must be >0 if set)")
    amount: float = Field(..., gt=0, description="Quantity of the asset traded")
    entry_timestamp: datetime = Field(..., description="Time the trade was entered")
    exit_timestamp: Optional[datetime] = Field(None, description="Time the trade was exited")
    pnl: Optional[float] = Field(None, description="Profit or Loss for the trade")
    pnl_percentage: Optional[float] = Field(None, description="Profit or Loss percentage for the trade")
    commission: float = Field(0.0, ge=0, description="Trading commission paid")
    result: Optional[TradeResult] = Field(None, description="Outcome of the trade (Win/Loss/Breakeven)")
    notes: Optional[str] = Field(None, description="Any notes related to the trade")
    custom_fields: Dict[str, Any] = Field(default_factory=dict, description="Custom fields for additional trade data, e.g., atr_at_entry")

    @pydantic.field_validator('exit_price')
    def exit_price_must_be_positive(cls, v):
        if v is not None and v <= 0:
            raise ValueError('exit_price must be positive if set')
        return v

class NewsArticle(BaseModel):
    id: str = Field(..., description="Unique identifier for the news article (e.g., URL hash or URL itself)")
    url: str = Field(..., description="Source URL of the article")
    title: str = Field(..., description="Headline or title of the article")
    publication_date: Optional[datetime] = Field(None, description="Date the article was published (UTC)")
    retrieval_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description="Date the article was retrieved (UTC)")
    source: str = Field(..., description="Source of the news (e.g., CoinDesk, CoinTelegraph, RSS feed name)")
    content: Optional[str] = Field(None, description="Full text content of the article")
    summary: Optional[str] = Field(None, description="AI-generated or scraped summary")
    sentiment_score: Optional[float] = Field(None, description="Overall sentiment score (-1.0 to 1.0)")
    sentiment_label: Optional[str] = Field(None, description="Sentiment label (e.g., positive, negative, neutral, bullish, bearish)")
    sentiment_confidence: Optional[float] = Field(None, description="Confidence of the sentiment analysis (0.0 to 1.0)")
    key_themes: Optional[List[str]] = Field(default_factory=list, description="Key themes identified by sentiment analysis")
    related_symbols: Optional[List[str]] = Field(default_factory=list, description="Cryptocurrencies mentioned or related")
    raw_llm_response: Optional[Dict[str, Any]] = Field(None, description="Raw response from LLM for sentiment if available")

class PortfolioSnapshot(BaseModel):
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    total_value_usd: float = Field(..., description="Total portfolio value in USD")
    cash_balance_usd: float = Field(..., description="Available cash in USD")
    positions: Dict[str, float] = Field(default_factory=dict, description="Asset quantities, e.g., {'BTC': 0.5, 'ETH': 10}")
    open_pnl_usd: float = Field(0.0, description="Total open Profit/Loss in USD for current positions")

class PairTrade(BaseModel):
    id: str = Field(..., description="Unique identifier for the pair trade")
    asset1_symbol: str = Field(..., description="Symbol of the first asset in the pair")
    asset2_symbol: str = Field(..., description="Symbol of the second asset in the pair")
    asset1_trade_id: Optional[str] = Field(None, description="Trade ID for the leg involving asset1")
    asset2_trade_id: Optional[str] = Field(None, description="Trade ID for the leg involving asset2")
    entry_timestamp: datetime = Field(..., description="Time the pair trade was initiated")
    exit_timestamp: Optional[datetime] = Field(None, description="Time the pair trade was closed")
    entry_spread: float = Field(..., description="Spread value at the time of entry")
    entry_zscore: Optional[float] = Field(None, description="Z-score of the spread at entry")
    exit_spread: Optional[float] = Field(None, description="Spread value at the time of exit")
    exit_zscore: Optional[float] = Field(None, description="Z-score of the spread at exit")
    pnl: Optional[float] = Field(None, description="Overall Profit or Loss for the pair trade")
    pnl_percentage: Optional[float] = Field(None, description="Overall Profit or Loss percentage for the pair trade")
    total_commission: float = Field(0.0, ge=0, description="Total commission for both legs of the pair trade")
    status: str = Field("open", description="Status of the pair trade (e.g., open, closed)")
    exit_reason: Optional[str] = Field(None, description="Reason for closing the pair trade (e.g., spread reversion, stop loss)")
    notes: Optional[str] = Field(None, description="Any notes related to the pair trade")
    custom_fields: Dict[str, Any] = Field(default_factory=dict, description="Custom fields for additional pair trade data")
</code>

kamikaze_komodo/core/__init__.py:
<code>
# kamikaze_komodo/core/__init__.py
# This file makes the 'core' directory a Python package.
</code>

kamikaze_komodo/core/enums.py:
<code>
# kamikaze_komodo/core/enums.py

from enum import Enum


class OrderType(Enum):

    """

    Represents the type of an order.

    """

    MARKET = "market"

    LIMIT = "limit"

    STOP = "stop"

    STOP_LIMIT = "stop_limit"

    TAKE_PROFIT = "take_profit"

    TAKE_PROFIT_LIMIT = "take_profit_limit"


class OrderSide(Enum):

    """

    Represents the side of an order.

    """

    BUY = "buy"

    SELL = "sell"


class SignalType(Enum):

    """

    Represents the type of trading signal generated by a strategy.

    """

    LONG = "LONG"

    SHORT = "SHORT"

    HOLD = "HOLD"

    CLOSE_LONG = "CLOSE_LONG"

    CLOSE_SHORT = "CLOSE_SHORT"


class CandleInterval(Enum):

    """

    Represents common candle intervals for market data.

    Follows CCXT conventions where possible.

    """

    ONE_MINUTE = "1m"

    THREE_MINUTES = "3m"

    FIVE_MINUTES = "5m"

    FIFTEEN_MINUTES = "15m"

    THIRTY_MINUTES = "30m"

    ONE_HOUR = "1h"

    TWO_HOURS = "2h"

    FOUR_HOURS = "4h"

    SIX_HOURS = "6h"

    EIGHT_HOURS = "8h"

    TWELVE_HOURS = "12h"

    ONE_DAY = "1d"

    THREE_DAYS = "3d"

    ONE_WEEK = "1w"

    ONE_MONTH = "1M"


class TradeResult(Enum):

    """

    Represents the outcome of a trade.

    """

    WIN = "WIN"

    LOSS = "LOSS"

    BREAKEVEN = "BREAKEVEN"

</code>

kamikaze_komodo/backtesting_engine/__init__.py:
<code>
# kamikaze_komodo/backtesting_engine/__init__.py
# This file makes the 'backtesting_engine' directory a Python package.
</code>

kamikaze_komodo/backtesting_engine/optimizer.py:
<code>
# FILE: kamikaze_komodo/backtesting_engine/optimizer.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
import itertools

from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.backtesting_engine.engine import BacktestingEngine
from kamikaze_komodo.backtesting_engine.performance_analyzer import PerformanceAnalyzer
from kamikaze_komodo.config.settings import settings as app_settings
from kamikaze_komodo.risk_control_module.position_sizer import FixedFractionalPositionSizer
from kamikaze_komodo.risk_control_module.stop_manager import PercentageStopManager
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class StrategyOptimizer:
    """
    Optimizes strategy parameters using Grid Search for Walk-Forward Optimization.
    Includes a warmup period to ensure indicators are ready.
    """
    def __init__(
        self,
        strategy_class: type,
        data_feed_df: pd.DataFrame,
        param_grid: Dict[str, List[Any]],
        optimization_metric: str = 'sortino_ratio',
        initial_capital: float = 10000.0,
        commission_bps: float = 0.0,
        slippage_bps: float = 0.0,
        slippage_model_type: str = 'fixed', # <-- ADDED
        symbol: Optional[str] = None,
        timeframe: Optional[str] = None,
        warmup_period: int = 200,
        strategy_init_kwargs: Optional[Dict[str, Any]] = None # <-- ADDED
    ):
        self.strategy_class = strategy_class
        self.data_feed_df = data_feed_df
        self.param_grid = param_grid if param_grid else {}
        self.optimization_metric = optimization_metric
        self.initial_capital = initial_capital
        self.commission_bps = commission_bps
        self.slippage_bps = slippage_bps
        self.slippage_model_type = slippage_model_type # <-- ADDED
        self.warmup_period = warmup_period
        self.symbol = symbol or (app_settings.default_symbol if app_settings else "OPTIMIZE_SYMBOL")
        self.timeframe = timeframe or (app_settings.default_timeframe if app_settings else "OPTIMIZE_TF")
        self.strategy_init_kwargs = strategy_init_kwargs or {} # <-- ADDED
        logger.info(f"StrategyOptimizer initialized for {strategy_class.__name__}. Optimizing for: {optimization_metric}")

    def _run_backtest_for_params(self, params_set: Dict[str, Any], data_feed: pd.DataFrame) -> Dict[str, Any]:
        """Runs a single backtest and returns a dictionary of all performance metrics."""
        try:
            base_params = app_settings.get_strategy_params(self.strategy_class.__name__) if app_settings else {}
            combined_params = {**base_params, **params_set}

            # MODIFIED: Pass strategy_init_kwargs to the strategy constructor
            strategy_instance = self.strategy_class(
                symbol=self.symbol,
                timeframe=self.timeframe,
                params=combined_params,
                **self.strategy_init_kwargs
            )
            
            # MODIFIED: Pass the slippage model type to the engine
            engine = BacktestingEngine(
                data_feed_df=data_feed,
                strategy=strategy_instance,
                initial_capital=self.initial_capital,
                commission_bps=self.commission_bps,
                slippage_bps=self.slippage_bps,
                slippage_model_type=self.slippage_model_type
            )
            trades_log, final_portfolio, equity_curve = engine.run()
            
            analyzer = PerformanceAnalyzer(
                trades=trades_log,
                initial_capital=self.initial_capital,
                final_capital=final_portfolio['final_portfolio_value'],
                equity_curve_df=equity_curve
            )
            return analyzer.calculate_metrics()
        except Exception as e:
            logger.error(f"Error during backtest for params {params_set}: {e}", exc_info=True)
            return {}

    def grid_search(self) -> Tuple[Optional[Dict[str, Any]], float, pd.DataFrame]:
        """Performs grid search on the entire data_feed_df provided to the optimizer instance."""
        param_names = list(self.param_grid.keys())
        
        if not self.param_grid or not all(self.param_grid.values()):
            combinations = [{}]
        else:
            combinations = [dict(zip(param_names, combo)) for combo in itertools.product(*self.param_grid.values())]

        results = []
        best_metric = -float('inf')
        best_params = None

        logger.info(f"Starting Grid Search with {len(combinations)} combinations.")

        for i, params in enumerate(combinations):
            logger.debug(f"Grid Search - Combo {i+1}/{len(combinations)}: {params}")
            
            metrics = self._run_backtest_for_params(params, self.data_feed_df)
            metric_value = metrics.get(self.optimization_metric)
            
            if metric_value is None or pd.isna(metric_value):
                metric_value = -float('inf')

            results.append({**params, 'metric_value': metric_value})

            if metric_value > best_metric:
                best_metric = metric_value
                best_params = params

        results_df = pd.DataFrame(results)
        if best_params is not None:
            logger.info(f"Grid Search completed. Best params: {best_params}, Best {self.optimization_metric}: {best_metric:.4f}")
        else:
            logger.warning("Grid Search completed but no best parameters found.")
        
        return best_params, best_metric, results_df

    def walk_forward_optimization(
        self,
        training_period_bars: int,
        testing_period_bars: int,
        step_size_bars: int,
    ) -> List[Dict[str, Any]]:
        if not isinstance(self.data_feed_df.index, pd.DatetimeIndex):
            raise ValueError("data_feed_df must have a DatetimeIndex for WFO.")

        full_data = self.data_feed_df
        n_total_bars = len(full_data)

        if self.warmup_period + training_period_bars + testing_period_bars > n_total_bars:
            logger.error(f"Not enough data for even one WFO window. Required: {self.warmup_period + training_period_bars + testing_period_bars}, Available: {n_total_bars}")
            return []

        results_over_time = []
        start_idx = self.warmup_period

        logger.info(f"Starting WFO. Train: {training_period_bars}, Test: {testing_period_bars}, Step: {step_size_bars}, Warmup: {self.warmup_period}")

        while start_idx + training_period_bars <= n_total_bars:
            train_slice_start = start_idx - self.warmup_period
            train_slice_end = start_idx + training_period_bars
            
            test_slice_start = train_slice_end - self.warmup_period
            test_slice_end = min(test_slice_start + self.warmup_period + testing_period_bars, n_total_bars)

            if test_slice_start >= test_slice_end:
                break

            training_data = full_data.iloc[train_slice_start:train_slice_end]
            testing_data = full_data.iloc[test_slice_start:test_slice_end]
            
            logger.info(f"WFO Step: Training from {training_data.index[self.warmup_period]} to {training_data.index[-1]}")
            
            step_optimizer = StrategyOptimizer(
                strategy_class=self.strategy_class,
                data_feed_df=training_data,
                param_grid=self.param_grid,
                optimization_metric=self.optimization_metric,
                initial_capital=self.initial_capital,
                commission_bps=self.commission_bps,
                slippage_bps=self.slippage_bps,
                slippage_model_type=self.slippage_model_type, # Pass it down
                symbol=self.symbol,
                timeframe=self.timeframe,
                strategy_init_kwargs=self.strategy_init_kwargs # Pass it down
            )
            
            best_params_for_step, train_metric_value, _ = step_optimizer.grid_search()

            if best_params_for_step is not None:
                logger.info(f"WFO Step: Best params from training: {best_params_for_step} (Metric: {train_metric_value:.4f})")
                logger.info(f"WFO Step: Testing from {testing_data.index[self.warmup_period]} to {testing_data.index[-1]}")
                
                test_metrics = self._run_backtest_for_params(best_params_for_step, testing_data)
                
                if test_metrics:
                    result_entry = {
                        'train_start_date': training_data.index[self.warmup_period],
                        'train_end_date': training_data.index[-1],
                        'test_start_date': testing_data.index[self.warmup_period],
                        'test_end_date': testing_data.index[-1],
                        'best_params': best_params_for_step,
                        f'train_{self.optimization_metric}': train_metric_value,
                    }
                    result_entry.update({f'test_{k}': v for k, v in test_metrics.items()})
                    results_over_time.append(result_entry)
                    logger.info(f"WFO Step: Test period {self.optimization_metric}: {test_metrics.get(self.optimization_metric):.4f}")
                else:
                    logger.warning("WFO Step: Backtest on testing data failed or produced no metrics.")
            else:
                logger.warning("WFO Step: No best parameters found in training phase. Skipping test for this step.")

            start_idx += step_size_bars

        logger.info("Walk-Forward Optimization completed.")
        return results_over_time
</code>

kamikaze_komodo/backtesting_engine/performance_analyzer.py:
<code>
# FILE: kamikaze_komodo/backtesting_engine/performance_analyzer.py
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from scipy.stats import norm

from kamikaze_komodo.core.models import Trade
from kamikaze_komodo.core.enums import OrderSide, TradeResult
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class PerformanceAnalyzer:
    def __init__(
        self,
        trades: List[Trade],
        initial_capital: float,
        final_capital: float,
        equity_curve_df: Optional[pd.DataFrame] = None, # Timestamp-indexed 'total_value_usd'
        risk_free_rate_annual: float = 0.02, # Annual risk-free rate (e.g., 2%)
        annualization_factor: int = 252 # Trading days in a year for Sharpe/Sortino
    ):
        if not trades:
            logger.warning("PerformanceAnalyzer initialized with no trades. Some metrics might be zero or NaN.")
        self.trades_df = pd.DataFrame([trade.model_dump() for trade in trades])
        if not self.trades_df.empty:
            self.trades_df['entry_timestamp'] = pd.to_datetime(self.trades_df['entry_timestamp'])
            self.trades_df['exit_timestamp'] = pd.to_datetime(self.trades_df['exit_timestamp'])
    
        self.initial_capital = initial_capital
        self.final_capital = final_capital
        self.equity_curve_df = equity_curve_df
        self.risk_free_rate_annual = risk_free_rate_annual
        self.annualization_factor = annualization_factor
    
        logger.info(f"PerformanceAnalyzer initialized. Trades: {len(trades)}, Initial: ${initial_capital:,.2f}, Final: ${final_capital:,.2f}")
        logger.info(f"Using Annual Risk-Free Rate: {self.risk_free_rate_annual*100:.2f}%, Annualization Factor: {self.annualization_factor}")


    def _calculate_periodic_returns(self) -> Optional[pd.Series]:
        # FIX: Use 'total_value_usd' to match the backtesting engine's output
        if self.equity_curve_df is None or self.equity_curve_df.empty or 'total_value_usd' not in self.equity_curve_df.columns:
            logger.warning("Equity curve data is missing or invalid. Cannot calculate periodic returns for Sharpe/Sortino.")
            return None
        # Resample to daily returns for annualization, handling potential non-unique index if multiple records per day
        daily_equity = self.equity_curve_df['total_value_usd'].resample('D').last().ffill()
        periodic_returns = daily_equity.pct_change().dropna()
        return periodic_returns

    def calculate_metrics(self) -> Dict[str, Any]:
        metrics: Dict[str, Any] = {
            "initial_capital": self.initial_capital,
            "final_capital": self.final_capital,
            "total_net_profit": 0.0,
            "total_return_pct": 0.0,
            "total_trades": 0,
            "winning_trades": 0,
            "losing_trades": 0,
            "breakeven_trades": 0,
            "win_rate_pct": 0.0,
            "loss_rate_pct": 0.0,
            "average_pnl_per_trade": 0.0,
            "average_win_pnl": 0.0,
            "average_loss_pnl": 0.0,
            "profit_factor": np.nan,
            "max_drawdown_pct": 0.0,
            "sharpe_ratio": np.nan,
            "sortino_ratio": np.nan,
            "calmar_ratio": np.nan,
            "total_fees_paid": 0.0,
            "average_holding_period_hours": 0.0,
            "longest_win_streak": 0,
            "longest_loss_streak": 0,
            "time_in_market_pct": 0.0,
            "turnover_rate": np.nan,
        }

        if self.trades_df.empty:
            metrics["total_net_profit"] = self.final_capital - self.initial_capital
            if self.initial_capital > 0:
                metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
            logger.warning("No trades to analyze. Returning basic capital metrics.")
            return metrics

        pnl_series = self.trades_df['pnl'].dropna()
        if pnl_series.empty:
            metrics["total_net_profit"] = self.final_capital - self.initial_capital # If PnL couldn't be calculated for trades
            if self.initial_capital > 0: metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
            metrics["total_trades"] = len(self.trades_df)
            metrics["total_fees_paid"] = self.trades_df['commission'].sum()
            return metrics
    
        metrics["total_net_profit"] = pnl_series.sum()
        if self.initial_capital > 0: metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
        metrics["total_trades"] = len(pnl_series)

        wins = pnl_series[pnl_series > 0]
        losses = pnl_series[pnl_series < 0]
        breakevens = pnl_series[pnl_series == 0]
        metrics["winning_trades"] = len(wins)
        metrics["losing_trades"] = len(losses)
        metrics["breakeven_trades"] = len(breakevens)

        if metrics["total_trades"] > 0:
            metrics["win_rate_pct"] = (metrics["winning_trades"] / metrics["total_trades"]) * 100
            metrics["loss_rate_pct"] = (metrics["losing_trades"] / metrics["total_trades"]) * 100
            metrics["average_pnl_per_trade"] = pnl_series.mean()
        if not wins.empty: metrics["average_win_pnl"] = wins.mean()
        if not losses.empty: metrics["average_loss_pnl"] = losses.mean()

        gross_profit = wins.sum()
        gross_loss = abs(losses.sum())
        if gross_loss > 0: metrics["profit_factor"] = gross_profit / gross_loss
        elif gross_profit > 0: metrics["profit_factor"] = np.inf
    
        metrics["total_fees_paid"] = self.trades_df['commission'].sum()

        # Max Drawdown (from equity curve)
        # FIX: Use 'total_value_usd'
        if self.equity_curve_df is not None and not self.equity_curve_df.empty and 'total_value_usd' in self.equity_curve_df.columns:
            equity_values = self.equity_curve_df['total_value_usd']
            if len(equity_values) > 1:
                peak = equity_values.expanding(min_periods=1).max()
                drawdown = (equity_values - peak) / peak
                metrics["max_drawdown_pct"] = abs(drawdown.min()) * 100 if not drawdown.empty else 0.0
    
        # Sharpe and Sortino Ratios
        periodic_returns = self._calculate_periodic_returns()
        if periodic_returns is not None and len(periodic_returns) > 1:
            risk_free_rate_periodic = self.risk_free_rate_annual / self.annualization_factor
            excess_returns = periodic_returns - risk_free_rate_periodic
        
            # Sharpe Ratio
            sharpe_avg_excess_return = excess_returns.mean()
            sharpe_std_excess_return = excess_returns.std()
            if sharpe_std_excess_return is not None and sharpe_std_excess_return != 0:
                metrics["sharpe_ratio"] = (sharpe_avg_excess_return / sharpe_std_excess_return) * np.sqrt(self.annualization_factor)
        
            # Sortino Ratio
            downside_returns = excess_returns[excess_returns < 0]
            if not downside_returns.empty:
                downside_deviation = downside_returns.std()
                if downside_deviation is not None and downside_deviation != 0:
                    metrics["sortino_ratio"] = (sharpe_avg_excess_return / downside_deviation) * np.sqrt(self.annualization_factor)
    
        # Calmar Ratio
        if metrics["max_drawdown_pct"] is not None and metrics["max_drawdown_pct"] > 0:
            if self.equity_curve_df is not None and not self.equity_curve_df.empty:
                start_date = self.equity_curve_df.index.min()
                end_date = self.equity_curve_df.index.max()
                duration_years = (end_date - start_date).days / 365.25 if (end_date - start_date).days > 0 else 1.0/365.25
                total_return = (self.final_capital / self.initial_capital) - 1 if self.initial_capital > 0 else 0
                annualized_return = ((1 + total_return) ** (1 / duration_years)) - 1 if duration_years > 0 else total_return
                metrics["calmar_ratio"] = (annualized_return * 100) / metrics["max_drawdown_pct"]

        # Average Holding Period
        if not self.trades_df.empty and 'exit_timestamp' in self.trades_df.columns and 'entry_timestamp' in self.trades_df.columns:
            valid_trades_for_duration = self.trades_df.dropna(subset=['entry_timestamp', 'exit_timestamp'])
            if not valid_trades_for_duration.empty:
                holding_periods = (valid_trades_for_duration['exit_timestamp'] - valid_trades_for_duration['entry_timestamp'])
                metrics["average_holding_period_hours"] = holding_periods.mean().total_seconds() / 3600 if not holding_periods.empty else 0.0

        # Win/Loss Streaks
        if not pnl_series.empty:
            win_streak, loss_streak = 0, 0
            current_win_streak, current_loss_streak = 0, 0
            for pnl_val in pnl_series:
                if pnl_val > 0:
                    current_win_streak += 1
                    current_loss_streak = 0
                elif pnl_val < 0:
                    current_loss_streak += 1
                    current_win_streak = 0
                else: # Breakeven
                    current_win_streak = 0
                    current_loss_streak = 0
                win_streak = max(win_streak, current_win_streak)
                loss_streak = max(loss_streak, current_loss_streak)
            metrics["longest_win_streak"] = win_streak
            metrics["longest_loss_streak"] = loss_streak
        
        # Time in Market
        if self.equity_curve_df is not None and not self.equity_curve_df.empty:
            total_duration = self.equity_curve_df.index.max() - self.equity_curve_df.index.min()
            if total_duration.total_seconds() > 0:
                time_in_trades = timedelta(0)
                for _, trade in self.trades_df.iterrows():
                    if pd.notna(trade['exit_timestamp']):
                        time_in_trades += trade['exit_timestamp'] - trade['entry_timestamp']
                metrics["time_in_market_pct"] = (time_in_trades / total_duration) * 100

        # Turnover Rate
        # FIX: Use 'total_value_usd'
        if self.equity_curve_df is not None and not self.equity_curve_df.empty and len(self.equity_curve_df) > 1:
            total_traded_value = self.trades_df.apply(lambda x: abs(x['amount'] * x['entry_price']), axis=1).sum()
            total_traded_value += self.trades_df.apply(lambda x: abs(x['amount'] * x['exit_price']) if pd.notna(x['exit_price']) else 0, axis=1).sum()

            time_diffs = self.equity_curve_df.index.to_series().diff().dt.total_seconds().fillna(0)
            time_weighted_avg_equity = np.average(self.equity_curve_df['total_value_usd'], weights=time_diffs)
            
            if time_weighted_avg_equity > 0:
                metrics["turnover_rate"] = total_traded_value / time_weighted_avg_equity

        return metrics

    def print_summary(self, metrics: Optional[Dict[str, Any]] = None):
        if metrics is None:
            metrics = self.calculate_metrics()

        summary = f"""
        --------------------------------------------------
        |          Backtest Performance Summary          |
        --------------------------------------------------
        | Metric                       | Value               |
        --------------------------------------------------
        | Initial Capital              | ${metrics.get("initial_capital", 0):<15,.2f} |
        | Final Capital                | ${metrics.get("final_capital", 0):<15,.2f} |
        | Total Net Profit             | ${metrics.get("total_net_profit", 0):<15,.2f} |
        | Total Return                 | {metrics.get("total_return_pct", 0):<15.2f}% |
        | Total Trades                 | {metrics.get("total_trades", 0):<16} |
        | Winning Trades               | {metrics.get("winning_trades", 0):<16} |
        | Losing Trades                | {metrics.get("losing_trades", 0):<16} |
        | Breakeven Trades             | {metrics.get("breakeven_trades", 0):<16} |
        | Win Rate                     | {metrics.get("win_rate_pct", 0):<15.2f}% |
        | Loss Rate                    | {metrics.get("loss_rate_pct", 0):<15.2f}% |
        | Average PnL per Trade        | ${metrics.get("average_pnl_per_trade", 0):<15,.2f} |
        | Average Win PnL              | ${metrics.get("average_win_pnl", 0):<15,.2f} |
        | Average Loss PnL             | ${metrics.get("average_loss_pnl", 0):<15,.2f} |
        | Profit Factor                | {metrics.get("profit_factor", float('nan')):<16.2f} |
        | Max Drawdown                 | {metrics.get("max_drawdown_pct", 0):<15.2f}% |
        | Sharpe Ratio                 | {metrics.get("sharpe_ratio", float('nan')):<16.2f} |
        | Sortino Ratio                | {metrics.get("sortino_ratio", float('nan')):<16.2f} |
        | Calmar Ratio                 | {metrics.get("calmar_ratio", float('nan')):<16.2f} |
        | Avg Holding Period (hours)   | {metrics.get("average_holding_period_hours", 0):<16.2f} |
        | Longest Win Streak           | {metrics.get("longest_win_streak", 0):<16} |
        | Longest Loss Streak          | {metrics.get("longest_loss_streak", 0):<16} |
        | Time in Market               | {metrics.get("time_in_market_pct", 0):<15.2f}% |
        | Turnover Rate                | {metrics.get("turnover_rate", float('nan')):<16.2f} |
        | Total Fees Paid              | ${metrics.get("total_fees_paid", 0):<15,.2f} |
        --------------------------------------------------
        """
        print(summary)
        logger.info("Performance summary generated." + summary.replace("\n      |", "\n"))


    @staticmethod
    def calculate_deflated_sharpe_ratio(
        sharpe_ratios_series: pd.Series,
        num_bars_in_backtest: int,
        selected_sharpe: float
    ) -> Optional[float]:
        """
        Calculates the Deflated Sharpe Ratio (DSR) based on a series of Sharpe Ratios from multiple trials.
        This indicates the probability that the selected Sharpe Ratio is a false positive.
        Based on "The Deflated Sharpe Ratio" by Lopez de Prado.

        Args:
            sharpe_ratios_series (pd.Series): A series of Sharpe Ratios from an optimization run (e.g., grid search).
            num_bars_in_backtest (int): The number of observations (e.g., days, bars) in the backtest period.
            selected_sharpe (float): The Sharpe Ratio of the strategy selected from the trials.

        Returns:
            Optional[float]: The Deflated Sharpe Ratio, or None if calculation fails.
        """
        if sharpe_ratios_series.empty or num_bars_in_backtest < 30:
            logger.warning("DSR calculation requires a series of Sharpe Ratios and a sufficient number of backtest bars.")
            return None

        n_trials = len(sharpe_ratios_series)
        var_sr = sharpe_ratios_series.var()

        if pd.isna(var_sr) or var_sr == 0:
            logger.warning("Variance of Sharpe Ratios is zero or NaN. Cannot calculate DSR.")
            return None
        
        # Expected maximum Sharpe Ratio approximation
        euler_mascheroni = 0.5772156649
        e_max_sr = ( (1 - euler_mascheroni) * norm.ppf(1 - 1/n_trials) ) + \
                   ( euler_mascheroni * norm.ppf(1 - 1/(n_trials * np.e)) )
        
        # Deflated Sharpe Ratio Calculation
        # DSR = P[SR_real <= SR_selected | N trials] = CDF of Z score
        try:
            # The Z-score compares the selected SR to the expected maximum SR from random trials
            z_score = (selected_sharpe - (e_max_sr * np.sqrt(var_sr))) * \
                      np.sqrt(num_bars_in_backtest - 1)
            
            deflated_sharpe = norm.cdf(z_score)
            logger.info(f"DSR Calculation: N_trials={n_trials}, Var(SR)={var_sr:.4f}, E[MaxSR]={e_max_sr:.4f}, SelectedSR={selected_sharpe:.4f}, Z-Score={z_score:.4f}")
            return deflated_sharpe
        except Exception as e:
            logger.error(f"Error calculating Deflated Sharpe Ratio: {e}", exc_info=True)
            return None
</code>

kamikaze_komodo/backtesting_engine/engine.py:
<code>
# FILE: kamikaze_komodo/backtesting_engine/engine.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Union
from kamikaze_komodo.core.models import BarData, Trade, PortfolioSnapshot
from kamikaze_komodo.core.enums import SignalType, OrderType, OrderSide, TradeResult
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone

# Explicitly import all position sizers and stop managers
from kamikaze_komodo.risk_control_module.position_sizer import BasePositionSizer, FixedFractionalPositionSizer, ATRBasedPositionSizer, PairTradingPositionSizer
from kamikaze_komodo.risk_control_module.optimal_f_position_sizer import OptimalFPositionSizer
from kamikaze_komodo.risk_control_module.ml_confidence_position_sizer import MLConfidencePositionSizer

from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager, PercentageStopManager, ATRStopManager
from kamikaze_komodo.risk_control_module.parabolic_sar_stop import ParabolicSARStop
from kamikaze_komodo.risk_control_module.triple_barrier_stop import TripleBarrierStop, StopTriggerType

from kamikaze_komodo.risk_control_module.risk_manager import RiskManager
from kamikaze_komodo.portfolio_constructor.base_portfolio_constructor import BasePortfolioConstructor
from kamikaze_komodo.portfolio_constructor.asset_allocator import FixedWeightAssetAllocator, OptimalFAllocator, HRPAllocator, BaseAssetAllocator
from kamikaze_komodo.config.settings import settings

logger = get_logger(__name__)

class BacktestingEngine:
    def __init__(
        self,
        data_feed_df: pd.DataFrame,
        strategy: BaseStrategy,
        initial_capital: float = 10000.0,
        commission_bps: float = 0.0,
        position_sizer_type: Optional[str] = None,
        stop_manager_type: Optional[str] = None,
        portfolio_constructor_type: Optional[str] = None,
        asset_allocator_type: Optional[str] = None,
        slippage_bps: float = 0.0,
        slippage_model_type: str = 'fixed'
    ):
        if data_feed_df.empty:
            raise ValueError("Data feed DataFrame cannot be empty.")
        if not isinstance(data_feed_df.index, pd.DatetimeIndex):
            raise ValueError("Data feed DataFrame must be indexed by pd.DatetimeIndex.")

        self.strategy = strategy
        
        logger.info(f"Preparing data for strategy '{self.strategy.name}'...")
        self.prepared_data_feed = self.strategy.prepare_data(data_feed_df.copy())
        
        self.initial_capital = initial_capital
        self.commission_rate = commission_bps / 10000.0
        self.slippage_bps = slippage_bps
        self.slippage_model_type = slippage_model_type

        self.risk_manager = RiskManager(settings)
        self.position_sizer = self._initialize_position_sizer(position_sizer_type)
        self.stop_manager = self._initialize_stop_manager(stop_manager_type)
        self.portfolio_constructor = self._initialize_portfolio_constructor(portfolio_constructor_type, asset_allocator_type)
        
        self.portfolio_history: List[Dict[str, Any]] = []
        self.trades_log: List[Trade] = []
        self.current_cash = initial_capital
        self.active_trades: Dict[str, Trade] = {}
        self.trade_id_counter = 0

        logger.info(
            f"BacktestingEngine initialized for strategy '{self.strategy.name}'. "
            f"Slippage model: {self.slippage_model_type}. "
            f"Position Sizer: {self.position_sizer.__class__.__name__}. "
            f"Stop Manager: {self.stop_manager.__class__.__name__}."
        )

    def _initialize_position_sizer(self, sizer_type: Optional[str]) -> BasePositionSizer:
        params = settings.get_strategy_params('RiskManagement')
        if sizer_type is None:
            sizer_type = settings.position_sizer_type

        if sizer_type == 'FixedFractional':
            return FixedFractionalPositionSizer(params=params)
        elif sizer_type == 'ATRBased':
            return ATRBasedPositionSizer(params=params)
        elif sizer_type == 'OptimalF':
            return OptimalFPositionSizer(params=params)
        elif sizer_type == 'MLConfidence':
            return MLConfidencePositionSizer(params=params)
        else:
            logger.error(f"Unknown position sizer type: {sizer_type}. Defaulting to FixedFractional.")
            return FixedFractionalPositionSizer(params=params)

    def _initialize_stop_manager(self, stop_type: Optional[str]) -> BaseStopManager:
        params = settings.get_strategy_params('RiskManagement')
        if stop_type is None:
            stop_type = settings.stop_manager_type

        if stop_type == 'PercentageBased':
            return PercentageStopManager(params=params)
        elif stop_type == 'ATRBased':
            return ATRStopManager(params=params)
        elif stop_type == 'ParabolicSAR':
            return ParabolicSARStop(params=params)
        elif stop_type == 'TripleBarrier':
            return TripleBarrierStop(params=params)
        else:
            logger.error(f"Unknown stop manager type: {stop_type}. Defaulting to PercentageBased.")
            return PercentageStopManager(params=params)

    def _initialize_portfolio_constructor(self, constructor_type: Optional[str], allocator_type: Optional[str]) -> BasePortfolioConstructor:
        if allocator_type is None:
            allocator_type = settings.asset_allocator_type
        
        asset_allocator_instance: Optional[BaseAssetAllocator] = None
        portfolio_constructor_params = settings.get_strategy_params('PortfolioConstructor')

        if allocator_type == 'FixedWeight':
            default_symbol = settings.default_symbol
            clean_symbol_key = f'defaultallocation_{default_symbol.replace("/", "").replace(":", "").lower()}'
            default_allocation = portfolio_constructor_params.get(clean_symbol_key, 1.0)
            target_weights = {default_symbol: float(default_allocation)}
            asset_allocator_instance = FixedWeightAssetAllocator(target_weights=target_weights, params=portfolio_constructor_params)
        elif allocator_type == 'OptimalF':
            asset_allocator_instance = OptimalFAllocator(params=portfolio_constructor_params)
        elif allocator_type == 'HRP':
            asset_allocator_instance = HRPAllocator(params=portfolio_constructor_params)
        else:
            logger.error(f"Unknown asset allocator type: {allocator_type}. Defaulting to FixedWeight.")
            target_weights = {settings.default_symbol: 1.0}
            asset_allocator_instance = FixedWeightAssetAllocator(target_weights=target_weights, params=portfolio_constructor_params)
        
        class ConcretePortfolioConstructor(BasePortfolioConstructor):
            def __init__(self, settings: Any, risk_manager: RiskManager, asset_allocator: BaseAssetAllocator):
                super().__init__(settings, risk_manager)
                self.asset_allocator = asset_allocator

            def calculate_target_allocations(self, current_portfolio: PortfolioSnapshot, market_data: pd.DataFrame, trades_log: pd.DataFrame) -> Dict[str, float]:
                assets_to_allocate = [settings.default_symbol]
                historical_data_for_allocator = {settings.default_symbol: market_data}
                
                capital_allocations = self.asset_allocator.allocate(
                    assets=assets_to_allocate,
                    portfolio_value=current_portfolio.total_value_usd,
                    historical_data=historical_data_for_allocator,
                    trade_history=trades_log
                )
                
                if current_portfolio.total_value_usd <= 0: return {s: 0.0 for s in assets_to_allocate}
                
                return {asset: capital / current_portfolio.total_value_usd for asset, capital in capital_allocations.items()}

        return ConcretePortfolioConstructor(settings, self.risk_manager, asset_allocator_instance)

    def _get_next_trade_id(self) -> str:
        self.trade_id_counter += 1
        return f"trade_{self.trade_id_counter:05d}"

    def _apply_slippage(self, price: float, side: OrderSide, amount: float, latest_bar: BarData) -> float:
        if self.slippage_model_type == 'fixed':
            slippage_rate = self.slippage_bps / 10000.0
            filled_price = price * (1 + slippage_rate) if side == OrderSide.BUY else price * (1 - slippage_rate)
        elif self.slippage_model_type == 'volume_volatility_based':
            atr = getattr(latest_bar, 'atr', 0.0)
            volume = getattr(latest_bar, 'volume', 1.0)
            if price <= 0 or volume <= 0 or atr is None or atr <= 0:
                slippage_rate = self.slippage_bps / 10000.0
                return price * (1 + slippage_rate) if side == OrderSide.BUY else price * (1 - slippage_rate)

            base_slippage_pct = settings.BASE_SLIPPAGE_BPS / 10000.0
            avg_daily_volume_proxy = volume * settings.AVERAGE_DAILY_VOLUME_FACTOR
            impact_component = (amount / (avg_daily_volume_proxy + 1e-9)) 
            volatility_component = (atr / price)
            dynamic_slippage_pct = impact_component * volatility_component * settings.VOLATILITY_SLIPPAGE_FACTOR
            
            total_slippage_pct = min(base_slippage_pct + dynamic_slippage_pct, 0.1)

            filled_price = price * (1 + total_slippage_pct) if side == OrderSide.BUY else price * (1 - total_slippage_pct)
        else:
            raise NotImplementedError(f"Slippage model type '{self.slippage_model_type}' not implemented.")
        
        return filled_price if filled_price > 0 else price * 0.0001

    def _execute_trade_command(self, command: SignalCommand, current_bar: BarData, bar_index: int):
        trade_symbol = command.symbol
        execution_price_ideal = command.price if command.price else current_bar.close
        active_trade = self.active_trades.get(trade_symbol)
        
        strategy_info = {}
        if hasattr(self.strategy, 'get_latest_prediction_info') and callable(self.strategy.get_latest_prediction_info):
            strategy_info = self.strategy.get_latest_prediction_info() or {}

        if command.signal_type in [SignalType.LONG, SignalType.SHORT] and active_trade is None:
            side = OrderSide.BUY if command.signal_type == SignalType.LONG else OrderSide.SELL
            position_size = self.position_sizer.calculate_size(
                symbol=trade_symbol,
                current_price=execution_price_ideal,
                available_capital=self.current_cash,
                current_portfolio_value=self.portfolio_history[-1]['total_value_usd'],
                trade_signal=command.signal_type,
                strategy_info=strategy_info,
                latest_bar=current_bar,
                atr_value=getattr(current_bar, 'atr', None)
            )
            # FIX: Robustly check for invalid size (None, NaN, or zero)
            if position_size is None or not np.isfinite(position_size) or position_size <= 1e-8:
                return

            execution_price = self._apply_slippage(execution_price_ideal, side, position_size, current_bar)
            cost = position_size * execution_price
            commission = cost * self.commission_rate
            
            if side == OrderSide.BUY and cost + commission > self.current_cash:
                position_size = self.current_cash / (execution_price * (1 + self.commission_rate))
                cost = position_size * execution_price
                commission = cost * self.commission_rate

            self.current_cash -= commission
            if side == OrderSide.BUY: self.current_cash -= cost
            else: self.current_cash += cost

            new_trade = Trade(
                id=self._get_next_trade_id(), symbol=trade_symbol, entry_order_id=f"entry_{self.trade_id_counter}",
                side=side, entry_price=execution_price, amount=position_size, entry_timestamp=current_bar.timestamp,
                commission=commission, custom_fields={"atr_at_entry": getattr(current_bar, 'atr', None)}
            )
            self.active_trades[trade_symbol] = new_trade
            
            if isinstance(self.stop_manager, TripleBarrierStop):
                self.stop_manager.calculate_barriers(new_trade, current_bar)

        elif command.signal_type in [SignalType.CLOSE_LONG, SignalType.CLOSE_SHORT] and active_trade:
            if (active_trade.side == OrderSide.BUY and command.signal_type == SignalType.CLOSE_LONG) or \
               (active_trade.side == OrderSide.SELL and command.signal_type == SignalType.CLOSE_SHORT):
                
                exit_side = OrderSide.SELL if active_trade.side == OrderSide.BUY else OrderSide.BUY
                execution_price = self._apply_slippage(execution_price_ideal, exit_side, active_trade.amount, current_bar)
                exit_value = active_trade.amount * execution_price
                commission = exit_value * self.commission_rate
                
                if active_trade.side == OrderSide.BUY:
                    pnl = (execution_price - active_trade.entry_price) * active_trade.amount
                    self.current_cash += exit_value
                else:
                    pnl = (active_trade.entry_price - execution_price) * active_trade.amount
                    self.current_cash -= exit_value
                
                pnl -= (active_trade.commission + commission)
                self.current_cash -= commission
                
                self._log_and_clear_active_trade(trade_symbol, current_bar.timestamp, execution_price, pnl, commission, "SignalClose")

    def _log_and_clear_active_trade(self, symbol: str, timestamp: datetime, exit_price: float, pnl: float, exit_commission: float, reason: str):
        trade = self.active_trades.pop(symbol, None)
        if not trade: return
        trade.exit_price = exit_price
        trade.exit_timestamp = timestamp
        trade.pnl = pnl
        trade.commission += exit_commission
        trade.result = TradeResult.WIN if pnl > 0 else (TradeResult.LOSS if pnl < 0 else TradeResult.BREAKEVEN)
        trade.notes = reason
        self.trades_log.append(trade)
        if isinstance(self.stop_manager, TripleBarrierStop):
            self.stop_manager.reset_for_trade(trade.id)

    def _handle_stop_take_profit(self, current_bar: BarData, bar_index: int):
        for symbol, trade in list(self.active_trades.items()):
            stop_loss_price = self.stop_manager.check_stop_loss(trade, current_bar, bar_index, data_history_for_sar=self.prepared_data_feed.iloc[:bar_index+1])
            take_profit_price = self.stop_manager.check_take_profit(trade, current_bar)
            
            triggered_price = None
            trigger_reason = ""
            if stop_loss_price is not None:
                triggered_price = stop_loss_price
                trigger_reason = "StopLoss"
            elif take_profit_price is not None:
                triggered_price = take_profit_price
                trigger_reason = "TakeProfit"

            if triggered_price is not None:
                exit_value = trade.amount * triggered_price
                commission = exit_value * self.commission_rate
                if trade.side == OrderSide.BUY:
                    pnl = (triggered_price - trade.entry_price) * trade.amount
                    self.current_cash += exit_value
                else:
                    pnl = (trade.entry_price - triggered_price) * trade.amount
                    self.current_cash -= exit_value
                pnl -= (trade.commission + commission)
                self.current_cash -= commission
                self._log_and_clear_active_trade(symbol, current_bar.timestamp, triggered_price, pnl, commission, trigger_reason)

    def run(self) -> tuple[List[Trade], Dict[str, Any], pd.DataFrame]:
        logger.info(f"Starting backtest run for strategy '{self.strategy.name}'...")
        self.portfolio_history = [{"timestamp": self.prepared_data_feed.index[0] - pd.Timedelta(seconds=1), "total_value_usd": self.initial_capital}]

        for bar_index, (timestamp, row) in enumerate(self.prepared_data_feed.iterrows()):
            row_dict = row.to_dict()
            if 'market_regime' in row_dict and pd.isna(row_dict['market_regime']):
                row_dict['market_regime'] = None
            current_bar_data = BarData(timestamp=timestamp.to_pydatetime().replace(tzinfo=timezone.utc), **row_dict)
            
            mv_longs = sum(trade.amount * current_bar_data.close for trade in self.active_trades.values() if trade.side == OrderSide.BUY)
            mv_shorts = sum(trade.amount * current_bar_data.close for trade in self.active_trades.values() if trade.side == OrderSide.SELL)
            mtm_value = self.current_cash + mv_longs - mv_shorts

            self.portfolio_history.append({"timestamp": timestamp, "total_value_usd": mtm_value})
            equity_curve_df = pd.DataFrame(self.portfolio_history).set_index('timestamp')
            
            self.risk_manager.update_portfolio_metrics(equity_curve_df, timestamp)
            if self.risk_manager.check_portfolio_drawdown():
                for sym_to_close, trade_to_close in list(self.active_trades.items()):
                    close_signal = SignalType.CLOSE_LONG if trade_to_close.side == OrderSide.BUY else SignalType.CLOSE_SHORT
                    self._execute_trade_command(SignalCommand(signal_type=close_signal, symbol=sym_to_close), current_bar_data, bar_index)
                continue

            self._handle_stop_take_profit(current_bar_data, bar_index)
            
            if self.risk_manager.is_trading_halted():
                continue

            strategy_output = self.strategy.on_bar_data(current_bar_data)
            if strategy_output:
                commands = strategy_output if isinstance(strategy_output, list) else [SignalCommand(signal_type=strategy_output, symbol=self.strategy.symbol)]
                for command in commands:
                    if command.signal_type != SignalType.HOLD:
                        self._execute_trade_command(command, current_bar_data, bar_index)
                        
        final_portfolio_state = {"initial_capital": self.initial_capital, "final_portfolio_value": self.portfolio_history[-1]['total_value_usd']}
        equity_curve_df_final = pd.DataFrame(self.portfolio_history).set_index('timestamp')
        return self.trades_log, final_portfolio_state, equity_curve_df_final
</code>

kamikaze_komodo/exchange_interaction/__init__.py:
<code>
# kamikaze_komodo/exchange_interaction/__init__.py
# This file makes the 'exchange_interaction' directory a Python package.
</code>

kamikaze_komodo/exchange_interaction/exchange_api.py:
<code>
# kamikaze_komodo/exchange_interaction/exchange_api.py
import ccxt.async_support as ccxt
import asyncio
from typing import Dict, Optional, List
from kamikaze_komodo.core.enums import OrderType, OrderSide
from kamikaze_komodo.core.models import Order
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Import global settings
from datetime import datetime, timezone # Ensure timezone is imported

logger = get_logger(__name__)

class ExchangeAPI:
    """
    Handles interactions with the cryptocurrency exchange.
    Manages order placement, cancellation, and fetching account information.
    Phase 6: Added explicit check for short selling capability (though CCXT often handles this implicitly for derivative exchanges).
    """
    def __init__(self, exchange_id: Optional[str] = None): # exchange_id is now optional
        if not settings:
            logger.critical("Settings not loaded. ExchangeAPI cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.exchange_id = exchange_id if exchange_id else settings.exchange_id_to_use
        exchange_class = getattr(ccxt, self.exchange_id, None)

        if not exchange_class:
            logger.error(f"Exchange {self.exchange_id} is not supported by CCXT.")
            raise ValueError(f"Exchange {self.exchange_id} is not supported by CCXT.")

        # Determine API keys based on the exchange_id
        # This example assumes a single set of keys in settings (e.g., KRAKEN_API)
        # For a multi-exchange system, you'd fetch keys specific to self.exchange_id
        api_key = settings.kraken_api_key # Defaulting to Kraken keys for now
        secret_key = settings.kraken_secret_key # Defaulting to Kraken keys
        use_testnet = settings.kraken_testnet # Defaulting to Kraken testnet setting

        # Example for specific exchange key loading (if settings were structured differently)
        # if self.exchange_id == 'binance':
        #     api_key = settings.binance_api_key
        #     secret_key = settings.binance_secret_key
        #     use_testnet = settings.binance_testnet
        # elif self.exchange_id == 'krakenfutures':
        #     api_key = settings.kraken_futures_api_key # etc.

        config = {
            'apiKey': api_key,
            'secret': secret_key,
            'enableRateLimit': True,
        }
        self.exchange = exchange_class(config)
        logger.info(f"Initialized ExchangeAPI for {self.exchange_id}.")

        if use_testnet:
            if hasattr(self.exchange, 'set_sandbox_mode') and callable(self.exchange.set_sandbox_mode):
                try:
                    self.exchange.set_sandbox_mode(True)
                    logger.info(f"Sandbox mode enabled for {self.exchange_id}.")
                except ccxt.NotSupported:
                    logger.warning(f"{self.exchange_id} does not support unified set_sandbox_mode. Testnet functionality depends on API keys/URL.")
                except Exception as e_sandbox:
                    logger.error(f"Error setting sandbox mode for {self.exchange_id}: {e_sandbox}")
            else:
                logger.warning(f"{self.exchange_id} does not have set_sandbox_mode. Testnet relies on specific API keys or default URL pointing to sandbox.")
        else:
            logger.info(f"Running in live mode for {self.exchange_id}.")

        if not api_key or "YOUR_API_KEY" in str(api_key).upper() or (isinstance(api_key, str) and "D27PYGI95TLS" in api_key.upper()): # Check specific placeholder
            logger.warning(f"API key for {self.exchange_id} appears to be a placeholder or is not configured. Authenticated calls may fail.")

    async def fetch_balance(self) -> Optional[Dict]:
        if not self.exchange.has['fetchBalance']:
            logger.error(f"{self.exchange_id} does not support fetchBalance.")
            return None
        try:
            balance = await self.exchange.fetch_balance()
            logger.info(f"Successfully fetched balance from {self.exchange_id}.")
            return balance
        except ccxt.NetworkError as e:
            logger.error(f"Network error fetching balance: {e}", exc_info=True)
        except ccxt.AuthenticationError as e_auth:
            logger.error(f"Authentication error fetching balance from {self.exchange_id}. Check API keys and permissions: {e_auth}", exc_info=True)
            return None # Explicitly return None on auth error
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error fetching balance: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred fetching balance: {e}", exc_info=True)
        return None

    async def create_order(
        self,
        symbol: str,
        order_type: OrderType,
        side: OrderSide,
        amount: float,
        price: Optional[float] = None,
        params: Optional[Dict] = None
    ) -> Optional[Order]:
        if order_type == OrderType.LIMIT and price is None:
            logger.error("Price must be specified for a LIMIT order.")
            return None

        if not self.exchange.has['createOrder']:
            logger.error(f"{self.exchange_id} does not support createOrder.")
            return None

        order_type_str = order_type.value
        side_str = side.value

        # Phase 6: Check for short selling specific capabilities (conceptual for CCXT futures)
        if side == OrderSide.SELL: # This could be opening a short or closing a long
            # For many futures exchanges, 'sell' with no existing position implies short.
            # CCXT often handles this implicitly. Some exchanges might need specific params for short.
            # e.g., params = {'reduceOnly': False} if it was to ensure opening a new position.
            # We assume for now that a simple SELL order will open a short if no long position exists.
            # If the exchange has explicit shorting methods (less common in CCXT unified API), that'd be different.
            logger.info(f"Preparing to place a SELL order for {symbol}. This may open a short position.")

        try:
            logger.info(f"Attempting to place {side_str} {order_type_str} order for {amount} {symbol} at price {price if price else 'market'} on {self.exchange_id}")
            
            # Check for placeholder API keys again before actual call
            is_placeholder_key = not self.exchange.apiKey or "YOUR_API_KEY" in self.exchange.apiKey.upper() or "D27PYGI95TLS" in self.exchange.apiKey.upper()
            if settings.kraken_testnet and is_placeholder_key : # Use general testnet flag
                logger.warning(f"Simulating order creation for {self.exchange_id} due to testnet mode and placeholder API keys.")
                simulated_order_id = f"sim_{self.exchange_id}_{ccxt.Exchange.uuid()}"
                return Order(
                    id=simulated_order_id,
                    symbol=symbol,
                    type=order_type,
                    side=side,
                    amount=amount,
                    price=price if order_type == OrderType.LIMIT else None,
                    timestamp=datetime.now(timezone.utc), # Use timezone.utc
                    status="open", # Simulate as open
                    exchange_id=simulated_order_id
                )

            exchange_order_response = await self.exchange.create_order(symbol, order_type_str, side_str, amount, price, params or {})
            logger.info(f"Successfully placed order on {self.exchange_id}. Order ID: {exchange_order_response.get('id')}")
            
            created_order = Order(
                id=str(exchange_order_response.get('id')),
                symbol=exchange_order_response.get('symbol'),
                type=OrderType(exchange_order_response.get('type', order_type_str).lower()),
                side=OrderSide(exchange_order_response.get('side', side_str).lower()),
                amount=float(exchange_order_response.get('amount', amount)),
                price=float(exchange_order_response['price']) if exchange_order_response.get('price') else None,
                timestamp=datetime.fromtimestamp(exchange_order_response['timestamp'] / 1000, tz=timezone.utc) if exchange_order_response.get('timestamp') else datetime.now(timezone.utc),
                status=exchange_order_response.get('status', 'open'),
                filled_amount=float(exchange_order_response.get('filled', 0.0)),
                average_fill_price=float(exchange_order_response.get('average')) if exchange_order_response.get('average') else None,
                exchange_id=str(exchange_order_response.get('id'))
            )
            return created_order

        except ccxt.InsufficientFunds as e:
            logger.error(f"Insufficient funds to place order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.InvalidOrder as e:
            logger.error(f"Invalid order parameters for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.AuthenticationError as e_auth:
            logger.error(f"Authentication error placing order for {symbol} on {self.exchange_id}. Check API keys: {e_auth}", exc_info=True)
        except ccxt.NetworkError as e:
            logger.error(f"Network error placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e: # Catch specific exchange errors before generic Exception
            logger.error(f"Exchange error placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        return None

    async def cancel_order(self, order_id: str, symbol: Optional[str] = None, params: Optional[Dict] = None) -> bool:
        if not self.exchange.has['cancelOrder']:
            logger.error(f"{self.exchange_id} does not support cancelOrder.")
            return False
        try:
            await self.exchange.cancel_order(order_id, symbol, params or {})
            logger.info(f"Successfully requested cancellation for order ID {order_id} on {self.exchange_id}.")
            return True
        except ccxt.OrderNotFound as e:
            logger.error(f"Order ID {order_id} not found for cancellation on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.NetworkError as e:
            logger.error(f"Network error canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        return False

    async def fetch_order(self, order_id: str, symbol: Optional[str] = None) -> Optional[Order]:
        if not self.exchange.has['fetchOrder']:
            logger.warning(f"{self.exchange_id} does not support fetching individual orders directly.")
            return None
        try:
            exchange_order_response = await self.exchange.fetch_order(order_id, symbol)
            fetched_order = Order(
                id=str(exchange_order_response.get('id')),
                symbol=exchange_order_response.get('symbol'),
                type=OrderType(exchange_order_response.get('type').lower()),
                side=OrderSide(exchange_order_response.get('side').lower()),
                amount=float(exchange_order_response.get('amount')),
                price=float(exchange_order_response['price']) if exchange_order_response.get('price') else None,
                timestamp=datetime.fromtimestamp(exchange_order_response['timestamp'] / 1000, tz=timezone.utc) if exchange_order_response.get('timestamp') else datetime.now(timezone.utc),
                status=exchange_order_response.get('status'),
                filled_amount=float(exchange_order_response.get('filled', 0.0)),
                average_fill_price=float(exchange_order_response.get('average')) if exchange_order_response.get('average') else None,
                exchange_id=str(exchange_order_response.get('id'))
            )
            return fetched_order
        except ccxt.OrderNotFound:
            logger.warning(f"Order {order_id} not found on {self.exchange_id}.")
        except Exception as e:
            logger.error(f"Error fetching order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        return None

    async def fetch_open_orders(self, symbol: Optional[str] = None, since: Optional[datetime] = None, limit: Optional[int] = None) -> List[Order]: # Changed since to datetime
        open_orders_list = []
        if not self.exchange.has['fetchOpenOrders']:
            logger.warning(f"{self.exchange_id} does not support fetching open orders.")
            return open_orders_list

        try:
            since_timestamp_ms = int(since.timestamp() * 1000) if since else None
            raw_orders = await self.exchange.fetch_open_orders(symbol, since_timestamp_ms, limit)
            for ex_order in raw_orders:
                order = Order(
                    id=str(ex_order.get('id')),
                    symbol=ex_order.get('symbol'),
                    type=OrderType(ex_order.get('type').lower()),
                    side=OrderSide(ex_order.get('side').lower()),
                    amount=float(ex_order.get('amount')),
                    price=float(ex_order['price']) if ex_order.get('price') else None,
                    timestamp=datetime.fromtimestamp(ex_order['timestamp'] / 1000, tz=timezone.utc) if ex_order.get('timestamp') else datetime.now(timezone.utc),
                    status=ex_order.get('status', 'open'),
                    filled_amount=float(ex_order.get('filled', 0.0)),
                    average_fill_price=float(ex_order.get('average')) if ex_order.get('average') else None,
                    exchange_id=str(ex_order.get('id'))
                )
                open_orders_list.append(order)
            logger.info(f"Fetched {len(open_orders_list)} open orders for symbol {symbol if symbol else 'all'} on {self.exchange_id}.")
        except Exception as e:
            logger.error(f"Error fetching open orders on {self.exchange_id}: {e}", exc_info=True)
        return open_orders_list

    async def close(self):
        try:
            if hasattr(self.exchange, 'close') and callable(self.exchange.close):
                await self.exchange.close()
                logger.info(f"CCXT exchange connection for {self.exchange_id} closed.")
        except Exception as e:
            logger.error(f"Error closing CCXT exchange connection for {self.exchange_id}: {e}", exc_info=True)
</code>

kamikaze_komodo/data_handling/__init__.py:
<code>
# kamikaze_komodo/data_handling/__init__.py
# This file makes the 'data_handling' directory a Python package.
</code>

kamikaze_komodo/data_handling/data_handler.py:
<code>
# FILE: kamikaze_komodo/data_handling/data_handler.py
import pandas as pd
from datetime import datetime
from typing import Optional, List
import os

from .data_fetcher import DataFetcher
from .database_manager import DatabaseManager
from ..core.models import BarData
from ..app_logger import get_logger
from ..config.settings import settings

logger = get_logger(__name__)

class DataHandler:
    """
    Handles fetching, preparation, and enrichment of market data for backtesting and live trading.
    Encapsulates logic for merging different data sources like funding rates and sentiment,
    preventing data collision issues in the main application logic.
    """

    def __init__(self):
        self.fetcher = DataFetcher()
        self.db_manager = DatabaseManager()

    async def get_prepared_data(
        self,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime,
        needs_funding_rate: bool = False,
        needs_sentiment: bool = False
    ) -> pd.DataFrame:
        """
        Fetches, merges, and prepares data, returning a clean DataFrame with a DatetimeIndex.
        This is the primary method to get data for any backtest or analysis.
        """
        # 1. Fetch base OHLCV data
        bars = self.db_manager.retrieve_bar_data(symbol, timeframe, start_date, end_date)
        if not bars:
            logger.info(f"No data in DB for {symbol}/{timeframe}, fetching from exchange...")
            bars = await self.fetcher.fetch_historical_data_for_period(symbol, timeframe, start_date, end_date)
            if bars:
                self.db_manager.store_bar_data(bars)
        
        if not bars:
            logger.error(f"Could not retrieve or fetch any data for {symbol}/{timeframe}.")
            return pd.DataFrame()

        data_df = pd.DataFrame([bar.model_dump() for bar in bars])
        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
        data_df.sort_values('timestamp', inplace=True)

        # 2. Add Funding Rate data if required
        if needs_funding_rate:
            if 'funding_rate' in data_df.columns:
                data_df = data_df.drop(columns=['funding_rate'])
            
            logger.info(f"Fetching funding rates for {symbol}...")
            funding_rates_raw = await self.fetcher.fetch_funding_rate_history(symbol, since=start_date)
            if funding_rates_raw:
                fr_df = pd.DataFrame(funding_rates_raw)
                fr_df['timestamp'] = pd.to_datetime(fr_df['timestamp'], unit='ms', utc=True)
                fr_df = fr_df[['timestamp', 'fundingRate']].sort_values('timestamp')
                
                data_df = pd.merge_asof(
                    left=data_df,
                    right=fr_df,
                    on='timestamp',
                    direction='backward'
                )
                data_df.rename(columns={'fundingRate': 'funding_rate'}, inplace=True)
                logger.info("Funding rates merged.")
            else:
                logger.warning(f"Could not fetch funding rates for {symbol}. Column will be filled with 0.0.")
                data_df['funding_rate'] = 0.0
        
        # 3. Add Sentiment data if required
        if needs_sentiment:
            if 'sentiment_score' in data_df.columns:
                data_df = data_df.drop(columns=['sentiment_score'])
            
            if settings and settings.simulated_sentiment_data_path and os.path.exists(settings.simulated_sentiment_data_path):
                sentiment_df = pd.read_csv(settings.simulated_sentiment_data_path, parse_dates=['timestamp'])
                
                data_df = pd.merge_asof(
                    left=data_df.sort_values('timestamp'),
                    right=sentiment_df[['timestamp', 'sentiment_score']].sort_values('timestamp'),
                    on='timestamp',
                    direction='backward'
                )
                logger.info("Sentiment data merged.")
            else:
                logger.warning("Simulated sentiment data path not found. 'sentiment_score' column will be filled with 0.0.")
                data_df['sentiment_score'] = 0.0
        
        # 4. Final Processing and Cleanup
        data_df.set_index('timestamp', inplace=True)

        # Fill NaNs for columns that might have been added
        if 'funding_rate' in data_df.columns:
            data_df['funding_rate'] = data_df['funding_rate'].ffill().fillna(0.0)
        
        if 'sentiment_score' in data_df.columns:
            data_df['sentiment_score'] = data_df['sentiment_score'].ffill().fillna(0.0)

        # Ensure all core columns exist
        core_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in core_cols:
            if col not in data_df.columns:
                logger.error(f"Core column '{col}' is missing from the final DataFrame.")
                return pd.DataFrame()
        
        logger.info(f"Prepared data for {symbol}/{timeframe} with {len(data_df)} bars.")
        return data_df

    async def close(self):
        """Closes underlying connections."""
        await self.fetcher.close()
        self.db_manager.close()
</code>

kamikaze_komodo/data_handling/database_manager.py:
<code>
# kamikaze_komodo/data_handling/database_manager.py
import sqlite3
from typing import List, Optional, Dict, Any
from kamikaze_komodo.core.models import BarData, NewsArticle
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone, UTC 
import json

logger = get_logger(__name__)

class DatabaseManager:
    """
    Manages local storage of core data (OHLCV, News).
    Indicators are not stored here; they are calculated on-the-fly for backtests.
    """
    def __init__(self, db_name: str = "kamikaze_komodo_data.db"):
        self.db_name = db_name
        self.conn: Optional[sqlite3.Connection] = None
        self._connect()
        self._create_tables()

    def _connect(self):
        try:
            self.conn = sqlite3.connect(self.db_name, detect_types=sqlite3.PARSE_COLNAMES)
            self.conn.row_factory = sqlite3.Row 
            logger.info(f"Successfully connected to database: {self.db_name}")
        except sqlite3.Error as e:
            logger.error(f"Error connecting to database {self.db_name}: {e}")
            self.conn = None

    def _create_tables(self):
        if not self.conn:
            logger.error("Cannot create tables, no database connection.")
            return
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS bar_data (
                    timestamp TEXT NOT NULL, 
                    symbol TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    open REAL NOT NULL,
                    high REAL NOT NULL,
                    low REAL NOT NULL,
                    close REAL NOT NULL,
                    volume REAL NOT NULL,
                    funding_rate REAL,
                    PRIMARY KEY (timestamp, symbol, timeframe)
                )
            """)
            self.conn.commit()
            logger.info("Core tables checked/created successfully.")
        except sqlite3.Error as e:
            logger.error(f"Error creating tables: {e}")

    def _to_iso_format(self, dt: Optional[datetime]) -> Optional[str]:
        if dt is None: return None
        if dt.tzinfo is None: dt = dt.replace(tzinfo=UTC)
        else: dt = dt.astimezone(UTC)
        return dt.isoformat()

    def _from_iso_format(self, iso_str: Optional[str]) -> Optional[datetime]:
        if iso_str is None: return None
        try:
            dt = datetime.fromisoformat(iso_str)
            if dt.tzinfo is None: return dt.replace(tzinfo=UTC)
            return dt.astimezone(UTC)
        except ValueError:
            return None

    def store_bar_data(self, bar_data_list: List[BarData]):
        if not self.conn: return
        if not bar_data_list: return
        try:
            cursor = self.conn.cursor()
            data_to_insert = [
                (
                    self._to_iso_format(bd.timestamp), bd.symbol, bd.timeframe,
                    bd.open, bd.high, bd.low, bd.close, bd.volume,
                    bd.funding_rate
                ) for bd in bar_data_list
            ]
            cursor.executemany("""
                INSERT OR REPLACE INTO bar_data 
                (timestamp, symbol, timeframe, open, high, low, close, volume, funding_rate)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?) 
            """, data_to_insert) 
            self.conn.commit()
            logger.info(f"Stored/Replaced {len(data_to_insert)} bar data entries.")
        except Exception as e:
            logger.error(f"Error storing bar data: {e}", exc_info=True)

    def retrieve_bar_data(self, symbol: str, timeframe: str, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> List[BarData]:
        if not self.conn: return []
        try:
            cursor = self.conn.cursor()
            query = "SELECT * FROM bar_data WHERE symbol = ? AND timeframe = ?"
            params = [symbol, timeframe]
            if start_date: query += " AND timestamp >= ?"; params.append(self._to_iso_format(start_date))
            if end_date: query += " AND timestamp <= ?"; params.append(self._to_iso_format(end_date))
            query += " ORDER BY timestamp ASC"

            cursor.execute(query, tuple(params))
            rows = cursor.fetchall()
            return [
                BarData(
                    timestamp=self._from_iso_format(row['timestamp']),
                    **{k: row[k] for k in row.keys() if k != 'timestamp'}
                ) for row in rows if self._from_iso_format(row['timestamp'])
            ]
        except Exception as e:
            logger.error(f"Error retrieving bar data: {e}", exc_info=True)
            return []

    def close(self):
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed.")
            self.conn = None

    def __del__(self):
        self.close()
</code>

kamikaze_komodo/data_handling/data_fetcher.py:
<code>
# kamikaze_komodo/data_handling/data_fetcher.py
import ccxt.async_support as ccxt # Use async version for future compatibility
import asyncio
from typing import List, Optional, Tuple, Dict
from datetime import datetime, timedelta, timezone
# Assuming these are correctly located relative to this file for your project structure
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.core.utils import ohlcv_to_bardata
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Ensure settings is loaded globally

logger = get_logger(__name__)

class DataFetcher:
    """
    Fetches historical and real-time market data using CCXT.
    Phase 6: Added fetch_historical_data_for_pair for pair trading strategies.
    """
    def __init__(self): # MODIFIED: No longer takes exchange_id as an argument
        if not settings:
            logger.critical("Settings not loaded. DataFetcher cannot be initialized.")
            raise ValueError("Settings not loaded. Ensure config files are present and correct.")

        self.exchange_id = settings.exchange_id_to_use # MODIFIED: Get from global settings
        exchange_class = getattr(ccxt, self.exchange_id, None)
        
        if not exchange_class:
            logger.error(f"Exchange '{self.exchange_id}' is not supported by CCXT.")
            raise ValueError(f"Exchange '{self.exchange_id}' is not supported by CCXT.")

        # API keys should be specific to the selected exchange_id 
        # (e.g., Kraken Spot keys for 'kraken', Kraken Futures Demo keys for 'krakenfutures')
        config = {
            'apiKey': settings.kraken_api_key, # This assumes kraken_api_key holds the relevant key
            'secret': settings.kraken_secret_key, # This assumes kraken_secret_key holds the relevant secret
            'enableRateLimit': True, # Recommended by CCXT
        }
        
        # Example: If your settings had distinct keys for different exchanges:
        # if self.exchange_id == 'krakenfutures':
        #     config['apiKey'] = settings.kraken_futures_api_key 
        #     config['secret'] = settings.kraken_futures_secret_key
        # elif self.exchange_id == 'kraken':
        #     config['apiKey'] = settings.kraken_spot_api_key
        #     config['secret'] = settings.kraken_spot_secret_key
        # For now, we use the general kraken_api_key/secret from settings.

        self.exchange = exchange_class(config)
        logger.info(f"Instantiated CCXT exchange class: {self.exchange_id}")

        if settings.kraken_testnet: # This flag now controls sandbox mode for the selected exchange
            if hasattr(self.exchange, 'set_sandbox_mode') and callable(self.exchange.set_sandbox_mode):
                try:
                    self.exchange.set_sandbox_mode(True)
                    logger.info(f"CCXT sandbox mode successfully enabled for {self.exchange_id}.")
                    # You can log the API URL to verify it changed, e.g.:
                    # logger.info(f"Using API URLs: {self.exchange.urls['api']}")
                except ccxt.NotSupported:
                    logger.warning(f"{self.exchange_id} does not support unified set_sandbox_mode via method. Testnet functionality might depend on specific API keys or default URLs for this exchange class.")
                except Exception as e:
                    logger.error(f"An error occurred while trying to set sandbox mode for {self.exchange_id}: {e}", exc_info=True)
            else:
                logger.warning(f"{self.exchange_id} CCXT class does not have a 'set_sandbox_mode' method. Testnet operation relies on correct API keys for the test environment and default URLs.")
        
        self.exchange.verbose = False # Set to True for debugging API calls
        logger.info(f"Initialized DataFetcher for '{self.exchange_id}'. Configured Testnet (Sandbox) from settings: {settings.kraken_testnet}")

    async def fetch_historical_ohlcv(
        self,
        symbol: str,
        timeframe: str,
        since: Optional[datetime] = None,
        limit: Optional[int] = None,
        params: Optional[dict] = None
    ) -> List[BarData]:
        if not self.exchange.has['fetchOHLCV']:
            logger.error(f"{self.exchange_id} does not support fetchOHLCV.")
            # await self.close() # Closing here might be premature if other operations are pending
            return []

        since_timestamp_ms = None
        if since:
            if since.tzinfo is None: 
                since = since.replace(tzinfo=timezone.utc)
            since_timestamp_ms = int(since.timestamp() * 1000)

        ohlcv_data_list: List[BarData] = []
        try:
            logger.info(f"Fetching historical OHLCV for {symbol} ({timeframe}) from exchange {self.exchange_id} since {since} with limit {limit}")
            raw_ohlcv = await self.exchange.fetch_ohlcv(symbol, timeframe, since_timestamp_ms, limit, params or {})            
            # More robust check for raw_ohlcv
            if raw_ohlcv is not None and isinstance(raw_ohlcv, list):
                if not raw_ohlcv: # Empty list
                    logger.info(f"No OHLCV data returned (empty list) for {symbol} ({timeframe}) from {self.exchange_id} with the given parameters.")
                else:
                    for entry in raw_ohlcv:
                        try:
                            bar = ohlcv_to_bardata(entry, symbol, timeframe)
                            ohlcv_data_list.append(bar)
                        except ValueError as e_bar:
                            logger.warning(f"Skipping invalid OHLCV entry for {symbol} ({timeframe}): {entry}. Error: {e_bar}")
                    logger.info(f"Successfully fetched {len(ohlcv_data_list)} candles for {symbol} ({timeframe}) from {self.exchange_id}.")
            elif raw_ohlcv is None:
                logger.info(f"No OHLCV data returned (got None) for {symbol} ({timeframe}) from {self.exchange_id} with the given parameters.")
            else: # It's something else, not None and not a list
                logger.warning(f"Unexpected data type received for OHLCV for {symbol} ({timeframe}) from {self.exchange_id}: {type(raw_ohlcv)}. Data: {str(raw_ohlcv)[:200]}")
        except ccxt.NetworkError as e:
            logger.error(f"Network error fetching OHLCV for {symbol} from {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error fetching OHLCV for {symbol} from {self.exchange_id}: {e}", exc_info=True)
        except Exception as e: # Generic catch-all
            logger.error(f"An unexpected error occurred in fetch_historical_ohlcv for {symbol} from {self.exchange_id}: {e}", exc_info=True)
        
        return ohlcv_data_list

    async def fetch_historical_data_for_period(
        self,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime = datetime.now(timezone.utc)
    ) -> List[BarData]:
        all_bars: List[BarData] = []
        current_start_date = start_date
        
        try:
            timeframe_duration_seconds = self.exchange.parse_timeframe(timeframe)
        except Exception as e_tf:
            logger.error(f"Failed to parse timeframe '{timeframe}' using CCXT for {self.exchange_id}: {e_tf}")
            timeframe_duration_seconds = None # Fallback if parse_timeframe itself errors

        if timeframe_duration_seconds is None: # Still None after try-except
            logger.error(f"Could not parse timeframe: {timeframe} for {self.exchange_id}. Cannot paginate effectively. Attempting single fetch.")
            return await self.fetch_historical_ohlcv(symbol, timeframe, since=start_date, limit=1000) # Example limit

        logger.info(f"Fetching historical period data for {symbol} ({timeframe}) on {self.exchange_id} from {start_date} to {end_date}")

        while current_start_date < end_date:
            limit_per_call = 500 # Adjust as needed
            
            logger.debug(f"Fetching batch for {symbol} from {current_start_date} with limit {limit_per_call}")
            bars = await self.fetch_historical_ohlcv(symbol, timeframe, since=current_start_date, limit=limit_per_call)
            
            if not bars: # Includes None or empty list after fetch_historical_ohlcv's logging
                logger.info(f"No more data found for {symbol} ({timeframe}) starting {current_start_date}, or an error occurred during fetch.")
                break 
            
            # Filter bars that are strictly before the overall end_date
            # The timestamp from OHLCV is the start of the candle.
            # If a candle's start is >= end_date, we don't need it or subsequent ones.
            relevant_bars = [b for b in bars if b.timestamp < end_date]
            
            if not relevant_bars:
                if bars and bars[0].timestamp >= end_date: # First fetched bar is already past our period
                    logger.debug(f"First bar fetched ({bars[0].timestamp}) is already at or after end_date ({end_date}). Stopping pagination.")
                break # No relevant bars in this batch

            all_bars.extend(relevant_bars)
            
            # Move to the next period: start after the last fetched relevant candle
            last_fetched_timestamp = relevant_bars[-1].timestamp
            # To get the start of the *next* candle, add the timeframe duration
            current_start_date = last_fetched_timestamp + timedelta(seconds=timeframe_duration_seconds)
            
            if current_start_date >= end_date: # Optimization: if next fetch starts at or after end_date
                logger.debug("Next calculated start_date is at or after end_date. Concluding pagination.")
                break
            
            logger.debug(f"Fetched {len(relevant_bars)} relevant bars. Next fetch for {symbol} will start from {current_start_date}. Total collected: {len(all_bars)}")
            
            # Respect rate limits (ensure rateLimit is a number)
            if isinstance(self.exchange.rateLimit, (int, float)) and self.exchange.rateLimit > 0:
                await asyncio.sleep(self.exchange.rateLimit / 1000.0) 
            else:
                await asyncio.sleep(0.2) # Default small delay if rateLimit is not standard

        # Remove duplicates (if any from overlapping fetches, though logic above tries to avoid it) and sort
        if all_bars:
            unique_bars_dict = {bar.timestamp: bar for bar in all_bars}
            all_bars = sorted(list(unique_bars_dict.values()), key=lambda b: b.timestamp)
            logger.info(f"Total unique historical bars fetched for {symbol} ({timeframe}) in period: {len(all_bars)}")
        
        return all_bars

    async def fetch_historical_data_for_pair(
        self,
        symbol1: str,
        symbol2: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime = datetime.now(timezone.utc)
    ) -> Tuple[Optional[List[BarData]], Optional[List[BarData]]]:
        """
        Fetches historical data for two symbols forming a pair.
        Returns a tuple of (data_symbol1, data_symbol2).
        Data is attempted to be synchronized by timestamp, but perfect sync is not guaranteed
        if one asset has missing bars where the other doesn't.
        Further alignment might be needed in the strategy.
        """
        logger.info(f"Fetching historical data for pair: {symbol1} and {symbol2} ({timeframe}) from {start_date} to {end_date}")
        
        data_symbol1 = await self.fetch_historical_data_for_period(symbol1, timeframe, start_date, end_date)
        data_symbol2 = await self.fetch_historical_data_for_period(symbol2, timeframe, start_date, end_date)

        if not data_symbol1:
            logger.warning(f"No data fetched for {symbol1} in the pair.")
        if not data_symbol2:
            logger.warning(f"No data fetched for {symbol2} in the pair.")
        
        # Basic check for data presence
        if not data_symbol1 or not data_symbol2:
            logger.warning(f"Could not fetch data for one or both assets in the pair ({symbol1}, {symbol2}).")
            return None, None # Indicate failure to fetch for one or both

        # Strategies will need to handle potential misalignments or use pandas to merge/align.
        logger.info(f"Fetched {len(data_symbol1)} bars for {symbol1} and {len(data_symbol2)} bars for {symbol2} for pair trading.")
        return data_symbol1, data_symbol2

    async def fetch_funding_rate_history(
        self,
        symbol: str,
        since: Optional[datetime] = None,
        limit: Optional[int] = None
    ) -> List[Dict]:
        """
        Fetches historical funding rate data for a perpetual futures symbol.
        """
        if not self.exchange.has.get('fetchFundingRateHistory'):
            logger.error(f"{self.exchange_id} does not support fetchFundingRateHistory.")
            return []

        since_timestamp_ms = int(since.timestamp() * 1000) if since else None
        funding_rates = []
        try:
            logger.info(f"Fetching funding rate history for {symbol} from {self.exchange_id} since {since}.")
            funding_rates = await self.exchange.fetch_funding_rate_history(symbol, since_timestamp_ms, limit)
            logger.info(f"Successfully fetched {len(funding_rates)} funding rate entries for {symbol}.")
        except ccxt.NetworkError as e:
            logger.error(f"Network error fetching funding rates for {symbol}: {e}", exc_info=True)
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error fetching funding rates for {symbol}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred fetching funding rates for {symbol}: {e}", exc_info=True)
        return funding_rates


    async def subscribe_to_realtime_trades(self, symbol: str):
        # (Your existing placeholder code for this method)
        if not self.exchange.has['watchTrades']:
            logger.warning(f"{self.exchange_id} does not support real-time trade watching via WebSockets in CCXT.")
            # await self.close() # Consider if closing here is always appropriate
            return

        logger.info(f"Attempting to subscribe to real-time trades for {symbol} on {self.exchange_id}...")
        logger.warning("Real-time data subscription is a placeholder and not fully implemented.")
        pass

    async def close(self):
        """Closes the CCXT exchange connection."""
        try:
            if hasattr(self.exchange, 'close') and callable(self.exchange.close):
                await self.exchange.close()
            logger.info(f"CCXT exchange connection for {self.exchange_id} closed.")
        except Exception as e:
            logger.error(f"Error closing CCXT exchange connection for {self.exchange_id}: {e}", exc_info=True)
</code>

kamikaze_komodo/config/__init__.py:
<code>
# kamikaze_komodo/config/__init__.py
# This file makes the 'config' directory a Python package.
</code>

kamikaze_komodo/config/secrets.ini:
<code>
; kamikaze_komodo/config/secrets.ini
; This file should be in .gitignore and contain sensitive information.
[KRAKEN_API]
API_KEY = 'd27PYGi95tlsV4gVotVNXinHOTAxXY2usUta7kw3IogO9/9kpLHCHgcv'
SECRET_KEY = 'kB+i8be+l7J6Lr+RyjodrqNyQXrIn6reFeNfDsmMs01zsQg3KPGSSshd9l4KwvY92LQyYamDc1lMrHsnZ6+LaWQP'

[DATABASE]
User = db_user
Password = db_password
</code>

kamikaze_komodo/config/settings.py:
<code>
# FILE: kamikaze_komodo/config/settings.py
import configparser
import os
from kamikaze_komodo.app_logger import get_logger
from typing import Dict, List, Optional, Any

logger = get_logger(__name__)

# Define a single, reliable project root that other modules can import.
# This file is in .../kamikaze_komodo/config/, so two levels up is the project root.
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


class Config:
    """
    Manages application configuration using config.ini and secrets.ini.
    """
    def __init__(self, config_file_rel_path='config/config.ini', secrets_file_rel_path='config/secrets.ini'):
        self.config = configparser.ConfigParser()
        self.secrets = configparser.ConfigParser()

        self.config_file_path = os.path.join(PROJECT_ROOT, config_file_rel_path)
        self.secrets_file_path = os.path.join(PROJECT_ROOT, secrets_file_rel_path)

        if not os.path.exists(self.config_file_path):
            logger.error(f"Config file not found: {self.config_file_path}")
            raise FileNotFoundError(f"Config file not found: {self.config_file_path}")
        if not os.path.exists(self.secrets_file_path):
            logger.warning(f"Secrets file not found: {self.secrets_file_path}. Some features might be unavailable.")

        self.config.read(self.config_file_path)
        self.secrets.read(self.secrets_file_path)

        # General Settings
        self.log_level: str = self.config.get('General', 'LogLevel', fallback='INFO')
        self.log_file_path: str = self.config.get('General', 'LogFilePath', fallback='logs/kamikaze_komodo.log')

        # API Settings
        self.exchange_id_to_use: str = self.config.get('API', 'ExchangeID', fallback='krakenfutures')
        self.kraken_api_key: Optional[str] = self.secrets.get('KRAKEN_API', 'API_KEY', fallback=None)
        self.kraken_secret_key: Optional[str] = self.secrets.get('KRAKEN_API', 'SECRET_KEY', fallback=None)
        self.kraken_testnet: bool = self.config.getboolean('API', 'KrakenTestnet', fallback=True)

        # Data Fetching Settings
        self.default_symbol: str = self.config.get('DataFetching', 'DefaultSymbol', fallback='PF_XBTUSD')
        self.default_timeframe: str = self.config.get('DataFetching', 'DefaultTimeframe', fallback='4h')
        self.historical_data_days: int = self.config.getint('DataFetching', 'HistoricalDataDays', fallback=365)
        self.data_fetch_limit_per_call: int = self.config.getint('DataFetching', 'DataFetchLimitPerCall', fallback=500)

        # Trading Settings
        self.max_portfolio_risk: float = self.config.getfloat('Trading', 'MaxPortfolioRisk', fallback=0.02)
        self.default_leverage: float = self.config.getfloat('Trading', 'DefaultLeverage', fallback=1.0)
        self.commission_bps: float = self.config.getfloat('Trading', 'CommissionBPS', fallback=10.0)
        self.slippage_bps: float = self.config.getfloat('Trading', 'SlippageBPS', fallback=2.0)
        
        # Phase 1: Slippage and Precision Settings
        self.BASE_SLIPPAGE_BPS: float = self.config.getfloat('Trading', 'BASE_SLIPPAGE_BPS', fallback=1.0)
        self.AVERAGE_DAILY_VOLUME_FACTOR: float = self.config.getfloat('Trading', 'AVERAGE_DAILY_VOLUME_FACTOR', fallback=0.02)
        self.VOLATILITY_SLIPPAGE_FACTOR: float = self.config.getfloat('Trading', 'VOLATILITY_SLIPPAGE_FACTOR', fallback=0.1)
        self.MIN_TICK_SIZE: float = self.config.getfloat('Trading', 'MIN_TICK_SIZE', fallback=0.5)
        self.PRICE_PRECISION: int = self.config.getint('Trading', 'PRICE_PRECISION', fallback=1)

        # EWMAC Strategy Settings (Example, specific strategies below)
        self.ewmac_short_window: int = self.config.getint('EWMAC_Strategy', 'ShortWindow', fallback=12)
        self.ewmac_long_window: int = self.config.getint('EWMAC_Strategy', 'LongWindow', fallback=26)
        self.ewmac_signal_window: int = self.config.getint('EWMAC_Strategy', 'SignalWindow', fallback=9)
        self.ewmac_atr_period: int = self.config.getint('EWMAC_Strategy', 'atr_period', fallback=14)


        # --- Phase 2: Risk Management Settings (Updated & New) ---
        self.max_portfolio_drawdown_pct: float = self.config.getfloat('RiskManagement', 'MaxPortfolioDrawdownPct', fallback=0.20)
        
        self.position_sizer_type: str = self.config.get('RiskManagement', 'PositionSizer', fallback='FixedFractional')
        self.fixed_fractional_allocation_fraction: float = self.config.getfloat('RiskManagement', 'FixedFractional_AllocationFraction', fallback=0.10)
        self.atr_based_risk_per_trade_fraction: float = self.config.getfloat('RiskManagement', 'ATRBased_RiskPerTradeFraction', fallback=0.01)
        self.atr_based_atr_multiple_for_stop: float = self.config.getfloat('RiskManagement', 'ATRBased_ATRMultipleForStop', fallback=2.0)

        # New Position Sizer Params
        self.optimal_f_win_rate_estimate: float = self.config.getfloat('RiskManagement', 'OptimalF_WinRateEstimate', fallback=0.51)
        self.optimal_f_avg_win_loss_ratio_estimate: float = self.config.getfloat('RiskManagement', 'OptimalF_AvgWinLossRatioEstimate', fallback=1.1)
        self.optimal_f_kelly_fraction: float = self.config.getfloat('RiskManagement', 'OptimalF_KellyFraction', fallback=0.5)

        self.ml_confidence_min_size_factor: float = self.config.getfloat('RiskManagement', 'MLConfidence_MinSizeFactor', fallback=0.5)
        self.ml_confidence_max_size_factor: float = self.config.getfloat('RiskManagement', 'MLConfidence_MaxSizeFactor', fallback=1.5)
        self.ml_confidence_base_allocation_fraction: float = self.config.getfloat('RiskManagement', 'MLConfidence_BaseAllocationFraction', fallback=0.05)


        self.stop_manager_type: str = self.config.get('RiskManagement', 'StopManager_Default', fallback='PercentageBased')
        _sl_pct_str = self.config.get('RiskManagement', 'PercentageStop_LossPct', fallback='0.02')
        self.percentage_stop_loss_pct: Optional[float] = float(_sl_pct_str) if _sl_pct_str and _sl_pct_str.lower() not in ['none', '0', '0.0'] else None
        _tp_pct_str = self.config.get('RiskManagement', 'PercentageStop_TakeProfitPct', fallback='0.05')
        self.percentage_stop_take_profit_pct: Optional[float] = float(_tp_pct_str) if _tp_pct_str and _tp_pct_str.lower() not in ['none', '0', '0.0'] else None
        self.atr_stop_atr_multiple: float = self.config.getfloat('RiskManagement', 'ATRStop_ATRMultiple', fallback=2.0)

        # New Stop Manager Params
        self.parabolic_sar_acceleration_factor: float = self.config.getfloat('RiskManagement', 'ParabolicSAR_AccelerationFactor', fallback=0.02)
        self.parabolic_sar_max_acceleration: float = self.config.getfloat('RiskManagement', 'ParabolicSAR_MaxAcceleration', fallback=0.2)
        
        self.triple_barrier_profit_multiplier: float = self.config.getfloat('RiskManagement', 'TripleBarrier_ProfitMultiplier', fallback=1.5)
        self.triple_barrier_loss_multiplier: float = self.config.getfloat('RiskManagement', 'TripleBarrier_LossMultiplier', fallback=1.0)
        self.triple_barrier_time_limit_days: int = self.config.getint('RiskManagement', 'TripleBarrier_TimeLimitDays', fallback=10)

        # Existing VolatilityBandStopManager params
        self.volatility_band_stop_band_type: str = self.config.get('RiskManagement', 'VolatilityBandStop_BandType', fallback='bollinger')
        self.volatility_band_stop_bb_period: int = self.config.getint('RiskManagement', 'VolatilityBandStop_BB_Period', fallback=20)
        self.volatility_band_stop_bb_std_dev: float = self.config.getfloat('RiskManagement', 'VolatilityBandStop_BB_StdDev', fallback=2.0)
        self.volatility_band_stop_kc_period: int = self.config.getint('RiskManagement', 'VolatilityBandStop_KC_Period', fallback=20)
        self.volatility_band_stop_kc_atr_period: int = self.config.getint('RiskManagement', 'VolatilityBandStop_KC_ATR_Period', fallback=10)
        self.volatility_band_stop_kc_atr_multiplier: float = self.config.getfloat('RiskManagement', 'VolatilityBandStop_KC_ATR_Multiplier', fallback=1.5)
        self.volatility_band_stop_trail_type: str = self.config.get('RiskManagement', 'VolatilityBandStop_TrailType', fallback='none')

        # Pair Trading Sizer (existing)
        self.pair_trading_position_sizer_dollar_neutral: bool = self.config.getboolean('RiskManagement', 'PairTradingPositionSizer_DollarNeutral', fallback=True)


        # --- Phase 2: Portfolio Constructor Settings (Updated & New) ---
        self.portfolio_constructor_type: str = self.config.get('PortfolioConstructor', 'ConstructorType', fallback='Default') # Not used yet, but good to have
        self.asset_allocator_type: str = self.config.get('PortfolioConstructor', 'AssetAllocator', fallback='FixedWeight')
        default_symbol_config_key = f'DefaultAllocation_{self.default_symbol.replace("/", "").replace(":", "")}'
        self.default_allocation_for_symbol: float = self.config.getfloat('PortfolioConstructor', default_symbol_config_key, fallback=1.0)
        
        # New Rebalancing Triggers
        self.rebalance_threshold_pct: float = self.config.getfloat('PortfolioConstructor', 'Rebalance_Threshold_Pct', fallback=0.05)
        
        # New Volatility Targeting Parameters
        self.volatility_targeting_enable: bool = self.config.getboolean('PortfolioConstructor', 'Volatility_Targeting_Enable', fallback=False)
        self.target_portfolio_volatility: float = self.config.getfloat('PortfolioConstructor', 'Target_Portfolio_Volatility', fallback=0.15) # e.g., 15% annual vol target
        self.volatility_targeting_lookback_period: int = self.config.getint('PortfolioConstructor', 'Volatility_Targeting_Lookback_Period', fallback=60) # e.g., 60 bars


        # Optimal F Allocator (existing as BaseAssetAllocator, not specific here)
        self.optimalf_default_win_probability: float = self.config.getfloat('PortfolioConstructor', 'OptimalF_Default_Win_Probability', fallback=0.51)
        self.optimalf_default_payoff_ratio: float = self.config.getfloat('PortfolioConstructor', 'OptimalF_Default_Payoff_Ratio', fallback=1.1)
        self.optimalf_kelly_fraction: float = self.config.getfloat('PortfolioConstructor', 'OptimalF_Kelly_Fraction', fallback=0.25)


        # --- Phase 4: AI News Analysis Settings ---
        self.enable_sentiment_analysis: bool = self.config.getboolean('AI_NewsAnalysis', 'EnableSentimentAnalysis', fallback=True)
        self.sentiment_llm_provider: str = self.config.get('AI_NewsAnalysis', 'SentimentLLMProvider', fallback='VertexAI')
        self.browser_agent_llm_provider: str = self.config.get('AI_NewsAnalysis', 'BrowserAgent_LLMProvider', fallback='VertexAI')
        self.browser_agent_max_steps: int = self.config.getint('AI_NewsAnalysis', 'BrowserAgent_Max_Steps', fallback=20)


        self.sentiment_filter_threshold_long: float = self.config.getfloat('AI_NewsAnalysis', 'SentimentFilter_Threshold_Long', fallback=0.1)
        self.sentiment_filter_threshold_short: float = self.config.getfloat('AI_NewsAnalysis', 'SentimentFilter_Threshold_Short', fallback=-0.1)
        
        self.simulated_sentiment_data_path: Optional[str] = self.config.get('AI_NewsAnalysis', 'SimulatedSentimentDataPath', fallback=None)
        if self.simulated_sentiment_data_path and self.simulated_sentiment_data_path.lower() in ['none', '']:
                self.simulated_sentiment_data_path = None
        if self.simulated_sentiment_data_path and not os.path.isabs(self.simulated_sentiment_data_path):
            path_parts = self.simulated_sentiment_data_path.split(os.sep)
            if path_parts[0] == 'kamikaze_komodo':
                correct_relative_path = os.path.join(*path_parts[1:])
            else:
                correct_relative_path = self.simulated_sentiment_data_path
            self.simulated_sentiment_data_path = os.path.join(PROJECT_ROOT, correct_relative_path)


        self.news_scraper_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'NewsScraper_Enable', fallback=True)
        self.notification_listener_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'NotificationListener_Enable', fallback=False)
        self.browser_agent_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'BrowserAgent_Enable', fallback=False)
        
        # VertexAI Settings
        self.vertex_ai_project_id: Optional[str] = self.config.get('VertexAI', 'ProjectID', fallback=None)
        self.vertex_ai_location: Optional[str] = self.config.get('VertexAI', 'Location', fallback=None)
        self.vertex_ai_sentiment_model_name: str = self.config.get('VertexAI', 'SentimentModelName', fallback='gemini-1.5-flash-preview-0514')
        self.vertex_ai_browser_agent_model_name: str = self.config.get('VertexAI', 'BrowserAgentModelName', fallback='gemini-1.5-pro-preview-0514')

        if self.vertex_ai_project_id and self.vertex_ai_project_id.lower() == 'your-gcp-project-id':
            logger.warning("Vertex AI ProjectID is set to 'your-gcp-project-id'. Please update it in config.ini.")
            self.vertex_ai_project_id = None

        self.rss_feeds: List[Dict[str, str]] = []
        if self.config.has_section('AI_NewsAnalysis'):
            for key, value in self.config.items('AI_NewsAnalysis'):
                clean_key = key.strip().lower()
                if clean_key.startswith("rssfeed_"):
                    feed_name_part = clean_key.replace("rssfeed_", "")
                    feed_name = feed_name_part.replace("_", " ").title()
                    self.rss_feeds.append({"name": feed_name, "url": value})
        if not self.rss_feeds:
            logger.warning("No RSS feeds configured in config.ini under [AI_NewsAnalysis] with 'RSSFeed_' prefix.")


    def get_strategy_params(self, strategy_or_component_name: str) -> dict:
        """
        Retrieves parameters for a given strategy or component section name.
        Example section names: EWMAC_Strategy, LightGBM_Forecaster, MLForecaster_Strategy
        FIX: Made matching case-insensitive and underscore-insensitive.
        """
        params = {}
        found_section = None
        # Clean the input name for comparison
        cleaned_name = strategy_or_component_name.lower().replace('_', '')
        
        for section in self.config.sections():
            # Clean the section name from the config file for comparison
            cleaned_section = section.lower().replace('_', '')
            if cleaned_section == cleaned_name:
                found_section = section
                break
        
        if found_section and self.config.has_section(found_section):
            params = dict(self.config.items(found_section))
            for key, value in params.items():
                original_value = value
                try:
                    if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                        params[key] = int(value)
                    else:
                        try:
                            params[key] = float(value)
                        except ValueError:
                            if value.lower() == 'true': params[key] = True
                            elif value.lower() == 'false': params[key] = False
                            elif value.lower() in ['none', '']: params[key] = None
                            else:
                                params[key] = original_value
                except Exception as e:
                    logger.debug(f"Could not auto-convert param '{key}' with value '{original_value}' in section '{found_section}'. Kept as string. Error: {e}")
                    params[key] = original_value
        else:
            logger.warning(f"No specific configuration section found for: {strategy_or_component_name}. Using defaults or globally passed params.")
        
        if 'sentimentfilter_long_threshold' not in params:
            params['sentimentfilter_long_threshold'] = self.sentiment_filter_threshold_long
        if 'sentimentfilter_short_threshold' not in params:
            params['sentimentfilter_short_threshold'] = self.sentiment_filter_threshold_short
            
        return params

    def get_news_scraper_config(self) -> Dict[str, Any]:
        cfg = {"rss_feeds": self.rss_feeds, "websites": []}
        return cfg


try:
    settings = Config()
except FileNotFoundError as e:
    logger.critical(f"Could not initialize settings due to missing configuration file: {e}")
    settings = None # type: ignore
except Exception as e_global:
    logger.critical(f"Failed to initialize Config object: {e_global}", exc_info=True)
    settings = None # type: ignore

if settings and (not settings.kraken_api_key or "YOUR_API_KEY" in str(settings.kraken_api_key).upper() or "D27PYGI95TLS" in str(settings.kraken_api_key).upper()):
    logger.warning(f"API Key for '{settings.exchange_id_to_use}' appears to be a placeholder or is not configured in secrets.ini. Authenticated interaction will be limited/simulated.")
</code>

kamikaze_komodo/config/config.ini:
<code>
# kamikaze_komodo/config/config.ini

[General]
LogLevel = INFO
LogFilePath = logs/kamikaze_komodo.log

[API]
ExchangeID = krakenfutures
KrakenTestnet = True

[DataFetching]
DefaultSymbol = PF_XBTUSD
DefaultTimeframe = 4h
HistoricalDataDays = 730
DataFetchLimitPerCall = 500

[Trading]
MaxPortfolioRisk = 0.02
DefaultLeverage = 1.0
CommissionBPS = 10
; SlippageBPS is now the default for the 'fixed' slippage model
SlippageBPS = 2
; Parameters for more advanced slippage models
BASE_SLIPPAGE_BPS = 1
AVERAGE_DAILY_VOLUME_FACTOR = 0.02
VOLATILITY_SLIPPAGE_FACTOR = 0.1
; Parameters for exchange precision
MIN_TICK_SIZE = 0.5
PRICE_PRECISION = 1
FundingRateAnnualized = 0.00

[EWMAC_Strategy]
ShortWindow = 12
LongWindow = 26
SignalWindow = 9
atr_period = 14
SentimentFilter_Long_Threshold = 0.05
SentimentFilter_Short_Threshold = -0.05
EnableShorting = True

[RiskManagement]
; Overall portfolio drawdown control
MaxPortfolioDrawdownPct = 0.20

; Position Sizing Method
PositionSizer = ATRBased
FixedFractional_AllocationFraction = 0.10
ATRBased_RiskPerTradeFraction = 0.01
ATRBased_ATRMultipleForStop = 2.0
PairTradingPositionSizer_DollarNeutral = True

; New Position Sizer Parameters for Phase 2
OptimalF_WinRateEstimate = 0.51
OptimalF_AvgWinLossRatioEstimate = 1.1
OptimalF_KellyFraction = 0.5 

MLConfidence_MinSizeFactor = 0.5
MLConfidence_MaxSizeFactor = 1.5
MLConfidence_BaseAllocationFraction = 0.05

; Stop Manager Method
StopManager_Default = ATRBased
PercentageStop_LossPct = 0.02
PercentageStop_TakeProfitPct = 0.05
ATRStop_ATRMultiple = 2.0

; New Stop Manager Parameters for Phase 2
ParabolicSAR_AccelerationFactor = 0.02
ParabolicSAR_MaxAcceleration = 0.2

TripleBarrier_ProfitMultiplier = 2.0
TripleBarrier_LossMultiplier = 1.0
TripleBarrier_TimeLimitDays = 10

VolatilityBandStop_BandType = bollinger
VolatilityBandStop_BB_Period = 20
VolatilityBandStop_BB_StdDev = 2.0
VolatilityBandStop_KC_Period = 20
VolatilityBandStop_KC_ATR_Period = 10
VolatilityBandStop_KC_ATR_Multiplier = 1.5
VolatilityBandStop_TrailType = none

[PortfolioConstructor]
; Type of asset allocator to use (FixedWeight, OptimalF, HRP)
AssetAllocator = FixedWeight
DefaultAllocation_PFXBTUSD = 1.0

; New Phase 2: Portfolio Volatility Targeting
Volatility_Targeting_Enable = False
Target_Portfolio_Volatility = 0.15
Volatility_Targeting_Lookback_Period = 60

; New Phase 2: Rule-Based Rebalancing Triggers
Rebalance_Threshold_Pct = 0.05

; Settings for OptimalF Allocator if used by PortfolioConstructor
OptimalF_Default_Win_Probability = 0.51
OptimalF_Default_Payoff_Ratio = 1.1
OptimalF_Kelly_Fraction = 0.25

[AI_NewsAnalysis]
EnableSentimentAnalysis = True
SentimentLLMProvider = VertexAI
SentimentFilter_Threshold_Long = 0.1
SentimentFilter_Threshold_Short = -0.1
SimulatedSentimentDataPath = kamikaze_komodo/data/simulated_sentiment_data.csv
NewsScraper_Enable = True
NotificationListener_Enable = False
BrowserAgent_Enable = False
BrowserAgent_LLMProvider = VertexAI
BrowserAgent_Max_Steps = 20
; RSS Feeds are now correctly placed within this section
RSSFeed_Coindesk = https://www.coindesk.com/arc/outboundfeeds/rss/?outputType=xml
RSSFeed_Cointelegraph = https://cointelegraph.com/rss
RSSFeed_Decrypt = https://decrypt.co/feed/
RSSFeed_BitcoinComNews = https://news.bitcoin.com/feed/
RSSFeed_Bitcoinist = https://bitcoinist.com/feed/
RSSFeed_UToday = https://u.today/feed/
RSSFeed_CCNNews = https://www.ccn.com/news/crypto-news/feeds/
RSSFeed_CryptoPotato = https://cryptopotato.com/feed/
RSSFeed_CryptoSlate = https://cryptoslate.com/feed/
RSSFeed_TheDefiant = https://thedefiant.io/feed/
RSSFeed_ConsensysNews = https://consensys.io/category/news/feed/

[VertexAI]
ProjectID = kamikazekomodo
Location = us-central1
SentimentModelName = gemini-1.5-flash-preview-0514
BrowserAgentModelName = gemini-1.5-pro-preview-0514

[LightGBM_Forecaster]
ModelSavePath = ml_models/trained_models
TargetColumnName = close_change_lag_1_future
TrainingDaysHistory = 730
MinBarsForTraining = 200
FeatureColumns = close,log_return_lag_1,close_change_lag_1,volatility_5,RSI_14,sentiment_score

[MLForecaster_Strategy]
ForecasterType = lightgbm
ModelConfigSection = LightGBM_Forecaster
LongThreshold = 0.0005
ShortThreshold = -0.0005
ExitLongThreshold = -0.0001
ExitShortThreshold = 0.0001
MinBarsForPrediction = 50
atr_period = 14
EnableShorting = True

[EhlersInstantaneousTrendline_Strategy]
IT_Lag_Trigger = 1
atr_period = 14
EnableShorting = True

[BollingerBandBreakout_Strategy]
bb_period = 20
bb_std_dev = 2.0
atr_period = 14
volume_filter_enabled = True
volume_sma_period = 20
volume_factor_above_sma = 1.5
min_breakout_atr_multiple = 0.5
EnableShorting = True

; --- Phase 6.2 Strategies ---
[BollingerBandMeanReversion_Strategy]
bb_period = 20
bb_std_dev = 2.0
atr_period = 14
EnableShorting = True

[VolatilitySqueezeBreakout_Strategy]
bb_period = 20
bb_std_dev = 2.0
kc_period = 20
kc_atr_period = 10
kc_atr_multiplier = 1.5
EnableShorting = True

[RegimeSwitching_Strategy]
Trending_Strategy_Section = EhlersInstantaneousTrendline_Strategy
Ranging_Strategy_Section = BollingerBandMeanReversion_Strategy
High-Volatility/Choppy_Strategy_Section = HOLD 
EnableShorting = True 
regime_confirmation_period = 3
; FIX: Add and increase cooldown period to reduce whipsawing
regime_cooldown_period = 5

; --- Phase 6.3 Strategies ---
[FundingRateStrategy]
lookback_period = 14
short_threshold = 0.0005
long_threshold = -0.0005
exit_threshold_short = 0.0001
exit_threshold_long = -0.0001
enable_shorting = True
atr_period = 14

[EnsembleMLStrategy]
ensemble_method = majority_vote
model_weights_lgbm = 0.4
model_weights_xgb = 0.4
model_weights_lstm = 0.2
lgbm_config_section = LightGBM_Forecaster
xgb_config_section = XGBoost_Classifier_Forecaster
lstm_config_section = LSTM_Forecaster
enable_shorting = True
atr_period = 14

; --- End Phase 6.3 ---

[PairTrading_Strategy]
Asset1_Symbol = PF_XBTUSD
Asset2_Symbol = PF_ETHUSD
Cointegration_Lookback_Days = 90
Cointegration_Test_PValue_Threshold = 0.05
Spread_ZScore_Entry_Threshold = 2.0
Spread_ZScore_Exit_Threshold = 0.5
Spread_Calculation_Window = 20
EnableShorting = True

[XGBoost_Classifier_Forecaster]
ModelSavePath = ml_models/trained_models
TargetDefinition = next_bar_direction
NumClasses = 3
ReturnThresholds_Percent = -0.001, 0.001
TrainingDaysHistory = 730
MinBarsForTraining = 200
FeatureColumns = close,log_return_lag_1,close_change_lag_1,volatility_5,RSI_14,sentiment_score

[LSTM_Forecaster]
ModelSavePath = ml_models/trained_models
ModelFileName = lstm_pf_xbtusd_4h.pth
TargetColumnName = close_change_lag_1_future
TrainingDaysHistory = 730
MinBarsForTraining = 200
SequenceLength = 60
NumFeatures = 6
HiddenSize = 50
NumLayers = 2
Dropout = 0.2
NumEpochs = 25
BatchSize = 32
LearningRate = 0.001
FeatureColumns = close,log_return_lag_1,close_change_lag_1,volatility_5,RSI_14,sentiment_score

[KMeans_Regime_Model]
ModelSavePath = ml_models/trained_models
NumClusters = 3
FeaturesForClustering = volatility_20d,atr_14d_percentage
TrainingDaysHistory = 730

[BacktestingPerformance]
RiskFreeRateAnnual = 0.02
AnnualizationFactor = 252
</code>

kamikaze_komodo/ml_models/feature_engineering.py:
<code>
# FILE: kamikaze_komodo/ml_models/feature_engineering.py
import pandas as pd
import numpy as np
from typing import Optional, List

from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

def add_lag_features(df: pd.DataFrame, lags: List[int] = [1, 2, 3, 5, 10, 20]) -> pd.DataFrame:
    """Adds lag features for returns."""
    # FIX: Added lag 20 to the default list to match model expectations.
    for lag in lags:
        df[f'log_return_lag_{lag}'] = np.log(df['close'] / df['close'].shift(lag))
        df[f'close_change_lag_{lag}'] = df['close'].pct_change(lag)
    return df

def add_rolling_window_features(df: pd.DataFrame, windows: List[int] = [5, 10, 20]) -> pd.DataFrame:
    """Adds rolling window features like volatility."""
    if 'log_return_lag_1' not in df.columns:
        df = add_lag_features(df, lags=[1])
        
    for window in windows:
        df[f'volatility_{window}'] = df['log_return_lag_1'].rolling(window=window).std()
    return df

def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """Adds common technical indicators using pandas_ta."""
    try:
        import pandas_ta as ta
        df.ta.rsi(length=14, append=True, col_names=('RSI_14',))
        df.ta.macd(append=True, col_names=('MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9'))
        df.ta.atr(length=14, append=True, col_names=('ATR_14',))
        bbands = ta.bbands(df['close'], length=20, std=2.0)
        if bbands is not None:
            df['bb_width'] = bbands['BBB_20_2.0']
            df['bb_percent'] = bbands['BBP_20_2.0']
    except ImportError:
        logger.warning("pandas_ta not installed. Skipping technical indicator features.")
    except Exception as e:
        logger.error(f"Error calculating technical indicators: {e}", exc_info=True)
    return df

def add_sentiment_features(df: pd.DataFrame) -> pd.DataFrame:
    """Adds features based on sentiment score."""
    if 'sentiment_score' in df.columns:
        df['sentiment_sma_5'] = df['sentiment_score'].rolling(window=5).mean()
        df['sentiment_cumulative'] = df['sentiment_score'].cumsum()
    else:
        # Ensure columns exist even if no sentiment data is provided
        df['sentiment_score'] = 0.0
        df['sentiment_sma_5'] = 0.0
        df['sentiment_cumulative'] = 0.0
    return df

def add_funding_rate_features(df: pd.DataFrame) -> pd.DataFrame:
    """Adds features based on funding rates."""
    if 'funding_rate' in df.columns:
        df['funding_rate_sma_8'] = df['funding_rate'].rolling(window=8).mean()
    else:
        df['funding_rate'] = 0.0
        df['funding_rate_sma_8'] = 0.0
    return df

def add_cyclical_time_features(df: pd.DataFrame) -> pd.DataFrame:
    """Adds sine/cosine transformations for cyclical time-based features."""
    if not isinstance(df.index, pd.DatetimeIndex):
        logger.warning("DataFrame index is not a DatetimeIndex. Cannot create cyclical time features.")
        return df
        
    df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)
    df['dayofweek_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7)
    df['dayofweek_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7)
    df['month_sin'] = np.sin(2 * np.pi * df.index.month / 12)
    df['month_cos'] = np.cos(2 * np.pi * df.index.month / 12)
    return df

def get_weights_ffd(d: float, thres: float) -> np.ndarray:
    """Helper to generate weights for fractional differentiation."""
    w, k = [1.], 1
    while True:
        w_ = -w[-1] / k * (d - k + 1)
        if abs(w_) < thres:
            break
        w.append(w_)
        k += 1
    return np.array(w[::-1]).reshape(-1, 1)

def fractional_differentiation(series: pd.Series, d: float, thres: float = 1e-4) -> pd.Series:
    """Computes fractionally differentiated series."""
    w = get_weights_ffd(d, thres)
    width = len(w) - 1
    
    df = pd.DataFrame({'original': series})
    df['frac_diff'] = np.nan
    
    for i in range(width, len(df)):
        series_slice = df.iloc[i - width:i + 1, 0].values.reshape(-1, 1)
        df.iloc[i, df.columns.get_loc('frac_diff')] = np.dot(w.T, series_slice)[0, 0]
        
    return df['frac_diff']

def add_fractional_diff_features(df: pd.DataFrame, d: float = 0.5, thres: float = 1e-4, column: str = 'close') -> pd.DataFrame:
    """Wrapper to apply fractional differentiation and add as a new column."""
    if column in df.columns:
        df[f'{column}_frac_diff'] = fractional_differentiation(df[column], d, thres)
        logger.info(f"Added fractional differentiation feature for column '{column}'.")
    else:
        logger.warning(f"Column '{column}' not found for fractional differentiation.")
    return df

def market_microstructure_features(df: pd.DataFrame) -> pd.DataFrame:
    """Adds features based on market microstructure (VWAP, OFI proxy)."""
    if not all(col in df.columns for col in ['high', 'low', 'close', 'volume']):
        logger.warning("Microstructure features require 'high', 'low', 'close', 'volume' columns.")
        return df

    # VWAP approximation (true VWAP requires tick data)
    df['typical_price'] = (df['high'] + df['low'] + df['close']) / 3
    df['tpv'] = df['typical_price'] * df['volume']
    
    # Use a rolling window for a more stable VWAP in a bar context
    rolling_tpv = df['tpv'].rolling(window=20).sum()
    rolling_volume = df['volume'].rolling(window=20).sum()
    df['vwap_proxy'] = rolling_tpv / rolling_volume
    
    df['vwap_deviation'] = ((df['close'] - df['vwap_proxy']) / df['vwap_proxy']) * 100

    # Order Flow Imbalance (OFI) proxy
    price_change = df['close'].diff()
    df['ofi_proxy'] = df['volume'] * np.sign(price_change)
    df['ofi_proxy_sma_10'] = df['ofi_proxy'].rolling(window=10).mean()

    df.drop(['typical_price', 'tpv'], axis=1, inplace=True, errors='ignore')
    return df

def add_microstructure_features(df: pd.DataFrame) -> pd.DataFrame:
    """Wrapper to add all market microstructure features."""
    df = market_microstructure_features(df)
    logger.info("Added market microstructure features.")
    return df
</code>

kamikaze_komodo/ml_models/__init__.py:
<code>
# kamikaze_komodo/ml_models/__init__.py
# This file makes the 'ml_models' directory a Python package.
from . import feature_engineering
</code>

kamikaze_komodo/ml_models/training_pipelines/kmeans_regime_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/kmeans_regime_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os

from kamikaze_komodo.ml_models.regime_detection.kmeans_regime_model import KMeansRegimeModel
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class KMeansRegimeTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "KMeans_Regime_Model"):
        if not settings:
            logger.critical("Settings not loaded. KMeansRegimeTrainingPipeline cannot be initialized.")
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        if not self.model_params:
            logger.warning(f"No parameters found for config section [{model_config_section}]. Using defaults for KMeansRegimeModel if any.")
            self.model_params = {} # Ensure it's a dict
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"kmeans_regime_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        # FIX: Use the consistent PROJECT_ROOT from settings.py
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path
            
        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            logger.info(f"Created directory for trained KMeans regime models: {self.model_save_path_dir}")
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.regime_model = KMeansRegimeModel(model_path=None, params=self.model_params) # Don't load, we are training
        logger.info(f"KMeansRegimeTrainingPipeline initialized for {symbol} ({timeframe}). Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int) -> pd.DataFrame:
        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        logger.info(f"Attempting to retrieve data from DB for KMeans training: {self.symbol} ({self.timeframe}) from {start_date} to {end_date}")
        historical_bars = db_manager.retrieve_bar_data(self.symbol, self.timeframe, start_date, end_date)
        
        min_bars_for_features = int(self.model_params.get('minbarsfortraining', 100)) 

        if not historical_bars or len(historical_bars) < min_bars_for_features:
            logger.info(f"Insufficient data in DB ({len(historical_bars)} bars found, need {min_bars_for_features}). Fetching fresh data for KMeans training...")
            historical_bars = await data_fetcher.fetch_historical_data_for_period(self.symbol, self.timeframe, start_date, end_date)
            if historical_bars:
                db_manager.store_bar_data(historical_bars)
            else:
                logger.error("Failed to fetch training data for KMeans.")
                await data_fetcher.close()
                db_manager.close()
                return pd.DataFrame()
        
        await data_fetcher.close()
        db_manager.close()

        if not historical_bars or len(historical_bars) < min_bars_for_features:
            logger.error(f"Still not enough data ({len(historical_bars)} bars) for KMeans training after fetch attempt. Need {min_bars_for_features}.")
            return pd.DataFrame()
            
        data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
        if data_df.empty:
            logger.error("DataFrame is empty after converting BarData list.")
            return pd.DataFrame()
            
        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
        data_df.set_index('timestamp', inplace=True)
        data_df.sort_index(inplace=True)
        
        logger.info(f"Fetched {len(data_df)} bars for KMeans training {self.symbol} ({self.timeframe}).")
        return data_df

    async def run_training(self):
        """
        Fetches data, trains the KMeans regime model, and saves it.
        """
        days_history = int(self.model_params.get('trainingdayshistory', 1095))
        if days_history <=0:
            logger.error(f"TrainingDaysHistory ({days_history}) must be positive. Cannot run training.")
            return

        historical_df = await self.fetch_training_data(days_history=days_history)

        if historical_df.empty:
            logger.error("Cannot run KMeans training, no historical data was retrieved or processed.")
            return

        logger.info(f"Starting KMeans regime model training using {self.regime_model.__class__.__name__}...")
        
        self.regime_model.train(historical_df) 
        
        if self.regime_model.model and self.regime_model.scaler:
            self.regime_model.save_model(self.model_full_save_path)
            logger.info(f"KMeans regime model training completed and model saved to {self.model_full_save_path}.")
        else:
            logger.error("KMeans regime model training did not produce a valid model or scaler. Model not saved.")
</code>

kamikaze_komodo/ml_models/training_pipelines/__init__.py:
<code>
# kamikaze_komodo/ml_models/training_pipelines/__init__.py
# This file makes the 'training_pipelines' directory a Python package.
</code>

kamikaze_komodo/ml_models/training_pipelines/lstm_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/lstm_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
from kamikaze_komodo.ml_models.price_forecasting.lstm_model import LSTMForecaster
from kamikaze_komodo.data_handling.data_handler import DataHandler
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class LSTMTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LSTM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"lstm_{symbol.replace('/', '_').lower()}_{timeframe}.pth")
        
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path

        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.forecaster = LSTMForecaster(params=self.model_params)
        logger.info(f"LSTM Training Pipeline initialized. Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int) -> pd.DataFrame:
        data_handler = DataHandler()
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        data_df = await data_handler.get_prepared_data(
            self.symbol, self.timeframe, start_date, end_date,
            needs_funding_rate=True, needs_sentiment=True
        )
        await data_handler.close()
        
        if not data_df.empty:
            logger.info(f"Fetched and prepared {len(data_df)} bars for LSTM training.")
        return data_df

    async def run_training(self, tune_hyperparameters: bool = False):
        # Note: Hyperparameter tuning for LSTMs is more complex and computationally
        # expensive than for tree-based models. This is a simplified placeholder.
        if tune_hyperparameters:
            logger.warning("Hyperparameter tuning for LSTM is not fully implemented in this phase. Running with default parameters.")

        days_history = int(self.model_params.get('trainingdayshistory', 730))
        historical_df = await self.fetch_training_data(days_history=days_history)

        if historical_df.empty:
            logger.error("Cannot run LSTM training, no historical data.")
            return

        target_col_name = self.model_params.get('targetcolumnname', 'close_change_lag_1_future')
        feature_cols_str = self.model_params.get('featurecolumns')
        feature_columns = [col.strip() for col in feature_cols_str.split(',')] if feature_cols_str else None
        
        self.forecaster.train(historical_df, target_column=target_col_name, feature_columns=feature_columns)
        
        if self.forecaster.model:
            self.forecaster.save_model(self.model_full_save_path)
        else:
            logger.error("LSTM training did not produce a model. Model not saved.")
</code>

kamikaze_komodo/ml_models/training_pipelines/xgboost_classifier_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/xgboost_classifier_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from scipy.stats import randint as sp_randint
from scipy.stats import uniform as sp_uniform
from typing import Optional, List
from kamikaze_komodo.ml_models.price_forecasting.xgboost_classifier_forecaster import XGBoostClassifierForecaster
from kamikaze_komodo.data_handling.data_handler import DataHandler
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class XGBoostClassifierTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "XGBoost_Classifier_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"xgb_classifier_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path
            
        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.forecaster = XGBoostClassifierForecaster(params=self.model_params)
        logger.info(f"XGBoost Training Pipeline initialized. Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int = 730) -> pd.DataFrame:
        data_handler = DataHandler()
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        data_df = await data_handler.get_prepared_data(
            self.symbol, self.timeframe, start_date, end_date,
            needs_funding_rate=True, needs_sentiment=True
        )
        await data_handler.close()
        if not data_df.empty:
            logger.info(f"Fetched and prepared {len(data_df)} bars for XGBoost training.")
        return data_df

    async def run_training(self, tune_hyperparameters: bool = False):
        days_history = int(self.model_params.get('trainingdayshistory', 730))
        historical_df = await self.fetch_training_data(days_history=days_history)
        if historical_df.empty:
            logger.error("Cannot run XGBoost training, no historical data.")
            return

        target_def = self.model_params.get('targetdefinition', 'next_bar_direction')
        feature_cols_str = self.model_params.get('feature_columns')
        feature_columns = [col.strip() for col in feature_cols_str.split(',')] if feature_cols_str else None
        
        if tune_hyperparameters:
            logger.info("Starting hyperparameter tuning for XGBoost...")
            self._tune_and_train(historical_df, target_def, feature_columns)
        else:
            logger.info(f"Starting XGBoost training with target: '{target_def}', features: {feature_columns or 'default'}")
            self.forecaster.train(historical_df, target_definition=target_def, feature_columns=feature_columns)
        
        if self.forecaster.model:
            self.forecaster.save_model(self.model_full_save_path)
        else:
            logger.error("XGBoost training did not produce a model. Model not saved.")

    def _tune_and_train(self, data: pd.DataFrame, target_definition: str, feature_columns: Optional[List[str]]):
        # Prepare data
        df = data.copy()
        df['target'] = self.forecaster._define_target(df) # Use internal method to create target
        df.dropna(subset=['target'], inplace=True)
        df_with_features = self.forecaster.create_features(df)
        
        features = feature_columns or [col for col in df_with_features.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'target']]
        
        X = df_with_features[features].copy()
        X.dropna(inplace=True)
        y = df_with_features.loc[X.index, 'target'].astype(int)

        if X.empty:
            logger.error("No data left for hyperparameter tuning after processing.")
            return
            
        y_encoded = self.forecaster.label_encoder.fit_transform(y)

        # Hyperparameter grid for RandomizedSearch
        param_dist = {
            'n_estimators': sp_randint(100, 1000),
            'learning_rate': sp_uniform(0.01, 0.2),
            'max_depth': sp_randint(3, 10),
            'subsample': sp_uniform(0.7, 0.3),
            'colsample_bytree': sp_uniform(0.7, 0.3),
            'gamma': sp_uniform(0, 0.5)
        }
        
        tscv = TimeSeriesSplit(n_splits=5)
        xgb_clf = xgb.XGBClassifier(**self.forecaster.xgb_params)
        
        random_search = RandomizedSearchCV(
            estimator=xgb_clf,
            param_distributions=param_dist,
            n_iter=25,
            cv=tscv,
            scoring='accuracy', # Or 'f1_weighted'
            n_jobs=-1,
            verbose=1,
            random_state=42
        )
        
        random_search.fit(X, y_encoded)
        
        logger.info(f"Best parameters found: {random_search.best_params_}")
        
        # Update forecaster with best model and retrain on full data
        self.forecaster.model = random_search.best_estimator_
        self.forecaster.xgb_params = random_search.best_params_
        self.forecaster.trained_feature_columns_ = list(X.columns)
        logger.info("Retraining final model on all available data with best parameters...")
        self.forecaster.model.fit(X, y_encoded)
</code>

kamikaze_komodo/ml_models/training_pipelines/lightgbm_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/lightgbm_pipeline.py
from typing import List, Optional
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from scipy.stats import randint as sp_randint
from scipy.stats import uniform as sp_uniform
import lightgbm as lgb
from kamikaze_komodo.ml_models.price_forecasting.lightgbm_forecaster import LightGBMForecaster
from kamikaze_komodo.data_handling.data_handler import DataHandler
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class LightGBMTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LightGBM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"lgbm_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path

        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            logger.info(f"Created directory for trained models: {self.model_save_path_dir}")
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.forecaster = LightGBMForecaster(params=self.model_params)
        logger.info(f"LightGBM Training Pipeline initialized for {symbol} ({timeframe}). Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int = 730) -> pd.DataFrame:
        data_handler = DataHandler()
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        
        # Fetch data with all potentially needed sources
        data_df = await data_handler.get_prepared_data(
            self.symbol, self.timeframe, start_date, end_date,
            needs_funding_rate=True, needs_sentiment=True
        )
        await data_handler.close()
        
        if not data_df.empty:
             logger.info(f"Fetched and prepared {len(data_df)} bars for training {self.symbol} ({self.timeframe}).")
        return data_df

    async def run_training(self, tune_hyperparameters: bool = False):
        days_history = int(self.model_params.get('trainingdayshistory', 730))
        historical_df = await self.fetch_training_data(days_history=days_history)
        if historical_df.empty:
            logger.error("Cannot run training, no historical data.")
            return

        target_col_name = self.model_params.get('targetcolumnname', 'close_change_lag_1_future')
        feature_cols_str = self.model_params.get('feature_columns')
        feature_columns = [col.strip() for col in feature_cols_str.split(',')] if feature_cols_str else None
        
        if tune_hyperparameters:
            logger.info("Starting hyperparameter tuning for LightGBM...")
            self._tune_and_train(historical_df, target_col_name, feature_columns)
        else:
            logger.info(f"Starting training with target: '{target_col_name}', features: {feature_columns or 'default in forecaster'}")
            self.forecaster.train(historical_df, target_column=target_col_name, feature_columns_to_use=feature_columns)
        
        if self.forecaster.model:
            self.forecaster.save_model(self.model_full_save_path)
        else:
            logger.error("Training did not produce a model. Model not saved.")
            
    def _tune_and_train(self, data: pd.DataFrame, target_column: str, feature_columns: Optional[List[str]]):
        # Prepare data
        df = data.copy()
        df['target'] = (df['close'].shift(-1) / df['close']) - 1
        df.dropna(subset=['target'], inplace=True)
        df_with_features = self.forecaster.create_features(df)
        
        features = feature_columns or [col for col in df_with_features.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'target']]
        
        X = df_with_features[features].copy()
        X.dropna(inplace=True)
        y = df_with_features.loc[X.index, 'target']

        if X.empty:
            logger.error("No data left for hyperparameter tuning after processing.")
            return

        # Hyperparameter grid for RandomizedSearch
        param_dist = {
            'n_estimators': sp_randint(100, 1000),
            'learning_rate': sp_uniform(0.01, 0.2),
            'num_leaves': sp_randint(20, 60),
            'max_depth': sp_randint(3, 10),
            'subsample': sp_uniform(0.6, 0.4),
            'colsample_bytree': sp_uniform(0.6, 0.4)
        }
        
        # Time series cross-validation
        tscv = TimeSeriesSplit(n_splits=5)
        
        lgbm = lgb.LGBMRegressor(**self.forecaster.lgbm_params)
        
        random_search = RandomizedSearchCV(
            estimator=lgbm,
            param_distributions=param_dist,
            n_iter=25,  # Number of parameter settings that are sampled
            cv=tscv,
            scoring='neg_mean_squared_error',
            n_jobs=-1,
            verbose=1,
            random_state=42
        )
        
        random_search.fit(X, y)
        
        logger.info(f"Best parameters found: {random_search.best_params_}")
        
        # Update forecaster with best model and retrain on full data
        self.forecaster.model = random_search.best_estimator_
        self.forecaster.lgbm_params = random_search.best_params_
        self.forecaster.trained_feature_columns_ = list(X.columns)
        logger.info("Retraining final model on all available data with best parameters...")
        self.forecaster.model.fit(X,y)
</code>

kamikaze_komodo/ml_models/inference_pipelines/lstm_inference.py:
<code>
# FILE: kamikaze_komodo/ml_models/inference_pipelines/lstm_inference.py
import pandas as pd
from typing import Optional
import os
from kamikaze_komodo.ml_models.price_forecasting.lstm_model import LSTMForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class LSTMInference:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LSTM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        
        model_params_config = settings.get_strategy_params(model_config_section)
        
        _model_base_path = model_params_config.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = model_params_config.get('modelfilename', f"lstm_{symbol.replace('/', '_').lower()}_{timeframe}.pth")
        
        if not os.path.isabs(_model_base_path):
            self.model_load_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_load_path_dir = _model_base_path
            
        self.model_full_load_path = os.path.join(self.model_load_path_dir, _model_filename)
        self.forecaster = LSTMForecaster(model_path=self.model_full_load_path, params=model_params_config)
        
        if self.forecaster.model is None:
            logger.warning(f"LSTMInference: Model could not be loaded from {self.model_full_load_path}. Predictions will not be available.")

    def get_prediction(self, current_data_history: pd.DataFrame) -> Optional[float]:
        """
        Gets a single prediction based on the current data history.
        """
        if self.forecaster.model is None:
            logger.warning("No model loaded, cannot get prediction.")
            return None
        if current_data_history.empty:
            logger.warning("Data history is empty, cannot get prediction.")
            return None
            
        prediction_output = self.forecaster.predict(current_data_history)
        
        if prediction_output is None:
            return None
            
        return float(prediction_output)
</code>

kamikaze_komodo/ml_models/inference_pipelines/__init__.py:
<code>
# kamikaze_komodo/ml_models/inference_pipelines/__init__.py
# This file makes the 'inference_pipelines' directory a Python package.
</code>

kamikaze_komodo/ml_models/inference_pipelines/xgboost_classifier_inference.py:
<code>
# FILE: kamikaze_komodo/ml_models/inference_pipelines/xgboost_classifier_inference.py
import pandas as pd
from typing import Optional, Dict, Any
import os
import numpy as np
from kamikaze_komodo.ml_models.price_forecasting.xgboost_classifier_forecaster import XGBoostClassifierForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class XGBoostClassifierInference:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "XGBoost_Classifier_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        
        model_params_config = settings.get_strategy_params(model_config_section)
        
        _model_base_path = model_params_config.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = model_params_config.get('modelfilename', f"xgb_classifier_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        if not os.path.isabs(_model_base_path):
            self.model_load_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_load_path_dir = _model_base_path
            
        self.model_full_load_path = os.path.join(self.model_load_path_dir, _model_filename)
        self.forecaster = XGBoostClassifierForecaster(model_path=self.model_full_load_path, params=model_params_config)
        
        if self.forecaster.model is None:
            logger.warning(f"XGBoostClassifierInference: Model could not be loaded from {self.model_full_load_path}. Predictions will not be available.")

    def get_prediction(self, current_data_history: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        Gets a single classification prediction based on the current data history.
        Returns a dictionary with 'predicted_class', 'confidence', and 'probabilities'.
        """
        if self.forecaster.model is None:
            logger.warning("XGBoost model not loaded, cannot get prediction.")
            return None
        if current_data_history.empty:
            logger.warning("Data history for XGBoost prediction is empty.")
            return None
            
        prediction_output = self.forecaster.predict(current_data_history) # XGBoostClassifierForecaster.predict returns a dict
        
        if prediction_output and isinstance(prediction_output, dict):
            return prediction_output
        else:
            logger.warning(f"Unexpected prediction output type from XGBoost forecaster: {type(prediction_output)}")
            return None
</code>

kamikaze_komodo/ml_models/inference_pipelines/lightgbm_inference.py:
<code>
# kamikaze_komodo/ml_models/inference_pipelines/lightgbm_inference.py
import pandas as pd
from typing import Optional, Union
import os
import numpy as np 
from kamikaze_komodo.ml_models.price_forecasting.lightgbm_forecaster import LightGBMForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class LightGBMInference:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LightGBM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        
        model_params_config = settings.get_strategy_params(model_config_section) 
        
        _model_base_path = model_params_config.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = model_params_config.get('modelfilename', f"lgbm_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        if not os.path.isabs(_model_base_path):
            self.model_load_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_load_path_dir = _model_base_path
            
        self.model_full_load_path = os.path.join(self.model_load_path_dir, _model_filename)
        self.forecaster = LightGBMForecaster(model_path=self.model_full_load_path, params=model_params_config)
        
        if self.forecaster.model is None:
            logger.warning(f"LightGBMInference: Model could not be loaded from {self.model_full_load_path}. Predictions will not be available.")

    def get_prediction(self, current_data_history: pd.DataFrame) -> Optional[float]:
        """
        Gets a single prediction based on the current data history.
        Assumes current_data_history has enough data to form features for the last point.
        """
        if self.forecaster.model is None:
            logger.warning("No model loaded, cannot get prediction.")
            return None
        if current_data_history.empty:
            logger.warning("Data history is empty, cannot get prediction.")
            return None
        
        feature_cols_to_pass = None
        feature_cols_str_from_params = self.forecaster.params.get('feature_columns')
        if isinstance(feature_cols_str_from_params, str) and feature_cols_str_from_params:
            feature_cols_to_pass = [col.strip() for col in feature_cols_str_from_params.split(',')]
        elif isinstance(feature_cols_str_from_params, list):
                feature_cols_to_pass = feature_cols_str_from_params
        
        prediction_output = self.forecaster.predict(current_data_history, feature_columns_to_use=feature_cols_to_pass)
        
        if prediction_output is None:
            return None
        
        if isinstance(prediction_output, pd.Series):
            if not prediction_output.empty:
                return prediction_output.iloc[-1] 
            else:
                logger.warning("Prediction series is empty.")
                return None
        elif isinstance(prediction_output, (float, np.float64)):
            return float(prediction_output)
        else:
            logger.warning(f"Unexpected prediction output type: {type(prediction_output)}")
            return None
</code>

kamikaze_komodo/ml_models/price_forecasting/base_forecaster.py:
<code>
# FILE: kamikaze_komodo/ml_models/price_forecasting/base_forecaster.py
from abc import ABC, abstractmethod
import pandas as pd
from typing import Optional, Dict, Any, Union
from kamikaze_komodo.app_logger import get_logger
logger = get_logger(__name__)
class BasePriceForecaster(ABC):
    """
    Abstract base class for price forecasting models.
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        self.model_path = model_path
        self.params = params if params is not None else {}
        self.model: Any = None
        # The call to load_model is removed from the base class.
        # Subclasses are now responsible for calling it at the appropriate time
        # (i.e., after the model architecture has been defined).
        logger.info(f"{self.__class__.__name__} initialized with model_path: {model_path}, params: {self.params}")
    @abstractmethod
    def train(self, historical_data: pd.DataFrame, target_column: str = 'close', feature_columns: Optional[list] = None):
        """
        Trains the forecasting model.
        Args:
            historical_data (pd.DataFrame): DataFrame with historical OHLCV and feature data.
            target_column (str): The name of the column to predict.
            feature_columns (Optional[list]): List of column names to be used as features. If None, uses defaults.
        """
        pass
    @abstractmethod
    def predict(self, new_data: pd.DataFrame, feature_columns: Optional[list] = None) -> Union[pd.Series, float, None, Dict[str, Any]]:
        """
        Makes predictions on new data.
        Args:
            new_data (pd.DataFrame): DataFrame with the latest data for prediction.
                                     For bar-by-bar, this might be a single row or a lookback window.
            feature_columns (Optional[list]): List of column names to be used as features, must match training.
        Returns:
            Union[pd.Series, float, None, Dict]: Predicted value(s) or None if prediction fails.
                                                 Could be a series for multi-step, single float for next step,
                                                 or a Dict for classifiers.
        """
        pass
    @abstractmethod
    def save_model(self, path: str):
        """
        Saves the trained model to the specified path.
        """
        pass
    @abstractmethod
    def load_model(self, path: str):
        """
        Loads a trained model from the specified path.
        """
        pass
    @abstractmethod
    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Creates features for the model from raw data.
        """
        pass
</code>

kamikaze_komodo/ml_models/price_forecasting/__init__.py:
<code>
# kamikaze_komodo/ml_models/price_forecasting/__init__.py
# This file makes the 'price_forecasting' directory a Python package.
</code>

kamikaze_komodo/ml_models/price_forecasting/lightgbm_forecaster.py:
<code>
# FILE: kamikaze_komodo/ml_models/price_forecasting/lightgbm_forecaster.py
import lightgbm as lgb
import pandas as pd
import numpy as np
import joblib # For saving/loading model
from typing import Optional, Dict, Any, List, Union
from kamikaze_komodo.ml_models.price_forecasting.base_forecaster import BasePriceForecaster
from kamikaze_komodo.ml_models.feature_engineering import (
    add_lag_features, add_rolling_window_features, add_technical_indicators,
    add_sentiment_features, add_cyclical_time_features
)
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class LightGBMForecaster(BasePriceForecaster):
    """
    LightGBM-based price forecaster.
    Predicts price movement (e.g., next bar's close relative to current).
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        super().__init__(model_path, params)
        self.default_lgbm_params = {
            'objective': 'regression_l1',
            'metric': 'rmse',
            'n_estimators': 100,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'n_jobs': -1,
            'seed': 42,
            'boosting_type': 'gbdt',
        }
        config_lgbm_params = {k.replace('lgbm_params_', ''): v for k, v in self.params.items() if k.startswith('lgbm_params_')}
        self.lgbm_params = {**self.default_lgbm_params, **config_lgbm_params}
        
        if not self.model and self.model_path:
            self.load_model(self.model_path)

    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Creates features by calling centralized feature engineering functions.
        """
        if data.empty:
            logger.warning("Data for feature creation is empty.")
            return pd.DataFrame()
            
        df = data.copy()
        df = add_lag_features(df)
        df = add_rolling_window_features(df)
        df = add_technical_indicators(df)
        df = add_sentiment_features(df)
        df = add_cyclical_time_features(df)
        
        df = df.replace([np.inf, -np.inf], np.nan)
        return df

    def train(self, historical_data: pd.DataFrame, target_column: str = 'close_change_lag_1_future', feature_columns_to_use: Optional[List[str]] = None):
        logger.info(f"Starting LightGBM training for target '{target_column}'. Data shape: {historical_data.shape}")
        df = historical_data.copy()
        
        if target_column == 'close_change_lag_1_future':
            df['target'] = (df['close'].shift(-1) / df['close']) - 1
        else:
            if target_column not in df.columns:
                logger.error(f"Target column '{target_column}' not found in data.")
                return
            df['target'] = df[target_column]

        df.dropna(subset=['target'], inplace=True) 
        df_with_all_features = self.create_features(df)
        
        if feature_columns_to_use:
            actual_features_present = [col for col in feature_columns_to_use if col in df_with_all_features.columns]
        else:
            # Default feature selection if none provided
            actual_features_present = [col for col in df_with_all_features.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'target']]

        if not actual_features_present:
            logger.error("No valid feature columns found for training. Cannot train.")
            return

        X_final_features = df_with_all_features[actual_features_present].copy()
        X_final_features.dropna(inplace=True) 
        y_train = df_with_all_features.loc[X_final_features.index, 'target'] 
        X_train = X_final_features
        
        if X_train.empty or y_train.empty:
            logger.error("Feature matrix or target vector is empty after processing. Training cannot proceed.")
            return

        self.model = lgb.LGBMRegressor(**self.lgbm_params)
        logger.info(f"Training LightGBM model with {len(X_train)} samples. Features: {list(X_train.columns)}")
        try:
            self.model.fit(X_train, y_train)
            logger.info("LightGBM model training completed.")
            self.trained_feature_columns_ = list(X_train.columns)
        except Exception as e:
            logger.error(f"Error during LightGBM model training: {e}", exc_info=True)
            self.model = None

    def predict(self, new_data: pd.DataFrame, feature_columns_to_use: Optional[list] = None) -> Union[pd.Series, float, None]:
        if self.model is None:
            logger.error("Model not loaded or trained. Cannot make predictions.")
            return None
        
        df_with_all_features = self.create_features(new_data)
        
        cols_for_prediction = feature_columns_to_use or getattr(self, 'trained_feature_columns_', None)
        if not cols_for_prediction:
            logger.error("No feature columns determined for prediction.")
            return None

        # Ensure prediction features are present
        cols_for_prediction = [col for col in cols_for_prediction if col in df_with_all_features.columns]
        
        X_new = df_with_all_features[cols_for_prediction].copy()
        if X_new.empty:
            logger.warning("Feature matrix is empty after selection. Cannot predict.")
            return None
        
        try:
            predictions = self.model.predict(X_new)
            if len(predictions) == 0: return None
            return predictions[-1] if isinstance(predictions, np.ndarray) else predictions
        except Exception as e:
            logger.error(f"Error during LightGBM prediction: {e}", exc_info=True)
            return None

    def save_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if self.model is None or not _path:
            logger.error(f"Model not available or path not specified. Cannot save.")
            return
        try:
            model_and_features = {
                'model': self.model,
                'feature_columns': getattr(self, 'trained_feature_columns_', None)
            }
            joblib.dump(model_and_features, _path)
            logger.info(f"LightGBM model and feature columns saved to {_path}")
        except Exception as e:
            logger.error(f"Error saving LightGBM model to {_path}: {e}", exc_info=True)

    def load_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if not _path:
            logger.debug("No path specified for loading the model.")
            return
        try:
            model_and_features = joblib.load(_path)
            self.model = model_and_features['model']
            self.trained_feature_columns_ = model_and_features.get('feature_columns') 
            self.model_path = _path 
            logger.info(f"LightGBM model loaded from {_path}. Trained features: {self.trained_feature_columns_}")
        except FileNotFoundError:
            logger.error(f"LightGBM model file not found at {_path}.")
            self.model = None
        except Exception as e:
            logger.error(f"Error loading LightGBM model from {_path}: {e}", exc_info=True)
            self.model = None
</code>

kamikaze_komodo/ml_models/price_forecasting/xgboost_classifier_forecaster.py:
<code>
# FILE: kamikaze_komodo/ml_models/price_forecasting/xgboost_classifier_forecaster.py
import xgboost as xgb
import pandas as pd
import numpy as np
import joblib
from typing import Optional, Dict, Any, List, Tuple
from sklearn.preprocessing import LabelEncoder

from kamikaze_komodo.ml_models.price_forecasting.base_forecaster import BasePriceForecaster
from kamikaze_komodo.ml_models.feature_engineering import (
    add_lag_features, add_rolling_window_features, add_technical_indicators,
    add_sentiment_features, add_cyclical_time_features
)
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class XGBoostClassifierForecaster(BasePriceForecaster):
    """
    XGBoost-based classifier for price movement prediction (UP, DOWN, SIDEWAYS).
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        self.label_encoder = LabelEncoder()
        self.trained_feature_columns_: Optional[List[str]] = None
        
        super().__init__(model_path, params)
        
        self.default_xgb_params = {
            'objective': 'multi:softprob',
            'eval_metric': 'mlogloss',
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 3,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'use_label_encoder': False,
            'seed': 42,
        }
        config_xgb_params = {k.replace('xgb_params_', ''): v for k, v in self.params.items() if k.startswith('xgb_params_')}
        self.xgb_params = {**self.default_xgb_params, **config_xgb_params}
        
        self.num_class = int(self.params.get('num_classes', 3))
        self.xgb_params['num_class'] = self.num_class
        
        if not self.model and self.model_path:
            self.load_model(self.model_path)

    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Creates features using centralized feature engineering functions."""
        if data.empty: return pd.DataFrame()
        df = data.copy()
        df = add_lag_features(df)
        df = add_rolling_window_features(df)
        df = add_technical_indicators(df)
        df = add_sentiment_features(df)
        df = add_cyclical_time_features(df)
        df = df.replace([np.inf, -np.inf], np.nan)
        return df

    def _define_target(self, data: pd.DataFrame, thresholds: Optional[Tuple[float, float]] = (-0.001, 0.001)) -> pd.Series:
        """Defines target classes: 0 (UP), 1 (DOWN), 2 (SIDEWAYS)."""
        future_returns = data['close'].pct_change(1).shift(-1)
        if thresholds is None: thresholds = (-0.001, 0.001)
        lower_thresh, upper_thresh = thresholds

        target = pd.Series(2, index=data.index) # Default to SIDEWAYS
        target[future_returns > upper_thresh] = 0 # UP
        target[future_returns < lower_thresh] = 1 # DOWN
        return target.astype(int)

    def train(self, historical_data: pd.DataFrame, target_definition: str = 'next_bar_direction', feature_columns: Optional[list] = None):
        logger.info(f"Starting XGBoost Classifier training. Data shape: {historical_data.shape}")
        df = historical_data.copy()

        return_thresholds_str = self.params.get('returnthresholds_percent', "-0.001,0.001")
        try:
            thresholds_list = [float(x.strip()) for x in return_thresholds_str.split(',')]
            return_thresholds = tuple(thresholds_list)
        except Exception:
            return_thresholds = (-0.001, 0.001)

        if target_definition == 'next_bar_direction':
            df['target'] = self._define_target(df, return_thresholds)
        else:
            logger.error(f"Unsupported target_definition: {target_definition}")
            return

        df.dropna(subset=['target'], inplace=True)
        df_with_features = self.create_features(df)

        actual_features = feature_columns or [col for col in df_with_features.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'symbol', 'timeframe', 'target']]
        
        X = df_with_features[actual_features].copy()
        X.dropna(inplace=True)
        y = df_with_features.loc[X.index, 'target'].astype(int)

        if X.empty or y.empty:
            logger.error("Feature matrix X or target vector y is empty after processing.")
            return

        self.label_encoder.fit(y)
        y_encoded = self.label_encoder.transform(y)

        self.model = xgb.XGBClassifier(**self.xgb_params)
        logger.info(f"Training XGBoostClassifier with {len(X)} samples. Features: {list(X.columns)}")
        try:
            self.model.fit(X, y_encoded)
            self.trained_feature_columns_ = list(X.columns)
            logger.info("XGBoostClassifier training completed.")
        except Exception as e:
            logger.error(f"Error during XGBoostClassifier training: {e}", exc_info=True)
            self.model = None

    def predict(self, new_data: pd.DataFrame, feature_columns: Optional[list] = None) -> Optional[Dict[str, Any]]:
        if self.model is None:
            logger.error("XGBoost model not loaded/trained. Cannot predict.")
            return None

        df_with_features = self.create_features(new_data)
        cols_for_pred = feature_columns or self.trained_feature_columns_
        if not cols_for_pred:
            logger.error("No feature columns determined for XGBoost prediction.")
            return None
            
        cols_for_pred = [col for col in cols_for_pred if col in df_with_features.columns]
        X_new = df_with_features[cols_for_pred].copy()
        if X_new.empty:
            return None

        try:
            last_row_features = X_new.iloc[[-1]]
            probabilities = self.model.predict_proba(last_row_features)[0]
            predicted_class_encoded = np.argmax(probabilities)
            predicted_class_label = self.label_encoder.inverse_transform([predicted_class_encoded])[0]
            confidence = probabilities[predicted_class_encoded]
            
            return {
                "predicted_class": int(predicted_class_label),
                "confidence": float(confidence),
                "probabilities": [float(p) for p in probabilities]
            }
        except Exception as e:
            logger.error(f"Error during XGBoostClassifier prediction: {e}", exc_info=True)
            return None

    def save_model(self, path: str):
        if self.model is None: return
        try:
            model_data = {
                'model': self.model,
                'label_encoder': self.label_encoder,
                'feature_columns': self.trained_feature_columns_
            }
            joblib.dump(model_data, path)
            logger.info(f"XGBoostClassifier model saved to {path}")
        except Exception as e:
            logger.error(f"Error saving XGBoostClassifier model to {path}: {e}", exc_info=True)

    def load_model(self, path: str):
        try:
            model_data = joblib.load(path)
            self.model = model_data['model']
            self.label_encoder = model_data['label_encoder']
            self.trained_feature_columns_ = model_data.get('feature_columns')
            self.model_path = path
            logger.info(f"XGBoostClassifier model loaded from {path}. Trained features: {self.trained_feature_columns_}")
        except FileNotFoundError:
            logger.error(f"XGBoostClassifier model file not found at {path}.")
            self.model = None
        except Exception as e:
            logger.error(f"Error loading XGBoostClassifier model from {path}: {e}", exc_info=True)
            self.model = None
</code>

kamikaze_komodo/ml_models/price_forecasting/lstm_model.py:
<code>
# FILE: kamikaze_komodo/ml_models/price_forecasting/lstm_model.py
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import joblib
from typing import Optional, Dict, Any, List, Union, Tuple
from sklearn.preprocessing import MinMaxScaler

from kamikaze_komodo.ml_models.price_forecasting.base_forecaster import BasePriceForecaster
from kamikaze_komodo.ml_models.feature_engineering import (
    add_lag_features, add_rolling_window_features, add_technical_indicators,
    add_sentiment_features, add_cyclical_time_features
)
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

# Define the PyTorch LSTM Model
class LSTMNetwork(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int, dropout: float):
        super(LSTMNetwork, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)
        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :]) # Get the output from the last time step
        return out

class LSTMForecaster(BasePriceForecaster):
    """
    LSTM-based price forecaster using PyTorch.
    Predicts future price movement based on a sequence of historical data.
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        super().__init__(model_path, params)
        self.scaler = MinMaxScaler(feature_range=(-1, 1))
        self.sequence_length = int(self.params.get('sequencelength', 60))
        
        feature_columns_str = self.params.get('featurecolumns', 'close,log_return_lag_1,close_change_lag_1,volatility_5,RSI_14,sentiment_score')
        self.feature_columns = [col.strip() for col in feature_columns_str.split(',')]
        self.num_features = len(self.feature_columns)

        # Model hyperparameters
        self.hidden_size = int(self.params.get('hiddensize', 50))
        self.num_layers = int(self.params.get('numlayers', 2))
        self.dropout = float(self.params.get('dropout', 0.2))
        self.num_epochs = int(self.params.get('numepochs', 20))
        self.batch_size = int(self.params.get('batchsize', 32))
        self.learning_rate = float(self.params.get('learningrate', 0.001))
        
        # Initialize model architecture
        self.model = LSTMNetwork(
            input_size=self.num_features,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            output_size=1,
            dropout=self.dropout
        )
        if self.model_path:
            self.load_model(self.model_path)

    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Creates features using centralized feature engineering functions."""
        if data.empty: return pd.DataFrame()
        df = data.copy()
        df = add_lag_features(df)
        df = add_rolling_window_features(df)
        df = add_technical_indicators(df)
        df = add_sentiment_features(df)
        df = add_cyclical_time_features(df)
        df = df.replace([np.inf, -np.inf], np.nan)
        return df

    def _create_sequences(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        X, y = [], []
        # data has features + target column at the end
        for i in range(len(data) - self.sequence_length):
            X.append(data[i:(i + self.sequence_length), :-1])
            y.append(data[i + self.sequence_length, -1])
        return np.array(X), np.array(y)

    def train(self, historical_data: pd.DataFrame, target_column: str = 'close_change_lag_1_future', feature_columns: Optional[list] = None):
        logger.info("Starting LSTM training...")
        df_features = self.create_features(historical_data)
        
        if feature_columns is None:
            feature_columns = self.feature_columns
            
        df_features['target'] = (df_features['close'].shift(-1) / df_features['close']) - 1
        
        final_columns_to_use = feature_columns + ['target']
        features_with_target = df_features[final_columns_to_use].dropna()

        if features_with_target.empty or len(features_with_target) < self.sequence_length + 1:
            logger.error("Not enough data to create sequences for LSTM training.")
            return
        
        scaled_data = self.scaler.fit_transform(features_with_target)
        
        X, y = self._create_sequences(scaled_data)
        X_train = torch.from_numpy(X).float()
        y_train = torch.from_numpy(y).float().view(-1, 1)
        
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        
        for epoch in range(self.num_epochs):
            self.model.train()
            outputs = self.model(X_train)
            optimizer.zero_grad()
            loss = criterion(outputs, y_train)
            loss.backward()
            optimizer.step()
            if (epoch + 1) % 5 == 0:
                logger.info(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {loss.item():.6f}')

        self.trained_feature_columns_ = feature_columns
        logger.info("LSTM model training completed.")

    def predict(self, new_data: pd.DataFrame, feature_columns: Optional[list] = None) -> Union[pd.Series, float, None]:
        if self.model is None or self.scaler is None:
            logger.error("Model or scaler not available.")
            return None
        
        self.model.eval()
        df_features = self.create_features(new_data)
        
        if feature_columns is None:
            feature_columns = self.trained_feature_columns_ or self.feature_columns
            
        if len(df_features) < self.sequence_length:
            return None
            
        last_sequence_unscaled = df_features[feature_columns].iloc[-self.sequence_length:]
        if last_sequence_unscaled.isnull().values.any():
            return None
        
        # We only need to transform the features for prediction
        scaled_sequence = self.scaler.transform(pd.concat([last_sequence_unscaled, pd.DataFrame(columns=['target'])], axis=1))[:, :-1]
        
        with torch.no_grad():
            input_tensor = torch.from_numpy(scaled_sequence).float().unsqueeze(0)
            prediction_scaled = self.model(input_tensor)
        
        # Inverse transform the prediction
        dummy_array = np.zeros((1, len(feature_columns) + 1))
        dummy_array[0, -1] = prediction_scaled.item()
        prediction_unscaled = self.scaler.inverse_transform(dummy_array)[0, -1]
        
        return float(prediction_unscaled)

    def save_model(self, path: str):
        try:
            state = {
                'model_state_dict': self.model.state_dict(),
                'scaler': self.scaler,
                'params': self.params,
                'trained_feature_columns': getattr(self, 'trained_feature_columns_', None)
            }
            torch.save(state, path)
            logger.info(f"LSTM model saved to {path}")
        except Exception as e:
            logger.error(f"Error saving LSTM model: {e}", exc_info=True)

    def load_model(self, path: str):
        try:
            state = torch.load(path, weights_only=False)
            self.model.load_state_dict(state['model_state_dict'])
            self.scaler = state['scaler']
            self.params = state['params']
            self.trained_feature_columns_ = state.get('trained_feature_columns')
            self.model.eval()
            self.model_path = path
            logger.info(f"LSTM model loaded from {path}")
        except FileNotFoundError:
            logger.error(f"LSTM model file not found at {path}.")
            self.model = None
        except Exception as e:
            logger.error(f"Error loading LSTM model: {e}", exc_info=True)
            self.model = None
</code>

kamikaze_komodo/ml_models/regime_detection/__init__.py:
<code>
# kamikaze_komodo/ml_models/regime_detection/__init__.py
# This file makes the 'regime_detection' directory a Python package.
</code>

kamikaze_komodo/ml_models/regime_detection/kmeans_regime_model.py:
<code>
# kamikaze_komodo/ml_models/regime_detection/kmeans_regime_model.py
import pandas as pd
import numpy as np
import joblib
import os
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from typing import Optional, Dict, Any, List

from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from datetime import datetime, timedelta, timezone

logger = get_logger(__name__)

class KMeansRegimeModel:
    """
    Identifies market regimes using K-Means clustering on specified features.
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        self.model_path = model_path
        
        self.n_clusters = int(self.params.get('num_clusters', 3))
        # Features string from config, e.g., "volatility_20d,atr_14d_percentage"
        features_str = self.params.get('featuresforclustering', 'volatility_20d,atr_14d_percentage')
        self.features_for_clustering = [f.strip() for f in features_str.split(',')]

        self.model: Optional[KMeans] = None
        self.scaler: Optional[StandardScaler] = None
        self.cluster_centers_: Optional[np.ndarray] = None # To store cluster centers post-training for interpretation
        self.regime_labels: Optional[Dict[int, str]] = None # To store interpreted labels

        if model_path:
            self.load_model(model_path)
        logger.info(f"KMeansRegimeModel initialized. Clusters: {self.n_clusters}, Features: {self.features_for_clustering}, Model Path: {model_path}")

    def _calculate_feature_volatility_X_day(self, data: pd.DataFrame, window: int = 20) -> pd.Series:
        """Calculates X-day rolling volatility of log returns."""
        if 'close' not in data.columns or len(data) < window:
            return pd.Series(np.nan, index=data.index)
        log_returns = np.log(data['close'] / data['close'].shift(1))
        return log_returns.rolling(window=window).std() * np.sqrt(window) # Annualize for context if daily, or use raw

    def _calculate_feature_atr_X_day_percentage(self, data: pd.DataFrame, window: int = 14) -> pd.Series:
        """Calculates X-day ATR as a percentage of closing price."""
        if not all(col in data.columns for col in ['high', 'low', 'close']) or len(data) < window:
            return pd.Series(np.nan, index=data.index)
        try:
            import pandas_ta as ta
            atr = ta.atr(high=data['high'], low=data['low'], close=data['close'], length=window)
            if atr is None or data['close'].rolling(window=window).min().eq(0).any(): # Avoid division by zero
                return pd.Series(np.nan, index=data.index)
            atr_percentage = (atr / data['close']) * 100
            return atr_percentage
        except ImportError:
            logger.warning("pandas_ta not found for ATR calculation in KMeansRegimeModel.")
            return pd.Series(np.nan, index=data.index)
        except Exception as e:
            logger.error(f"Error calculating ATR%: {e}")
            return pd.Series(np.nan, index=data.index)


    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        if data.empty:
            return pd.DataFrame()
        df = data.copy()
        
        generated_features = pd.DataFrame(index=df.index)

        for feature_name in self.features_for_clustering:
            if feature_name.startswith('volatility_') and feature_name.endswith('d'):
                try:
                    window = int(feature_name.split('_')[1][:-1])
                    generated_features[feature_name] = self._calculate_feature_volatility_X_day(df, window)
                except ValueError:
                    logger.warning(f"Could not parse window for volatility feature: {feature_name}")
            elif feature_name.startswith('atr_') and feature_name.endswith('d_percentage'):
                try:
                    window = int(feature_name.split('_')[1][:-1]) # atr_14d -> 14
                    generated_features[feature_name] = self._calculate_feature_atr_X_day_percentage(df, window)
                except ValueError:
                    logger.warning(f"Could not parse window for ATR feature: {feature_name}")
            else:
                logger.warning(f"Unsupported feature definition for Kmeans clustering: {feature_name}")
        
        # Do not dropna here; handle it in the methods that need complete data
        return generated_features

    def train(self, historical_data: pd.DataFrame):
        logger.info(f"Starting KMeans Regime Model training. Data shape: {historical_data.shape}")
        feature_df = self.create_features(historical_data).dropna()

        if feature_df.empty or len(feature_df) < self.n_clusters:
            logger.error("Not enough data points after feature creation to train KMeans model.")
            return

        self.scaler = StandardScaler()
        scaled_features = self.scaler.fit_transform(feature_df)

        self.model = KMeans(n_clusters=self.n_clusters, random_state=42, n_init='auto')
        try:
            self.model.fit(scaled_features)
            self.cluster_centers_ = self.scaler.inverse_transform(self.model.cluster_centers_) # Store unscaled centers
            logger.info(f"KMeans Regime Model training completed. Inertia: {self.model.inertia_:.2f}")
            self.interpret_regimes() # Interpret and label after training
        except Exception as e:
            logger.error(f"Error during KMeans model training: {e}", exc_info=True)
            self.model = None
            self.scaler = None

    def interpret_regimes(self) -> Optional[Dict[int, str]]:
        """
        Interprets and labels the clusters based on their feature characteristics.
        Assumes the first feature is related to volatility for labeling.
        Returns:
            Dict[int, str]: A mapping from cluster index to regime label (e.g., {0: 'Ranging', 1: 'Trending', 2: 'High-Volatility/Choppy'}).
        """
        if self.cluster_centers_ is None:
            logger.warning("Model has not been trained or loaded. Cannot interpret regimes.")
            return None

        # Use the first feature (assumed to be volatility) to sort and label clusters.
        # This is a heuristic and might need adjustment based on the chosen features.
        volatility_feature_index = 0
        centers_volatility = self.cluster_centers_[:, volatility_feature_index]
        
        # Sort cluster indices based on their volatility
        sorted_indices = np.argsort(centers_volatility)
        
        # Label them: lowest volatility is 'Ranging', highest is 'High-Volatility', middle is 'Trending'
        # This assumes n_clusters=3. More complex logic is needed for other cluster counts.
        if self.n_clusters == 3:
            self.regime_labels = {
                sorted_indices[0]: "Ranging",
                sorted_indices[1]: "Trending",
                sorted_indices[2]: "High-Volatility/Choppy"
            }
        else: # Generic labeling for other cluster counts
            self.regime_labels = {idx: f"Regime_{i+1}" for i, idx in enumerate(sorted_indices)}
            logger.warning(f"Automatic regime interpretation is set up for 3 clusters. Found {self.n_clusters}, using generic labels.")

        logger.info(f"Interpreted Regime Labels (based on first feature '{self.features_for_clustering[0]}'): {self.regime_labels}")
        return self.regime_labels

    def predict_regimes_for_dataframe(self, data: pd.DataFrame) -> Optional[pd.Series]:
        """
        Predicts the regime for each row in a historical DataFrame.
        """
        if self.model is None or self.scaler is None:
            logger.error("Model or scaler not trained/loaded. Cannot predict regimes for DataFrame.")
            return None

        feature_df = self.create_features(data)
        
        # Create a series to hold the predictions, aligned with the original index
        regime_series = pd.Series(np.nan, index=data.index)
        
        # Filter for rows where features could be calculated
        valid_feature_df = feature_df.dropna()
        if valid_feature_df.empty:
            logger.warning("No valid features could be calculated for the provided DataFrame.")
            return regime_series

        scaled_features = self.scaler.transform(valid_feature_df)
        predictions = self.model.predict(scaled_features)
        
        # Place predictions back into the series with the correct index
        regime_series.loc[valid_feature_df.index] = predictions
        
        return regime_series

    def predict(self, new_data: pd.DataFrame) -> Optional[int]:
        if self.model is None or self.scaler is None:
            logger.error("KMeans model or scaler not trained/loaded. Cannot predict regime.")
            return None
        
        feature_df = self.create_features(new_data)
        if feature_df.empty:
            logger.warning("No features could be created from new_data for KMeans prediction.")
            return None

        last_features = feature_df.iloc[[-1]]
        if last_features.isnull().values.any():
            logger.warning(f"Latest features for KMeans prediction contain NaNs: {last_features}. Cannot predict.")
            return None

        scaled_features = self.scaler.transform(last_features)
        try:
            regime = self.model.predict(scaled_features)[0]
            logger.debug(f"Predicted regime for latest data: {regime}")
            return int(regime)
        except Exception as e:
            logger.error(f"Error during KMeans regime prediction: {e}", exc_info=True)
            return None

    def save_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if self.model is None or self.scaler is None:
            logger.error("No KMeans model or scaler to save.")
            return
        if not _path:
            logger.error("No path specified for saving KMeans model.")
            return
        try:
            model_data = {
                'model': self.model,
                'scaler': self.scaler,
                'features_for_clustering': self.features_for_clustering,
                'n_clusters': self.n_clusters,
                'cluster_centers_': self.cluster_centers_,
                'regime_labels': self.regime_labels
            }
            joblib.dump(model_data, _path)
            logger.info(f"KMeans Regime model saved to {_path}")
        except Exception as e:
            logger.error(f"Error saving KMeans model to {_path}: {e}", exc_info=True)

    def load_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if not _path:
            logger.debug("No path specified for loading KMeans model.")
            return
        try:
            model_data = joblib.load(_path)
            self.model = model_data['model']
            self.scaler = model_data['scaler']
            self.features_for_clustering = model_data.get('features_for_clustering', self.features_for_clustering)
            self.n_clusters = model_data.get('n_clusters', self.n_clusters)
            self.cluster_centers_ = model_data.get('cluster_centers_')
            self.regime_labels = model_data.get('regime_labels')
            self.model_path = _path
            logger.info(f"KMeans Regime model loaded from {_path}. Clusters: {self.n_clusters}, Features: {self.features_for_clustering}")
            if self.cluster_centers_ is not None:
                logger.info(f"Loaded Unscaled Cluster Centers:\n{self.cluster_centers_}")
            if self.regime_labels is not None:
                logger.info(f"Loaded Regime Labels: {self.regime_labels}")
        except FileNotFoundError:
            logger.error(f"KMeans model file not found at {_path}.")
            self.model = None
            self.scaler = None
        except Exception as e:
            logger.error(f"Error loading KMeans model from {_path}: {e}", exc_info=True)
            self.model = None
            self.scaler = None
</code>

kamikaze_komodo/strategy_framework/strategy_manager.py:
<code>
# kamikaze_komodo/strategy_framework/strategy_manager.py
from typing import List, Dict, Any
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.app_logger import get_logger
logger = get_logger(__name__)
class StrategyManager:
    """
    Manages the loading, initialization, and execution of trading strategies.
    """
    def __init__(self):
        self.strategies: List[BaseStrategy] = []
        logger.info("StrategyManager initialized.")
    def add_strategy(self, strategy: BaseStrategy):
        """Adds a strategy instance to the manager."""
        if not isinstance(strategy, BaseStrategy):
            logger.error("Attempted to add an invalid strategy object.")
            raise ValueError("Strategy must be an instance of BaseStrategy.")
        
        self.strategies.append(strategy)
        logger.info(f"Strategy '{strategy.name}' for {strategy.symbol} ({strategy.timeframe}) added to StrategyManager.")
    def remove_strategy(self, strategy_name: str, symbol: str, timeframe: str):
        """Removes a strategy by its name, symbol, and timeframe."""
        initial_count = len(self.strategies)
        self.strategies = [
            s for s in self.strategies 
            if not (s.name == strategy_name and s.symbol == symbol and s.timeframe == timeframe)
        ]
        if len(self.strategies) < initial_count:
            logger.info(f"Strategy '{strategy_name}' for {symbol} ({timeframe}) removed.")
        else:
            logger.warning(f"Strategy '{strategy_name}' for {symbol} ({timeframe}) not found for removal.")
    def load_strategies_from_config(self, config: Dict[str, Any]):
        """
        Loads strategies based on a configuration dictionary.
        This is a placeholder for a more dynamic loading mechanism.
        For now, strategies are added manually or via specific calls.
        """
        # Example:
        # for strategy_config in config.get('strategies', []):
        #     strategy_class = resolve_strategy_class(strategy_config['name']) # Utility to get class from name
        #     params = strategy_config.get('params', {})
        #     symbol = strategy_config.get('symbol')
        #     timeframe = strategy_config.get('timeframe')
        #     if strategy_class and symbol and timeframe:
        #         self.add_strategy(strategy_class(symbol, timeframe, params))
        logger.warning("load_strategies_from_config is a placeholder and not fully implemented.")
        pass
    def on_bar_data_all(self, bar_data: BarData) -> Dict[str, SignalType]:
        """
        Distributes new bar data to all relevant strategies and collects signals.
        A strategy is relevant if the bar_data.symbol and bar_data.timeframe match.
        Returns:
            Dict[str, SignalType]: A dictionary where keys are strategy identifiers
                                   (e.g., "EWMACStrategy_BTC/USD_1h") and values are signals.
        """
        signals_from_strategies: Dict[str, SignalType] = {}
        for strategy in self.strategies:
            if strategy.symbol == bar_data.symbol and strategy.timeframe == bar_data.timeframe:
                signal = strategy.on_bar_data(bar_data)
                if signal: # Only record actual signals, not None or HOLD if not meaningful here
                    strategy_id = f"{strategy.name}_{strategy.symbol.replace('/', '')}_{strategy.timeframe}"
                    signals_from_strategies[strategy_id] = signal
                    logger.debug(f"Signal from {strategy_id}: {signal.name}")
        return signals_from_strategies
    def get_all_strategies(self) -> List[BaseStrategy]:
        return self.strategies
</code>

kamikaze_komodo/strategy_framework/__init__.py:
<code>
# kamikaze_komodo/strategy_framework/__init__.py
# This file makes the 'strategy_framework' directory a Python package.
</code>

kamikaze_komodo/strategy_framework/base_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/base_strategy.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Union
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
import pandas as pd
from pydantic import BaseModel

logger = get_logger(__name__)

class SignalCommand(BaseModel):
    """Represents a command to be executed by the trading engine."""
    signal_type: SignalType
    symbol: str 
    price: Optional[float] = None # For limit/stop orders, or to specify execution price in backtest
    
class BaseStrategy(ABC):
    """
    Abstract base class for all trading strategies.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        self.symbol = symbol
        self.timeframe = timeframe
        self.params = params if params is not None else {}
        self.current_position_status: Optional[SignalType] = None
        
        enable_shorting_val = self.params.get('enableshorting', self.params.get('enable_shorting', False))
        if isinstance(enable_shorting_val, str):
            self.enable_shorting = enable_shorting_val.lower() == 'true'
        else:
            self.enable_shorting = bool(enable_shorting_val)
            
        # Data history for stateful strategies (like ML models)
        self.data_history = pd.DataFrame()
        
        logger.info(f"Initialized BaseStrategy '{self.__class__.__name__}' for {symbol} ({timeframe}). Shorting enabled: {self.enable_shorting}")

    @property
    def name(self) -> str:
        return self.__class__.__name__

    def update_data_history(self, current_bar: BarData):
        """Helper method for stateful strategies to append the latest bar data."""
        new_row_dict = current_bar.model_dump()
        new_row_df = pd.DataFrame([new_row_dict], index=[current_bar.timestamp])
        
        # Ensure index is a DatetimeIndex
        if not isinstance(new_row_df.index, pd.DatetimeIndex):
            new_row_df.index = pd.to_datetime(new_row_df.index, utc=True)

        if self.data_history.empty:
            self.data_history = new_row_df
        else:
            self.data_history = pd.concat([self.data_history, new_row_df])
            # Drop duplicates just in case
            self.data_history = self.data_history[~self.data_history.index.duplicated(keep='last')]


    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculates all necessary indicators and signal conditions in a vectorized manner.
        """
        logger.info(f"'{self.name}' uses default prepare_data. No indicators pre-calculated.")
        return data

    @abstractmethod
    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        """
        Processes a new bar of data and decides on a trading action.
        """
        pass

    def get_parameters(self) -> Dict[str, Any]:
        return self.params

    def set_parameters(self, params: Dict[str, Any]):
        self.params.update(params)
        enable_shorting_val = self.params.get('enableshorting', self.params.get('enable_shorting', False))
        if isinstance(enable_shorting_val, str):
            self.enable_shorting = enable_shorting_val.lower() == 'true'
        else:
            self.enable_shorting = bool(enable_shorting_val)
        logger.info(f"Strategy {self.name} parameters updated: {self.params}. Shorting enabled: {self.enable_shorting}")
</code>

kamikaze_komodo/strategy_framework/strategies/regime_switching_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/regime_switching_strategy.py
from typing import Dict, Any, Optional, Union, List, Type
import pandas as pd
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class RegimeSwitchingStrategy(BaseStrategy):
    """
    A meta-strategy that switches between different trading strategies based on the market regime.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None,
                 strategy_mapping: Dict[int, BaseStrategy] = None,
                 regime_labels: Optional[Dict[int, str]] = None):
        super().__init__(symbol, timeframe, params)
        
        if strategy_mapping is None:
            raise ValueError("RegimeSwitchingStrategy requires a 'strategy_mapping' dictionary.")
            
        self.strategy_mapping = strategy_mapping
        self.regime_labels = regime_labels if regime_labels is not None else {}
        self.active_regime: Optional[int] = None
        
        self.regime_confirmation_period = int(self.params.get('regime_confirmation_period', 3))
        # IMPROVEMENT: Add a cooldown period to prevent whipsawing
        self.regime_cooldown_period = int(self.params.get('regime_cooldown_period', 5))
        self.last_regime_switch_bar: int = -self.regime_cooldown_period
        self.previous_regime: Optional[int] = None

        self.pending_regime: Optional[int] = None
        self.consecutive_regime_count: int = 0
        self.current_bar_index: int = 0

        logger.info(f"Initialized RegimeSwitchingStrategy for {symbol} ({timeframe}).")
        logger.info(f"Regime confirmation: {self.regime_confirmation_period} bars, Cooldown: {self.regime_cooldown_period} bars.")
        for regime, strategy in self.strategy_mapping.items():
            regime_name = self.regime_labels.get(regime, f"Regime {regime}")
            logger.info(f" - Mapping {regime_name} to {strategy.name}")
            
    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        logger.info(f"Preparing data for all sub-strategies within '{self.name}'...")
        prepared_df = data.copy()
        for regime, strategy in self.strategy_mapping.items():
            prepared_df = strategy.prepare_data(prepared_df)
        return prepared_df

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.current_bar_index += 1
        current_regime_from_data = int(current_bar.market_regime) if hasattr(current_bar, 'market_regime') and pd.notna(current_bar.market_regime) else None

        if current_regime_from_data is None:
            return SignalType.HOLD

        if self.pending_regime is None:
            self.pending_regime = current_regime_from_data
            self.consecutive_regime_count = 1
        elif current_regime_from_data == self.pending_regime:
            self.consecutive_regime_count += 1
        else:
            self.pending_regime = current_regime_from_data
            self.consecutive_regime_count = 1

        regime_confirmed = self.consecutive_regime_count >= self.regime_confirmation_period
        
        # IMPROVEMENT: Cooldown logic to prevent whipsawing
        is_in_cooldown = (self.current_bar_index - self.last_regime_switch_bar) < self.regime_cooldown_period
        is_flipping_back = self.pending_regime == self.previous_regime

        if regime_confirmed and self.pending_regime != self.active_regime:
            if is_in_cooldown and is_flipping_back:
                logger.debug(f"Regime change from {self.active_regime} to {self.pending_regime} IGNORED due to cooldown.")
            else:
                old_regime_label = self.regime_labels.get(self.active_regime, self.active_regime)
                new_regime_label = self.regime_labels.get(self.pending_regime, self.pending_regime)
                logger.info(f"Regime change CONFIRMED from '{old_regime_label}' to '{new_regime_label}'.")
                
                self.previous_regime = self.active_regime
                self.active_regime = self.pending_regime
                self.last_regime_switch_bar = self.current_bar_index
                
                if self.current_position_status is not None:
                    close_signal = SignalType.CLOSE_LONG if self.current_position_status == SignalType.LONG else SignalType.CLOSE_SHORT
                    self.current_position_status = None
                    for sub_strategy in self.strategy_mapping.values():
                        sub_strategy.current_position_status = None
                    return close_signal
        
        active_strategy = self.strategy_mapping.get(self.active_regime)

        if active_strategy:
            active_strategy.current_position_status = self.current_position_status
            signal = active_strategy.on_bar_data(current_bar)
            
            if isinstance(signal, SignalType):
                if signal in [SignalType.LONG, SignalType.SHORT]:
                    self.current_position_status = signal
                elif signal in [SignalType.CLOSE_LONG, SignalType.CLOSE_SHORT]:
                    self.current_position_status = None
            
            return signal
        else:
            if self.current_position_status is not None:
                close_signal = SignalType.CLOSE_LONG if self.current_position_status == SignalType.LONG else SignalType.CLOSE_SHORT
                self.current_position_status = None
                return close_signal
            
            return SignalType.HOLD
</code>

kamikaze_komodo/strategy_framework/strategies/bollinger_band_breakout_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/bollinger_band_breakout_strategy.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BollingerBandBreakoutStrategy(BaseStrategy):
    """
    Implements a Bollinger Band Breakout strategy.
    Enters on price breakouts from Bollinger Bands, potentially filtered by volume or momentum.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.bb_period = int(self.params.get('bb_period', 20))
        self.bb_std_dev = float(self.params.get('bb_std_dev', 2.0))
        self.atr_period = int(self.params.get('atr_period', 14))
        self.volume_filter_enabled = str(self.params.get('volume_filter_enabled', 'false')).lower() == 'true'
        self.volume_sma_period = int(self.params.get('volume_sma_period', 20))
        self.volume_factor_above_sma = float(self.params.get('volume_factor_above_sma', 1.5))
        self.min_breakout_atr_multiple = float(self.params.get('min_breakout_atr_multiple', 0.0))

    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        df = data.copy()
        if df.empty or len(df) < self.bb_period:
            return pd.DataFrame()

        # Bollinger Bands
        bbands = ta.bbands(df['close'], length=self.bb_period, std=self.bb_std_dev)
        df['bb_lower'] = bbands[f'BBL_{self.bb_period}_{self.bb_std_dev:.1f}']
        df['bb_upper'] = bbands[f'BBU_{self.bb_period}_{self.bb_std_dev:.1f}']
        
        # ATR
        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)

        # Volume SMA
        if self.volume_filter_enabled:
            df['volume_sma'] = ta.sma(df['volume'], length=self.volume_sma_period)
        
        # --- Vectorized Signal Conditions ---
        # Filters
        volume_ok = True
        if self.volume_filter_enabled:
            volume_ok = df['volume'] > (df['volume_sma'] * self.volume_factor_above_sma)
        
        candle_size_ok = True
        if self.min_breakout_atr_multiple > 0:
            candle_size_ok = (df['high'] - df['low']) > (df['atr'] * self.min_breakout_atr_multiple)

        # Entry Signals
        df['long_entry'] = (df['close'] > df['bb_upper']) & (df['close'].shift(1) <= df['bb_upper'].shift(1)) & volume_ok & candle_size_ok
        df['short_entry'] = (df['close'] < df['bb_lower']) & (df['close'].shift(1) >= df['bb_lower'].shift(1)) & volume_ok & candle_size_ok
        
        # Exit Signals (simple version: close when price re-enters the band)
        df['long_exit'] = (df['close'] < df['bb_upper']) & (df['close'].shift(1) >= df['bb_upper'].shift(1))
        df['short_exit'] = (df['close'] > df['bb_lower']) & (df['close'].shift(1) <= df['bb_lower'].shift(1))

        return df

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        signal = SignalType.HOLD
        
        if self.current_position_status is None: # No position
            if getattr(current_bar, 'long_entry', False):
                signal = SignalType.LONG
                self.current_position_status = SignalType.LONG
            elif getattr(current_bar, 'short_entry', False) and self.enable_shorting:
                signal = SignalType.SHORT
                self.current_position_status = SignalType.SHORT
        elif self.current_position_status == SignalType.LONG:
            if getattr(current_bar, 'long_exit', False):
                signal = SignalType.CLOSE_LONG
                self.current_position_status = None
        elif self.current_position_status == SignalType.SHORT:
            if getattr(current_bar, 'short_exit', False):
                signal = SignalType.CLOSE_SHORT
                self.current_position_status = None
                
        return signal
</code>

kamikaze_komodo/strategy_framework/strategies/ml_forecaster_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/ml_forecaster_strategy.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
import os
import numpy as np
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings as app_settings
from kamikaze_komodo.ml_models.inference_pipelines.lightgbm_inference import LightGBMInference
from kamikaze_komodo.ml_models.inference_pipelines.xgboost_classifier_inference import XGBoostClassifierInference
from kamikaze_komodo.ml_models.inference_pipelines.lstm_inference import LSTMInference

logger = get_logger(__name__)

class MLForecasterStrategy(BaseStrategy):
    """
    A stateful strategy that uses an ML price forecaster to generate trading signals.
    It maintains a history of data to pass to the model for each prediction.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.model_config_section = self.params.get('modelconfigsection', 'LightGBM_Forecaster')
        self.forecaster_type = self.params.get('forecastertype', 'lightgbm')
        
        self.long_threshold = float(self.params.get('longthreshold', 0.0005))
        self.short_threshold = float(self.params.get('shortthreshold', -0.0005))
        self.exit_long_threshold = float(self.params.get('exitlongthreshold', 0.0))
        self.exit_short_threshold = float(self.params.get('exitshortthreshold', 0.0))
        self.min_prediction_confidence = float(self.params.get('minpredictionconfidence', 0.0))
        
        self.inference_engine = self._initialize_inference_engine()
        
        logger.info(
            f"Initialized MLForecasterStrategy for {symbol} ({timeframe}) "
            f"using {self.forecaster_type}. Long Thresh: {self.long_threshold}, Short Thresh: {self.short_threshold}."
        )

    def _initialize_inference_engine(self):
        """Initializes the correct inference engine based on config."""
        try:
            if self.forecaster_type.lower() == 'lightgbm':
                return LightGBMInference(self.symbol, self.timeframe, model_config_section=self.model_config_section)
            elif self.forecaster_type.lower() == 'xgboost_classifier':
                return XGBoostClassifierInference(self.symbol, self.timeframe, model_config_section=self.model_config_section)
            elif self.forecaster_type.lower() == 'lstm':
                return LSTMInference(self.symbol, self.timeframe, model_config_section=self.model_config_section)
            else:
                logger.error(f"Unsupported forecaster_type: {self.forecaster_type}")
                return None
        except Exception as e:
            logger.error(f"Failed to initialize Inference Engine for {self.forecaster_type}: {e}", exc_info=True)
            return None

    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        For ML strategies, prepare_data might only add basic indicators like ATR
        that don't depend on the ML model itself. Feature creation for the model
        is handled by the inference engine on the fly.
        """
        df = data.copy()
        atr_period = int(self.params.get('atr_period', 14))
        if 'high' in df.columns and len(df) >= atr_period:
            df['atr'] = ta.atr(high=df['high'], low=df['low'], close=df['close'], length=atr_period)
        return df

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        # This is a stateful strategy, so it must update its internal history.
        self.update_data_history(current_bar)
        
        if self.inference_engine is None or self.inference_engine.forecaster.model is None:
            return SignalType.HOLD

        min_history_len = int(self.params.get('min_bars_for_prediction', 60))
        if len(self.data_history) < min_history_len:
            return SignalType.HOLD

        prediction_output = self.inference_engine.get_prediction(self.data_history)
        
        if prediction_output is None:
            return SignalType.HOLD
            
        long_signal, short_signal, close_long, close_short = False, False, False, False

        if self.forecaster_type.lower() in ['lightgbm', 'lstm']:
            pred_val = float(prediction_output)
            if pred_val > self.long_threshold: long_signal = True
            if pred_val < self.short_threshold: short_signal = True
            if pred_val < self.exit_long_threshold: close_long = True
            if pred_val > self.exit_short_threshold: close_short = True
        elif self.forecaster_type.lower() == 'xgboost_classifier':
            pred_class = prediction_output.get('predicted_class')
            confidence = prediction_output.get('confidence', 1.0)
            if confidence < self.min_prediction_confidence:
                return SignalType.HOLD
            if pred_class == 0: long_signal = True
            if pred_class == 1: short_signal = True
            if self.current_position_status == SignalType.LONG and pred_class in [1, 2]: close_long = True
            if self.current_position_status == SignalType.SHORT and pred_class in [0, 2]: close_short = True
        
        # State machine for signals
        if self.current_position_status == SignalType.LONG and close_long:
            self.current_position_status = None
            return SignalType.CLOSE_LONG
        if self.current_position_status == SignalType.SHORT and close_short:
            self.current_position_status = None
            return SignalType.CLOSE_SHORT
        if self.current_position_status is None:
            if long_signal:
                self.current_position_status = SignalType.LONG
                return SignalType.LONG
            if short_signal and self.enable_shorting:
                self.current_position_status = SignalType.SHORT
                return SignalType.SHORT

        return SignalType.HOLD
</code>

kamikaze_komodo/strategy_framework/strategies/funding_rate_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/funding_rate_strategy.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class FundingRateStrategy(BaseStrategy):
    """
    Implements a contrarian strategy based on perpetual futures funding rates.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.lookback_period = int(self.params.get('lookback_period', 14))
        self.short_threshold = float(self.params.get('short_threshold', 0.0005))
        self.long_threshold = float(self.params.get('long_threshold', -0.0005))
        self.exit_threshold_short = float(self.params.get('exit_threshold_short', 0.0001))
        self.exit_threshold_long = float(self.params.get('exit_threshold_long', -0.0001))
        # FIX: Add atr_period to be used for ATR calculation
        self.atr_period = int(self.params.get('atr_period', 14))

    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        df = data.copy()
        if 'funding_rate' not in df.columns or df.empty:
            return pd.DataFrame()
            
        df['funding_rate_ma'] = df['funding_rate'].rolling(window=self.lookback_period).mean()

        # FIX: Add ATR calculation to ensure compatibility with ATR-based risk modules
        if len(df) >= self.atr_period:
            df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)

        # Vectorized Signal Conditions
        df['long_entry'] = df['funding_rate_ma'] < self.long_threshold
        df['short_entry'] = df['funding_rate_ma'] > self.short_threshold
        
        df['long_exit'] = df['funding_rate_ma'] > self.exit_threshold_long
        df['short_exit'] = df['funding_rate_ma'] < self.exit_threshold_short
        
        return df

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        signal = SignalType.HOLD

        if self.current_position_status == SignalType.LONG:
            if getattr(current_bar, 'long_exit', False):
                signal = SignalType.CLOSE_LONG
                self.current_position_status = None
        elif self.current_position_status == SignalType.SHORT:
            if getattr(current_bar, 'short_exit', False):
                signal = SignalType.CLOSE_SHORT
                self.current_position_status = None
        else: # No position
            if getattr(current_bar, 'long_entry', False):
                signal = SignalType.LONG
                self.current_position_status = SignalType.LONG
            elif getattr(current_bar, 'short_entry', False) and self.enable_shorting:
                signal = SignalType.SHORT
                self.current_position_status = SignalType.SHORT
                
        return signal
</code>

kamikaze_komodo/strategy_framework/strategies/funding_rate_arbitrage_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/funding_rate_arbitrage_strategy.py
import pandas as pd
from typing import Dict, Any, Optional, Union, List, Tuple
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class FundingRateArbitrageStrategy(BaseStrategy):
    """
    Implements a market-neutral funding rate arbitrage strategy.
    It simultaneously buys a spot asset and sells a futures contract (or vice-versa)
    to collect funding payments while aiming for market neutrality.

    NOTE: This is a specialized strategy. The backtesting engine requires modification
    to handle two simultaneous data feeds (spot and futures) for this strategy to work.
    """
    def __init__(self, symbol_spot: str, symbol_futures: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        # The 'symbol' for the base class is the primary one, e.g., the futures contract.
        super().__init__(symbol=symbol_futures, timeframe=timeframe, params=params)
        
        self.symbol_spot = symbol_spot
        self.symbol_futures = symbol_futures
        
        # Strategy parameters
        self.entry_funding_rate_threshold = float(self.params.get('entry_funding_rate_threshold', 0.0002)) # e.g., 0.02%
        self.exit_funding_rate_threshold = float(self.params.get('exit_funding_rate_threshold', 0.00005)) # e.g., 0.005%
        self.max_basis_pct_threshold = float(self.params.get('max_basis_pct_threshold', 1.0)) # e.g., max 1% deviation between spot and futures

        # State management
        self.in_position = False
        self.position_type = None # "positive_carry" (short futures) or "negative_carry" (long futures)
        
        logger.info(
            f"Initialized FundingRateArbitrageStrategy for Spot:{self.symbol_spot}/Futures:{self.symbol_futures}. "
            f"Entry Threshold: {self.entry_funding_rate_threshold}, Exit Threshold: {self.exit_funding_rate_threshold}"
        )

    def prepare_data(self, data_spot_df: pd.DataFrame, data_futures_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Prepares and aligns data for both spot and futures assets.
        This method is expected to be called by a specialized backtesting setup.
        """
        # For this strategy, data preparation is primarily about ensuring alignment.
        # The backtesting engine is expected to provide aligned data.
        # We can add features like the basis here.
        
        merged_df = pd.merge(
            data_spot_df[['close']].rename(columns={'close': 'spot_close'}),
            data_futures_df[['close', 'funding_rate']].rename(columns={'close': 'futures_close'}),
            left_index=True,
            right_index=True,
            how='inner'
        )
        
        # Calculate basis and basis percentage
        merged_df['basis'] = merged_df['futures_close'] - merged_df['spot_close']
        merged_df['basis_pct'] = (merged_df['basis'] / merged_df['spot_close']) * 100
        
        # Add these calculated features back to the original dataframes
        data_futures_df['basis_pct'] = merged_df['basis_pct']
        
        logger.info("Arbitrage strategy data prepared and basis calculated.")
        return data_spot_df, data_futures_df

    def _calculate_arbitrage_opportunity(self, bar_data_spot: BarData, bar_data_futures: BarData) -> Optional[str]:
        """
        Helper to identify when to enter or exit based on funding rate and basis.
        Returns the type of position to take or None.
        """
        funding_rate = bar_data_futures.funding_rate
        basis_pct = bar_data_futures.basis_pct

        if funding_rate is None or basis_pct is None:
            return None
        
        # Check if basis is within acceptable limits to avoid large price divergences
        if abs(basis_pct) > self.max_basis_pct_threshold:
            return "close" # Signal to close any position due to high divergence

        # Entry condition for positive carry (funding is positive, short futures)
        if funding_rate > self.entry_funding_rate_threshold:
            return "positive_carry"

        # Entry condition for negative carry (funding is negative, long futures)
        if funding_rate < -self.entry_funding_rate_threshold:
            return "negative_carry"
            
        # Exit condition for positive carry
        if self.position_type == "positive_carry" and funding_rate < self.exit_funding_rate_threshold:
            return "close"

        # Exit condition for negative carry
        if self.position_type == "negative_carry" and funding_rate > -self.exit_funding_rate_threshold:
            return "close"
            
        return None

    def _calculate_hedge_ratio(self) -> float:
        """
        Helper to determine the ratio of futures to spot.
        For simple dollar neutrality, it's 1.0. More complex strategies might use beta.
        """
        return 1.0

    def on_bar_data(self, bar_data_spot: BarData, bar_data_futures: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        """
        Processes new bars for both spot and futures to decide on trading actions.
        This is a specialized signature handled by the backtesting engine.
        """
        opportunity = self._calculate_arbitrage_opportunity(bar_data_spot, bar_data_futures)
        commands: List[SignalCommand] = []

        if opportunity == "close" and self.in_position:
            logger.info(f"Closing arbitrage position at {bar_data_spot.timestamp}.")
            if self.position_type == "positive_carry":
                # Close Short Futures, Close Long Spot
                commands.append(SignalCommand(signal_type=SignalType.CLOSE_SHORT, symbol=self.symbol_futures))
                commands.append(SignalCommand(signal_type=SignalType.CLOSE_LONG, symbol=self.symbol_spot))
            elif self.position_type == "negative_carry":
                # Close Long Futures, Close Short Spot
                commands.append(SignalCommand(signal_type=SignalType.CLOSE_LONG, symbol=self.symbol_futures))
                commands.append(SignalCommand(signal_type=SignalType.CLOSE_SHORT, symbol=self.symbol_spot))
            self.in_position = False
            self.position_type = None
            return commands

        if opportunity and not self.in_position:
            if opportunity == "positive_carry":
                logger.info(f"Entering POSITIVE CARRY arbitrage at {bar_data_spot.timestamp}. Funding Rate: {bar_data_futures.funding_rate}")
                # Short Futures, Long Spot
                commands.append(SignalCommand(signal_type=SignalType.SHORT, symbol=self.symbol_futures))
                commands.append(SignalCommand(signal_type=SignalType.LONG, symbol=self.symbol_spot))
                self.in_position = True
                self.position_type = "positive_carry"
                return commands

            if opportunity == "negative_carry":
                logger.info(f"Entering NEGATIVE CARRY arbitrage at {bar_data_spot.timestamp}. Funding Rate: {bar_data_futures.funding_rate}")
                # Long Futures, Short Spot
                commands.append(SignalCommand(signal_type=SignalType.LONG, symbol=self.symbol_futures))
                commands.append(SignalCommand(signal_type=SignalType.SHORT, symbol=self.symbol_spot))
                self.in_position = True
                self.position_type = "negative_carry"
                return commands

        return SignalType.HOLD
</code>

kamikaze_komodo/strategy_framework/strategies/ensemble_ml_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/ensemble_ml_strategy.py
import pandas as pd
import numpy as np
import os
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
from collections import Counter

from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.ml_models.inference_pipelines.lightgbm_inference import LightGBMInference
from kamikaze_komodo.ml_models.inference_pipelines.xgboost_classifier_inference import XGBoostClassifierInference
from kamikaze_komodo.ml_models.inference_pipelines.lstm_inference import LSTMInference
from kamikaze_komodo.config.settings import settings

logger = get_logger(__name__)

class EnsembleMLStrategy(BaseStrategy):
    """
    A stateful ensemble strategy that combines signals from multiple ML models.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.ensemble_method = self.params.get('ensemble_method', 'majority_vote').lower()
        
        lgbm_section = self.params.get('lgbm_config_section', 'LightGBM_Forecaster')
        xgb_section = self.params.get('xgb_config_section', 'XGBoost_Classifier_Forecaster')
        lstm_section = self.params.get('lstm_config_section', 'LSTM_Forecaster')

        # Initialize inference engines only if their model files exist
        self.models = {}
        try:
            lgbm_engine = LightGBMInference(symbol, timeframe, lgbm_section)
            if lgbm_engine.forecaster.model is not None:
                self.models["LGBM"] = lgbm_engine
        except Exception as e:
            logger.error(f"Failed to load LGBM model for ensemble: {e}")
        try:
            xgb_engine = XGBoostClassifierInference(symbol, timeframe, xgb_section)
            if xgb_engine.forecaster.model is not None:
                self.models["XGB"] = xgb_engine
        except Exception as e:
            logger.error(f"Failed to load XGB model for ensemble: {e}")
        try:
            lstm_engine = LSTMInference(symbol, timeframe, lstm_section)
            if lstm_engine.forecaster.model is not None:
                self.models["LSTM"] = lstm_engine
        except Exception as e:
            logger.error(f"Failed to load LSTM model for ensemble: {e}")

        self.regressor_thresholds = settings.get_strategy_params('MLForecaster_Strategy') if settings else {}
        
        self.model_weights = {
            "LGBM": float(self.params.get('model_weights_lgbm', 0.4)),
            "XGB": float(self.params.get('model_weights_xgb', 0.4)),
            "LSTM": float(self.params.get('model_weights_lstm', 0.2))
        }

        logger.info(f"Initialized EnsembleMLStrategy with method: {self.ensemble_method}. Models loaded: {list(self.models.keys())}")

    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculates ATR for compatibility with risk management modules.
        Other features are created on-the-fly by the inference models.
        """
        df = data.copy()
        # FIX: Add ATR calculation to ensure compatibility with ATR-based risk modules
        atr_period = int(self.params.get('atr_period', 14))
        if 'high' in df.columns and len(df) >= atr_period:
            df['atr'] = ta.atr(high=df['high'], low=df['low'], close=df['close'], length=atr_period)
        
        logger.info(f"Ensemble strategy '{self.name}' prepared base data (ATR).")
        return df

    def _get_model_predictions(self) -> Dict[str, Any]:
        predictions = {}
        for name, model_engine in self.models.items():
            predictions[name] = model_engine.get_prediction(self.data_history)
        return predictions

    def _get_ensemble_signal(self, model_predictions: Dict[str, Any]) -> SignalType:
        if self.ensemble_method == 'majority_vote':
            return self._get_majority_vote_signal(model_predictions)
        elif self.ensemble_method == 'weighted_average':
            return self._get_weighted_average_signal(model_predictions)
        return SignalType.HOLD

    def _convert_prediction_to_vote(self, model_name: str, prediction: Any) -> int:
        if prediction is None: return 0
        if model_name == "XGB":
            pred_class = prediction.get('predicted_class')
            if pred_class == 0: return 1
            if pred_class == 1: return -1
            return 0
        else: # Regressors
            long_thresh = self.regressor_thresholds.get('longthreshold', 0.0005)
            short_thresh = self.regressor_thresholds.get('shortthreshold', -0.0005)
            if prediction > long_thresh: return 1
            if prediction < short_thresh: return -1
            return 0

    def _get_majority_vote_signal(self, model_predictions: Dict[str, Any]) -> SignalType:
        votes = [self._convert_prediction_to_vote(name, pred) for name, pred in model_predictions.items() if pred is not None]
        if not votes: return SignalType.HOLD
        
        vote_counts = Counter(votes)
        if vote_counts.get(1, 0) > vote_counts.get(-1, 0): return SignalType.LONG
        if vote_counts.get(-1, 0) > vote_counts.get(1, 0): return SignalType.SHORT
        return SignalType.HOLD

    def _get_weighted_average_signal(self, model_predictions: Dict[str, Any]) -> SignalType:
        weighted_sum, total_weight = 0.0, 0.0
        for name, pred in model_predictions.items():
            if pred is None or name not in self.model_weights: continue
            
            numeric_pred = 0.0
            if name == "XGB":
                probs = pred.get('probabilities')
                if probs and len(probs) == 3: numeric_pred = probs[0] * 1 + probs[1] * -1
            else:
                numeric_pred = float(pred)

            weighted_sum += self.model_weights[name] * numeric_pred
            total_weight += self.model_weights[name]
        
        if total_weight == 0: return SignalType.HOLD
            
        final_score = weighted_sum / total_weight
        
        long_thresh = self.regressor_thresholds.get('longthreshold', 0.0005)
        short_thresh = self.regressor_thresholds.get('shortthreshold', -0.0005)

        if final_score > long_thresh: return SignalType.LONG
        if final_score < short_thresh: return SignalType.SHORT
        return SignalType.HOLD

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(current_bar)
        
        if len(self.data_history) < 65: return SignalType.HOLD

        model_predictions = self._get_model_predictions()
        if not model_predictions: return SignalType.HOLD

        logger.debug(f"{current_bar.timestamp} - Raw Model Predictions: {model_predictions}")
        
        entry_signal = self._get_ensemble_signal(model_predictions)
        
        if self.current_position_status is None:
            if entry_signal == SignalType.LONG:
                self.current_position_status = SignalType.LONG
                return SignalType.LONG
            elif entry_signal == SignalType.SHORT and self.enable_shorting:
                self.current_position_status = SignalType.SHORT
                return SignalType.SHORT
        elif self.current_position_status == SignalType.LONG and entry_signal == SignalType.SHORT:
            self.current_position_status = None
            return SignalType.CLOSE_LONG
        elif self.current_position_status == SignalType.SHORT and entry_signal == SignalType.LONG:
            self.current_position_status = None
            return SignalType.CLOSE_SHORT
                    
        return SignalType.HOLD
</code>

kamikaze_komodo/strategy_framework/strategies/__init__.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/__init__.py
# This file makes the 'strategies' directory a Python package.
</code>

kamikaze_komodo/strategy_framework/strategies/volatility_squeeze_breakout_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/volatility_squeeze_breakout_strategy.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class VolatilitySqueezeBreakoutStrategy(BaseStrategy):
    """
    Implements a Volatility Squeeze Breakout strategy (inspired by TTM Squeeze).
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.bb_period = int(self.params.get('bb_period', 20))
        self.bb_std_dev = float(self.params.get('bb_std_dev', 2.0))
        self.kc_period = int(self.params.get('kc_period', 20))
        self.kc_atr_period = int(self.params.get('kc_atr_period', 10))
        self.kc_atr_multiplier = float(self.params.get('kc_atr_multiplier', 1.5))
        self.atr_period = self.kc_atr_period

    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        df = data.copy()
        min_len = max(self.bb_period, self.kc_period, self.kc_atr_period)
        if df.empty or len(df) < min_len:
            return pd.DataFrame()

        # Bollinger Bands
        bbands = ta.bbands(df['close'], length=self.bb_period, std=self.bb_std_dev)
        df['bb_lower'] = bbands[f'BBL_{self.bb_period}_{self.bb_std_dev:.1f}']
        df['bb_upper'] = bbands[f'BBU_{self.bb_period}_{self.bb_std_dev:.1f}']

        # Keltner Channels
        kc = ta.kc(df['high'], df['low'], df['close'], length=self.kc_period, atr_length=self.kc_atr_period, multiplier=self.kc_atr_multiplier)
        df['kc_lower'] = kc.iloc[:, 0]
        df['kc_upper'] = kc.iloc[:, 2]
        
        # ATR
        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)

        # --- Vectorized Signal Conditions ---
        df['squeeze_on'] = (df['bb_lower'] > df['kc_lower']) & (df['bb_upper'] < df['kc_upper'])
        
        # Squeeze has just been released
        squeeze_released = (df['squeeze_on'].shift(1)) & (~df['squeeze_on'])

        # Entry Signals
        df['long_entry'] = squeeze_released & (df['close'] > df['bb_upper'])
        df['short_entry'] = squeeze_released & (df['close'] < df['bb_lower'])
        
        # Exit Signals (simple: cross back inside the bands)
        df['long_exit'] = df['close'] < df['bb_upper']
        df['short_exit'] = df['close'] > df['bb_lower']

        return df

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        signal = SignalType.HOLD
        
        if self.current_position_status is None:
            if getattr(current_bar, 'long_entry', False):
                signal = SignalType.LONG
                self.current_position_status = SignalType.LONG
            elif getattr(current_bar, 'short_entry', False) and self.enable_shorting:
                signal = SignalType.SHORT
                self.current_position_status = SignalType.SHORT
        elif self.current_position_status == SignalType.LONG:
            if getattr(current_bar, 'long_exit', False):
                signal = SignalType.CLOSE_LONG
                self.current_position_status = None
        elif self.current_position_status == SignalType.SHORT:
            if getattr(current_bar, 'short_exit', False):
                signal = SignalType.CLOSE_SHORT
                self.current_position_status = None
                
        return signal
</code>

kamikaze_komodo/strategy_framework/strategies/pair_trading_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/pair_trading_strategy.py
import pandas as pd
import pandas_ta as ta
import statsmodels.api as sm # For cointegration test
from statsmodels.tsa.stattools import adfuller # For stationarity test on spread
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType, OrderSide
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher # For fetching secondary asset data
from kamikaze_komodo.config.settings import settings as app_settings
from datetime import datetime, timedelta, timezone

logger = get_logger(__name__)

class PairTradingStrategy(BaseStrategy):
    """
    Implements a Pair Trading strategy based on cointegration.
    The 'symbol' parameter in __init__ will be considered asset1.
    Asset2 symbol must be provided in params.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params) # symbol is asset1
        
        self.asset1_symbol = symbol
        self.asset2_symbol = self.params.get('asset2_symbol')
        if not self.asset2_symbol:
            raise ValueError("PairTradingStrategy requires 'asset2_symbol' in params.")

        self.cointegration_lookback_days = int(self.params.get('cointegration_lookback_days', 90))
        self.cointegration_test_pvalue_threshold = float(self.params.get('cointegration_test_pvalue_threshold', 0.05))
        self.spread_zscore_entry_threshold = float(self.params.get('spread_zscore_entry_threshold', 2.0))
        self.spread_zscore_exit_threshold = float(self.params.get('spread_zscore_exit_threshold', 0.5))
        self.spread_calculation_window = int(self.params.get('spread_calculation_window', 20)) # For MA and StdDev of spread

        self.data_history_asset2 = pd.DataFrame(columns=['open', 'high', 'low', 'close', 'volume', 'atr'])
        self.is_cointegrated = False
        self.hedge_ratio: Optional[float] = None # From cointegration regression

        # Internal state for an active pair trade
        self.active_pair_trade_leg1_symbol: Optional[str] = None
        self.active_pair_trade_leg2_symbol: Optional[str] = None
        self.active_pair_trade_direction: Optional[str] = None # "long_spread" or "short_spread"

        logger.info(
            f"Initialized PairTradingStrategy for {self.asset1_symbol} / {self.asset2_symbol} ({timeframe}) "
            f"Cointegration Lookback: {self.cointegration_lookback_days} days, p-value: {self.cointegration_test_pvalue_threshold}. "
            f"Z-Score Entry: {self.spread_zscore_entry_threshold}, Exit: {self.spread_zscore_exit_threshold}. "
            f"Spread Window: {self.spread_calculation_window}. Shorting Enabled: {self.enable_shorting}"
        )
        # Note: self.enable_shorting must be true for pair trading to function correctly.
        if not self.enable_shorting:
            logger.warning(f"PairTradingStrategy for {self.asset1_symbol}/{self.asset2_symbol} has enable_shorting=False. This strategy requires shorting for one leg.")


    async def initialize_strategy_data(self, historical_data_asset1: pd.DataFrame, historical_data_asset2: pd.DataFrame):
        """
        Checks for cointegration using pre-fetched historical data.
        This method is called once at the start by the runner.
        """
        if historical_data_asset1.empty or historical_data_asset2.empty:
            logger.error("PairTradingStrategy received empty historical data for one or both assets.")
            self.is_cointegrated = False
            return

        # Store full history for later indicator calculation on individual assets
        self.data_history = historical_data_asset1.copy()
        self.data_history_asset2 = historical_data_asset2.copy()
        
        # FIX: Merge based on the index since 'timestamp' was set as the index.
        merged_df = pd.merge(
            self.data_history[['close']], 
            self.data_history_asset2[['close']], 
            left_index=True, 
            right_index=True, 
            how='inner', 
            suffixes=('_asset1', '_asset2')
        )
        merged_df.dropna(inplace=True)

        if len(merged_df) < self.spread_calculation_window * 2: # Need enough data
            logger.warning(f"Not enough synchronized historical data for {self.asset1_symbol} and {self.asset2_symbol} for cointegration analysis (found {len(merged_df)} bars).")
            self.is_cointegrated = False
            return
        
        # Check for cointegration using Engle-Granger
        close_asset1 = merged_df['close_asset1']
        close_asset2 = merged_df['close_asset2']

        # OLS regression: asset1 = hedge_ratio * asset2 + const
        model = sm.OLS(close_asset1, sm.add_constant(close_asset2, prepend=True))
        results = model.fit()
        # FIX: Use .iloc for explicit positional access to fix FutureWarning
        self.hedge_ratio = results.params.iloc[1]

        spread = close_asset1 - self.hedge_ratio * close_asset2
        
        # ADF test on the spread to check for stationarity
        adf_result = adfuller(spread.dropna())
        p_value = adf_result[1]

        if p_value < self.cointegration_test_pvalue_threshold:
            self.is_cointegrated = True
            logger.info(f"Pair {self.asset1_symbol}/{self.asset2_symbol} IS cointegrated. ADF p-value: {p_value:.4f}, Hedge Ratio: {self.hedge_ratio:.4f}")
        else:
            self.is_cointegrated = False
            self.hedge_ratio = None # Reset if not cointegrated
            logger.info(f"Pair {self.asset1_symbol}/{self.asset2_symbol} is NOT cointegrated. ADF p-value: {p_value:.4f}")
        
        return


    def _calculate_spread_zscore(self) -> Optional[float]:
        """Calculates the Z-score of the current spread."""
        if self.hedge_ratio is None or len(self.data_history) < self.spread_calculation_window or len(self.data_history_asset2) < self.spread_calculation_window:
            return None
            
        # Ensure both histories have the latest timestamp available
        last_ts_asset1 = self.data_history.index[-1]
        if last_ts_asset1 not in self.data_history_asset2.index:
            logger.debug(f"Latest timestamp {last_ts_asset1} for {self.asset1_symbol} not in {self.asset2_symbol} history. Cannot calculate current spread.")
            return None
            
        close1 = self.data_history['close'].loc[last_ts_asset1]
        close2 = self.data_history_asset2['close'].loc[last_ts_asset1]

        if pd.isna(close1) or pd.isna(close2): return None

        # Calculate historical spread for Z-score
        hist_close1 = self.data_history['close']
        hist_close2 = self.data_history_asset2['close']
        
        merged_closes = pd.merge(hist_close1.rename('c1'), hist_close2.rename('c2'), left_index=True, right_index=True, how='inner')
        if len(merged_closes) < self.spread_calculation_window: return None

        historical_spread = merged_closes['c1'] - self.hedge_ratio * merged_closes['c2']
        
        if len(historical_spread) < self.spread_calculation_window:
            return None
            
        spread_mean = historical_spread.rolling(window=self.spread_calculation_window).mean().iloc[-1]
        spread_std = historical_spread.rolling(window=self.spread_calculation_window).std().iloc[-1]

        if pd.isna(spread_mean) or pd.isna(spread_std) or spread_std == 0:
            return None
            
        current_spread = close1 - self.hedge_ratio * close2
        z_score = (current_spread - spread_mean) / spread_std
        logger.debug(f"Current Spread for {self.asset1_symbol}/{self.asset2_symbol}: {current_spread:.4f}, Mean: {spread_mean:.4f}, Std: {spread_std:.4f}, Z-Score: {z_score:.2f}")
        return z_score

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        # Pair trading decisions are made bar-by-bar based on spread Z-score.
        # This batch method is less suitable. Primary logic will be in on_bar_data.
        logger.warning("generate_signals is not the primary method for PairTradingStrategy; logic is in on_bar_data.")
        return pd.Series(index=data.index, dtype='object').fillna(SignalType.HOLD)


    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        # This strategy relies on the backtest engine to manage and align data for both assets.
        # It updates its internal history for asset1 (self.symbol) when its bar data arrives.
        # The backtest engine must provide the history for asset2 via its `data_feed_df_pair_asset2` parameter.
        if bar_data.symbol == self.asset1_symbol:
            self.update_data_history(bar_data)
        else:
            # This strategy instance should only process data for its primary symbol.
            # The engine is responsible for passing the right data.
            return SignalType.HOLD

        if not self.is_cointegrated or self.hedge_ratio is None:
            return SignalType.HOLD

        current_z_score = self._calculate_spread_zscore()
        if current_z_score is None:
            return SignalType.HOLD

        signals_to_execute: List[SignalCommand] = []

        # Exit logic first
        if self.current_position_status == SignalType.LONG: # Means we are long the spread (Long Asset1, Short Asset2)
            if current_z_score >= self.spread_zscore_exit_threshold:
                logger.info(f"Exiting LONG SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                # Backtest engine needs to get price for asset2 to close
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_LONG, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_SHORT, symbol=self.asset2_symbol))
                self.current_position_status = None
                return signals_to_execute

        elif self.current_position_status == SignalType.SHORT: # Means we are short the spread (Short Asset1, Long Asset2)
            if current_z_score <= -self.spread_zscore_exit_threshold:
                logger.info(f"Exiting SHORT SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_SHORT, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_LONG, symbol=self.asset2_symbol))
                self.current_position_status = None
                return signals_to_execute
        
        # Entry logic if no active pair trade
        if self.current_position_status is None:
            # Entry condition: Long the spread (Asset1 Long, Asset2 Short) if Z-score is very low
            if current_z_score < -self.spread_zscore_entry_threshold:
                logger.info(f"Entering LONG SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                signals_to_execute.append(SignalCommand(signal_type=SignalType.LONG, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.SHORT, symbol=self.asset2_symbol))
                self.current_position_status = SignalType.LONG # Representing "long the spread"
                return signals_to_execute

            # Entry condition: Short the spread (Asset1 Short, Asset2 Long) if Z-score is very high
            elif current_z_score > self.spread_zscore_entry_threshold:
                logger.info(f"Entering SHORT SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                signals_to_execute.append(SignalCommand(signal_type=SignalType.SHORT, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.LONG, symbol=self.asset2_symbol))
                self.current_position_status = SignalType.SHORT # Representing "short the spread"
                return signals_to_execute

        return SignalType.HOLD
</code>

kamikaze_komodo/strategy_framework/strategies/ewmac.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/ewmac.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class EWMACStrategy(BaseStrategy):
    """
    Exponential Weighted Moving Average Crossover (EWMAC) strategy.
    Phase 1 Refactor: Logic moved to vectorized prepare_data method.
    on_bar_data is now stateless and reads pre-calculated signal columns.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        self.short_window = int(self.params.get('shortwindow', 12))
        self.long_window = int(self.params.get('longwindow', 26))
        self.atr_period = int(self.params.get('atr_period', 14))

    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculates EMAs, ATR, and the crossover signal conditions.
        """
        df = data.copy()
        if df.empty or len(df) < self.long_window:
            logger.warning("Not enough data to calculate EWMAC indicators.")
            return df
            
        ema_short_col = f'ema_short'
        ema_long_col = f'ema_long'

        df[ema_short_col] = ta.ema(df['close'], length=self.short_window)
        df[ema_long_col] = ta.ema(df['close'], length=self.long_window)
        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)

        # Vectorized signal generation
        df['golden_cross'] = (df[ema_short_col].shift(1) <= df[ema_long_col].shift(1)) & \
                             (df[ema_short_col] > df[ema_long_col])
        
        df['death_cross'] = (df[ema_short_col].shift(1) >= df[ema_long_col].shift(1)) & \
                            (df[ema_short_col] < df[ema_long_col])
        
        logger.info(f"EWMAC data prepared. Columns added: {ema_short_col}, {ema_long_col}, atr, golden_cross, death_cross")
        return df

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        """
        Generates trade signals based on pre-calculated crossover columns in the BarData.
        """
        is_golden_cross = getattr(current_bar, 'golden_cross', False)
        is_death_cross = getattr(current_bar, 'death_cross', False)

        signal = SignalType.HOLD
        
        if self.current_position_status == SignalType.LONG:
            if is_death_cross:
                signal = SignalType.CLOSE_LONG
                self.current_position_status = None
        elif self.current_position_status == SignalType.SHORT:
            if is_golden_cross:
                signal = SignalType.CLOSE_SHORT
                self.current_position_status = None
        else: # No position
            if is_golden_cross:
                signal = SignalType.LONG
                self.current_position_status = SignalType.LONG
            elif is_death_cross and self.enable_shorting:
                signal = SignalType.SHORT
                self.current_position_status = SignalType.SHORT
                
        return signal
</code>

kamikaze_komodo/strategy_framework/strategies/bollinger_band_mean_reversion_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/bollinger_band_mean_reversion_strategy.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BollingerBandMeanReversionStrategy(BaseStrategy):
    """
    Implements a Bollinger Band Mean Reversion strategy, ideal for ranging markets.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.bb_period = int(self.params.get('bb_period', 20))
        self.bb_std_dev = float(self.params.get('bb_std_dev', 2.0))
        self.atr_period = int(self.params.get('atr_period', 14))

    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        df = data.copy()
        if df.empty or len(df) < self.bb_period:
            return pd.DataFrame()

        bbands = ta.bbands(df['close'], length=self.bb_period, std=self.bb_std_dev)
        df['bb_lower'] = bbands[f'BBL_{self.bb_period}_{self.bb_std_dev:.1f}']
        df['bb_middle'] = bbands[f'BBM_{self.bb_period}_{self.bb_std_dev:.1f}']
        df['bb_upper'] = bbands[f'BBU_{self.bb_period}_{self.bb_std_dev:.1f}']
        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)
        
        # Vectorized Signal Conditions
        df['long_entry'] = (df['close'].shift(1) >= df['bb_lower'].shift(1)) & (df['close'] < df['bb_lower'])
        df['short_entry'] = (df['close'].shift(1) <= df['bb_upper'].shift(1)) & (df['close'] > df['bb_upper'])
        
        df['long_exit'] = (df['close'].shift(1) <= df['bb_middle'].shift(1)) & (df['close'] > df['bb_middle'])
        df['short_exit'] = (df['close'].shift(1) >= df['bb_middle'].shift(1)) & (df['close'] < df['bb_middle'])

        return df

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        signal = SignalType.HOLD

        if self.current_position_status == SignalType.LONG:
            if getattr(current_bar, 'long_exit', False):
                signal = SignalType.CLOSE_LONG
                self.current_position_status = None
        elif self.current_position_status == SignalType.SHORT:
            if getattr(current_bar, 'short_exit', False):
                signal = SignalType.CLOSE_SHORT
                self.current_position_status = None
        else: # No position
            if getattr(current_bar, 'long_entry', False):
                signal = SignalType.LONG
                self.current_position_status = SignalType.LONG
            elif getattr(current_bar, 'short_entry', False) and self.enable_shorting:
                signal = SignalType.SHORT
                self.current_position_status = SignalType.SHORT

        return signal
</code>

kamikaze_komodo/strategy_framework/strategies/ehlers_instantaneous_trendline.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/ehlers_instantaneous_trendline.py
import pandas as pd
import pandas_ta as ta
import numpy as np
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class EhlersInstantaneousTrendlineStrategy(BaseStrategy):
    """
    Implements Ehlers' Instantaneous Trendline strategy.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        self.it_lag_trigger = int(self.params.get('it_lag_trigger', 1))
        self.atr_period = int(self.params.get('atr_period', 14))

    def prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:
        df = data.copy()
        if df.empty or len(df) < 3:
            return pd.DataFrame()

        # Ehlers' Instantaneous Trendline calculation
        close = df['close']
        df['it'] = (close + 2 * close.shift(1) + close.shift(2)) / 4
        df['it_trigger'] = df['it'].shift(self.it_lag_trigger)
        
        # ATR
        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)

        # Vectorized Signal Conditions
        df['bullish_cross'] = (df['it'] > df['it_trigger']) & (df['it'].shift(1) <= df['it_trigger'].shift(1))
        df['bearish_cross'] = (df['it'] < df['it_trigger']) & (df['it'].shift(1) >= df['it_trigger'].shift(1))
        
        return df

    def on_bar_data(self, current_bar: BarData) -> Union[Optional[SignalType], List[SignalCommand]]:
        signal = SignalType.HOLD

        is_bullish_cross = getattr(current_bar, 'bullish_cross', False)
        is_bearish_cross = getattr(current_bar, 'bearish_cross', False)

        if self.current_position_status == SignalType.LONG:
            if is_bearish_cross:
                signal = SignalType.CLOSE_LONG
                self.current_position_status = None
        elif self.current_position_status == SignalType.SHORT:
            if is_bullish_cross:
                signal = SignalType.CLOSE_SHORT
                self.current_position_status = None
        else: # No position
            if is_bullish_cross:
                signal = SignalType.LONG
                self.current_position_status = SignalType.LONG
            elif is_bearish_cross and self.enable_shorting:
                signal = SignalType.SHORT
                self.current_position_status = SignalType.SHORT
                
        return signal
</code>

kamikaze_komodo/portfolio_constructor/rebalancer.py:
<code>
# kamikaze_komodo/portfolio_constructor/rebalancer.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from kamikaze_komodo.core.models import PortfolioSnapshot # For current holdings
from kamikaze_komodo.core.enums import OrderSide, OrderType # For generating orders
from kamikaze_komodo.app_logger import get_logger
import pandas as pd

logger = get_logger(__name__)

class BaseRebalancer(ABC):
    """
    Abstract base class for portfolio rebalancing logic.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def needs_rebalancing(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float], # e.g. {'BTC/USD': 0.6, 'ETH/USD': 0.4} as fractions
        asset_prices: Dict[str, float] # Current market prices for assets {'BTC/USD': 50000, ...}
    ) -> bool:
        """
        Determines if the portfolio needs rebalancing based on current state and targets.
        Args:
            current_portfolio (PortfolioSnapshot): The current state of the portfolio (contains positions quantity).
            target_allocations_pct (Dict[str, float]): The desired target allocations as percentages.
            asset_prices (Dict[str, float]): Current market prices of the assets.
        Returns:
            bool: True if rebalancing is needed, False otherwise.
        """
        pass

    @abstractmethod
    def generate_rebalancing_orders(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float],
        asset_prices: Dict[str, float] # Current market prices for assets
    ) -> List[Dict[str, Any]]: # List of order parameters
        """
        Generates orders needed to rebalance the portfolio to target allocations.
        Args:
            current_portfolio (PortfolioSnapshot): The current state of the portfolio.
            target_allocations_pct (Dict[str, float]): The desired target allocations as percentages.
            asset_prices (Dict[str, float]): Current market prices of the assets.
        Returns:
            List[Dict[str, Any]]: A list of order parameters (e.g., for exchange_api.create_order).
        """
        pass

class BasicRebalancer(BaseRebalancer):
    """
    Rebalances the portfolio if the current weight of any asset deviates
    from its target weight by more than a specified threshold.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.deviation_threshold = float(self.params.get('rebalancer_deviationthreshold', 0.05)) # Default 5% deviation
        self.min_order_value_usd = float(self.params.get('rebalancer_min_order_value_usd', 10.0)) # Min order value to execute
        logger.info(f"BasicRebalancer initialized with deviation threshold: {self.deviation_threshold*100}%, Min Order Value: ${self.min_order_value_usd}")

    def _get_current_weights(self, current_portfolio: PortfolioSnapshot, asset_prices: Dict[str, float]) -> Dict[str, float]:
        current_weights: Dict[str, float] = {}
        total_value_from_positions = 0.0
        asset_values : Dict[str, float] = {}

        for asset, quantity in current_portfolio.positions.items():
            if asset in asset_prices and asset_prices[asset] > 0:
                value = quantity * asset_prices[asset]
                asset_values[asset] = value
                total_value_from_positions += value
            else:
                logger.warning(f"Price for asset {asset} not available or zero. Cannot calculate its value for rebalancing.")
                asset_values[asset] = 0.0

        # Effective portfolio value for weight calculation is cash + value of positions
        effective_total_value = current_portfolio.cash_balance_usd + total_value_from_positions
        if effective_total_value <= 0:
            return {asset: 0.0 for asset in current_portfolio.positions.keys()}

        for asset, value in asset_values.items():
            current_weights[asset] = value / effective_total_value

        # Add assets that are in target but not in current holdings (weight 0)
        for asset in asset_prices.keys():
            if asset not in current_weights:
                current_weights[asset] = 0.0
        return current_weights


    def needs_rebalancing(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float],
        asset_prices: Dict[str, float]
    ) -> bool:
        if not target_allocations_pct:
            logger.debug("No target allocations provided, rebalancing not needed by BasicRebalancer.")
            return False

        current_weights = self._get_current_weights(current_portfolio, asset_prices)
        if not current_weights and any(v > 0 for v in target_allocations_pct.values()): # No current holdings but target has allocations
            logger.info("Rebalancing needed: No current holdings, but target allocations exist.")
            return True

        all_assets = set(current_weights.keys()).union(set(target_allocations_pct.keys()))

        for asset in all_assets:
            current_w = current_weights.get(asset, 0.0)
            target_w = target_allocations_pct.get(asset, 0.0)
            if abs(current_w - target_w) > self.deviation_threshold:
                logger.info(f"Rebalancing needed for {asset}. Current weight: {current_w:.4f}, Target: {target_w:.4f}, Deviation: {abs(current_w - target_w):.4f} > {self.deviation_threshold:.4f}")
                return True
        logger.debug("BasicRebalancer: No rebalancing needed based on deviation threshold.")
        return False

    def generate_rebalancing_orders(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float],
        asset_prices: Dict[str, float]
    ) -> List[Dict[str, Any]]:
        orders: List[Dict[str, Any]] = []
        if current_portfolio.total_value_usd <=0 :
            logger.warning("Portfolio total value is zero or negative. Cannot generate rebalancing orders.")
            return orders

        # Calculate current values of each asset
        current_asset_values: Dict[str, float] = {}
        for asset, quantity in current_portfolio.positions.items():
            if asset in asset_prices and asset_prices[asset] > 0:
                current_asset_values[asset] = quantity * asset_prices[asset]
            else:
                current_asset_values[asset] = 0.0

        # Calculate target values based on total portfolio value
        target_asset_values: Dict[str, float] = {}
        for asset, target_pct in target_allocations_pct.items():
            target_asset_values[asset] = current_portfolio.total_value_usd * target_pct

        all_assets_in_scope = set(current_asset_values.keys()).union(set(target_asset_values.keys()))

        for asset in all_assets_in_scope:
            current_value = current_asset_values.get(asset, 0.0)
            target_value = target_asset_values.get(asset, 0.0)
            price = asset_prices.get(asset)

            if price is None or price <= 0:
                logger.warning(f"Cannot generate order for {asset}: price is missing or invalid ({price}).")
                continue

            value_diff = target_value - current_value
            amount_to_trade = value_diff / price

            if abs(value_diff) < self.min_order_value_usd: # Skip if trade value is too small
                logger.debug(f"Skipping rebalance for {asset}: change in value ({value_diff:.2f}) is less than min_order_value_usd (${self.min_order_value_usd:.2f}).")
                continue

            if abs(amount_to_trade) > 1e-8: # Ensure there's a non-negligible amount to trade
                order_side = OrderSide.BUY if amount_to_trade > 0 else OrderSide.SELL
                orders.append({
                    'symbol': asset,
                    'type': OrderType.MARKET, # Or allow configurable order type
                    'side': order_side,
                    'amount': abs(amount_to_trade)
                })
                logger.info(f"Generated rebalancing order for {asset}: {order_side.value} {abs(amount_to_trade):.6f} units. Target Value: ${target_value:.2f}, Current Value: ${current_value:.2f}")

        # Orders should ideally be prioritized (e.g., sells before buys if cash is needed)
        # This basic implementation doesn't handle that.
        return orders
</code>

kamikaze_komodo/portfolio_constructor/asset_allocator.py:
<code>
# kamikaze_komodo/portfolio_constructor/asset_allocator.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
import numpy as np
import pandas as pd
from kamikaze_komodo.core.models import BarData # Or other relevant models
from kamikaze_komodo.app_logger import get_logger

# For HRP
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from scipy.spatial.distance import squareform
import scipy.stats

logger = get_logger(__name__)

class BaseAssetAllocator(ABC):
    """
    Abstract base class for asset allocation strategies.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def allocate(
        self,
        assets: List[str], # List of available asset symbols
        portfolio_value: float, # Total current portfolio value
        historical_data: Optional[Dict[str, pd.DataFrame]] = None, # Dict of DataFrames {symbol: df_ohlcv}
        trade_history: Optional[pd.DataFrame] = None # For OptimalF
    ) -> Dict[str, float]: # Target allocation in terms of capital or percentage
        """
        Determines the target allocation for assets.
        Args:
            assets (List[str]): List of asset symbols to consider for allocation.
            portfolio_value (float): Total capital available for allocation.
            historical_data (Optional[Dict[str, pd.DataFrame]]): Historical OHLCV data for assets.
            trade_history (Optional[pd.DataFrame]): For OptimalF, needs past trade performance.
        Returns:
            Dict[str, float]: Dictionary mapping asset symbols to target capital allocation.
        """
        pass

class FixedWeightAssetAllocator(BaseAssetAllocator):
    """
    Allocates assets based on predefined fixed weights.
    """
    def __init__(self, target_weights: Dict[str, float], params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.target_weights = target_weights
        if not self.target_weights:
            logger.warning("FixedWeightAssetAllocator initialized with no target weights.")
        elif abs(sum(self.target_weights.values()) - 1.0) > 1e-6 and sum(self.target_weights.values()) != 0 : # Allow 0 for no allocation
            logger.warning(f"Sum of target weights ({sum(self.target_weights.values())}) is not 1.0. Allocations will be normalized or may behave unexpectedly.")

    def allocate(
        self,
        assets: List[str],
        portfolio_value: float,
        historical_data: Optional[Dict[str, pd.DataFrame]] = None,
        trade_history: Optional[pd.DataFrame] = None
    ) -> Dict[str, float]:
        allocation_targets_capital: Dict[str, float] = {}
        relevant_target_weights = {asset: self.target_weights.get(asset, 0.0) for asset in assets if asset in self.target_weights}
        total_weight_for_relevant_assets = sum(relevant_target_weights.values())

        if total_weight_for_relevant_assets == 0:
            logger.debug("No target weights specified for the given assets or total weight is zero. No allocation.")
            return {asset: 0.0 for asset in assets}

        for asset in assets:
            if asset in relevant_target_weights:
                normalized_weight = relevant_target_weights[asset] / total_weight_for_relevant_assets
                allocation_targets_capital[asset] = portfolio_value * normalized_weight
            else:
                allocation_targets_capital[asset] = 0.0
        logger.debug(f"FixedWeightAssetAllocator target capital allocation: {allocation_targets_capital}")
        return allocation_targets_capital

class OptimalFAllocator(BaseAssetAllocator):
    """
    Allocates capital based on Vince's Optimal f (Kelly Criterion variant).
    Optimal f calculation typically needs a series of past trade returns (HPRs - Holding Period Returns).
    f = ( (R+1) * P - 1 ) / R  where P is win rate, R is avg win / avg loss (payoff ratio).
    This implementation can use defaults or calculate from provided trade_history.
    The allocation is per asset/strategy; this allocator itself doesn't combine multiple Optimal f values.
    It calculates Optimal f for a *single series of trades* that it assumes represents the strategy for the given asset.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.default_win_probability = float(self.params.get('optimalf_default_win_probability', 0.51)) # Slight edge
        self.default_payoff_ratio = float(self.params.get('optimalf_default_payoff_ratio', 1.1)) # AvgWin / AvgLoss
        self.kelly_fraction = float(self.params.get('optimalf_kelly_fraction', 0.25)) # Fraction of optimal f to use (e.g., quarter Kelly)
        self.min_trades_for_stats = int(self.params.get('optimalf_min_trades_for_stats', 20))
        logger.info(f"OptimalFAllocator initialized. Default WinProb: {self.default_win_probability}, Default Payoff: {self.default_payoff_ratio}, Kelly Fraction: {self.kelly_fraction}, Min Trades: {self.min_trades_for_stats}")

    def _calculate_stats_from_history(self, asset_symbol: str, trade_history: Optional[pd.DataFrame]) -> Optional[Dict[str, float]]:
        if trade_history is None or trade_history.empty:
            return None
    
        asset_trades = trade_history[trade_history['symbol'] == asset_symbol]
        if len(asset_trades) < self.min_trades_for_stats:
            logger.debug(f"Not enough trades ({len(asset_trades)}) for {asset_symbol} to calculate Optimal F stats. Using defaults.")
            return None

        wins = asset_trades[asset_trades['pnl'] > 0]['pnl']
        losses = asset_trades[asset_trades['pnl'] < 0]['pnl'].abs() # Losses are positive for payoff ratio calculation

        if len(wins) == 0 or len(losses) == 0: # Avoid division by zero if no wins or no losses
            logger.debug(f"Not enough diversity in trades (wins: {len(wins)}, losses: {len(losses)}) for {asset_symbol}. Using defaults.")
            return None

        win_probability = len(wins) / len(asset_trades)
        average_win = wins.mean()
        average_loss = losses.mean()

        if average_loss == 0: # Avoid division by zero
            logger.debug(f"Average loss is zero for {asset_symbol}. Cannot calculate payoff ratio. Using defaults.")
            return None
        payoff_ratio = average_win / average_loss
        return {"win_probability": win_probability, "payoff_ratio": payoff_ratio}

    def allocate(
        self,
        assets: List[str],
        portfolio_value: float,
        historical_data: Optional[Dict[str, pd.DataFrame]] = None,
        trade_history: Optional[pd.DataFrame] = None
    ) -> Dict[str, float]:
        allocations: Dict[str, float] = {}
        for asset in assets:
            stats = self._calculate_stats_from_history(asset, trade_history)
            win_prob = self.default_win_probability
            payoff_ratio = self.default_payoff_ratio

            if stats:
                win_prob = stats['win_probability']
                payoff_ratio = stats['payoff_ratio']
                logger.info(f"Optimal F for {asset}: Using calculated WinProb={win_prob:.3f}, PayoffRatio={payoff_ratio:.3f}")
            else:
                logger.info(f"Optimal F for {asset}: Using default WinProb={win_prob:.3f}, PayoffRatio={payoff_ratio:.3f}")

            if payoff_ratio <= 0: # Ensure payoff ratio is positive
                optimal_f = -1.0 # Indicates no bet
            else:
                # Kelly formula: f = W - (1-W)/R
                optimal_f = win_prob - ((1 - win_prob) / payoff_ratio)
        
            allocated_capital = 0.0
            if optimal_f > 0:
                fraction_to_invest = optimal_f * self.kelly_fraction
                allocated_capital = portfolio_value * fraction_to_invest
                logger.info(f"Optimal F for {asset}: f*={optimal_f:.4f}, KellyFraction={self.kelly_fraction:.2f}. Target capital: ${allocated_capital:.2f}")
            else:
                logger.info(f"Optimal f for {asset} is not positive ({optimal_f:.4f}). No allocation.")
            allocations[asset] = allocated_capital
    
        # This allocator returns capital per asset based on its own Optimal F.
        # The sum of these allocations could exceed portfolio_value if not careful,
        # or if the user intends this for individual strategy sizing rather than portfolio allocation.
        # For now, it's direct capital per asset. Normalization might be needed by the caller.
        return allocations


class HRPAllocator(BaseAssetAllocator):
    """
    Allocates assets using Hierarchical Risk Parity (HRP) by De Prado.
    Requires historical returns data for assets.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.linkage_method = self.params.get('hrp_linkage_method', 'ward') # common: ward, single, complete
        logger.info(f"HRPAllocator initialized. Linkage method: {self.linkage_method}")

    def _get_cluster_var(self, cov_matrix: pd.DataFrame, cluster_items: List[int]) -> float:
        """Calculates variance of a cluster."""
        cluster_cov_matrix = cov_matrix.iloc[cluster_items, cluster_items]
        parity_w = 1.0 / np.diag(cluster_cov_matrix)
        parity_w = parity_w / parity_w.sum()
        cluster_var = np.dot(parity_w, np.dot(cluster_cov_matrix, parity_w))
        return cluster_var

    def _get_recursive_bisection(self, sort_ix: List[int], current_weights: np.ndarray, cov_matrix: pd.DataFrame) -> np.ndarray:
        """Performs recursive bisection for HRP weights."""
        if len(sort_ix) == 1:
            return current_weights

        # Bisection
        mid_point = len(sort_ix) // 2
        cluster1_items = sort_ix[:mid_point]
        cluster2_items = sort_ix[mid_point:]

        cluster1_var = self._get_cluster_var(cov_matrix, cluster1_items)
        cluster2_var = self._get_cluster_var(cov_matrix, cluster2_items)

        alpha = cluster2_var / (cluster1_var + cluster2_var) # Allocation factor

        # Allocate weights to clusters
        for i in cluster1_items:
            current_weights[i] *= alpha
        for i in cluster2_items:
            current_weights[i] *= (1 - alpha)

        # Recursively bisect sub-clusters
        current_weights = self._get_recursive_bisection(cluster1_items, current_weights, cov_matrix)
        current_weights = self._get_recursive_bisection(cluster2_items, current_weights, cov_matrix)

        return current_weights

    def allocate(
        self,
        assets: List[str],
        portfolio_value: float,
        historical_data: Optional[Dict[str, pd.DataFrame]] = None,
        trade_history: Optional[pd.DataFrame] = None
    ) -> Dict[str, float]:
        if not historical_data or len(assets) < 2:
            logger.warning("HRPAllocator requires historical data for at least 2 assets.")
            # Fallback to equal weight if only one asset or no data
            if len(assets) == 1: return {assets[0]: portfolio_value}
            return {asset: portfolio_value / len(assets) if assets else 0.0 for asset in assets}

        returns_data = {}
        for asset in assets:
            if asset in historical_data and not historical_data[asset].empty:
                returns_data[asset] = historical_data[asset]['close'].pct_change().dropna()
            else:
                logger.warning(f"No historical 'close' data for asset {asset} in HRPAllocator.")
                # Cannot proceed without data for all assets
                return {ast: portfolio_value / len(assets) if assets else 0.0 for ast in assets} # Fallback

        returns_df = pd.DataFrame(returns_data).dropna()
        if returns_df.shape[0] < 2 or returns_df.shape[1] < 2 : # Need enough observations and assets
            logger.warning(f"Not enough processed return data for HRP. Shape: {returns_df.shape}")
            return {asset: portfolio_value / len(assets) if assets else 0.0 for asset in assets}

        cov_matrix = returns_df.cov()
        corr_matrix = returns_df.corr()
    
        # Hierarchical Clustering
        dist_matrix = np.sqrt(0.5 * (1 - corr_matrix)) # Distance matrix
        condensed_dist_matrix = squareform(dist_matrix)
        link = linkage(condensed_dist_matrix, method=self.linkage_method)

        # Quasi-Diagonalization (sorting items by cluster leaves)
        sort_ix = dendrogram(link, no_plot=True)['leaves']

        # Recursive Bisection
        initial_weights = np.ones(len(assets))
        hrp_weights_array = self._get_recursive_bisection(sort_ix, initial_weights, cov_matrix)
    
        # Normalize weights
        hrp_weights = pd.Series(hrp_weights_array, index=[assets[i] for i in sort_ix])
        hrp_weights = hrp_weights / hrp_weights.sum() # Ensure they sum to 1
        hrp_weights = hrp_weights.reindex(assets).fillna(0.0) # Reorder to original asset list and fill NaNs for any missing

        allocations = {asset: portfolio_value * hrp_weights[asset] for asset in assets}
        logger.info(f"HRP Allocator target capital allocation: {allocations}")
        return allocations
</code>

kamikaze_komodo/portfolio_constructor/__init__.py:
<code>
# kamikaze_komodo/portfolio_constructor/__init__.py
# This file makes the 'portfolio_constructor' directory a Python package.
from .base_portfolio_constructor import BasePortfolioConstructor # Export BasePortfolioConstructor
from .asset_allocator import FixedWeightAssetAllocator, OptimalFAllocator, HRPAllocator # Export allocators
from .rebalancer import BasicRebalancer # Export rebalancers
</code>

kamikaze_komodo/portfolio_constructor/base_portfolio_constructor.py:
<code>
# kamikaze_komodo/portfolio_constructor/base_portfolio_constructor.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime

from kamikaze_komodo.core.models import PortfolioSnapshot
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.risk_control_module.risk_manager import RiskManager
from kamikaze_komodo.config.settings import settings as app_settings

logger = get_logger(__name__)

class BasePortfolioConstructor(ABC):
    """
    Abstract base class for portfolio construction and rebalancing.
    Handles target allocation calculation, volatility targeting, and rebalancing triggers.
    """
    def __init__(self, settings: Any, risk_manager: RiskManager):
        self.settings = settings
        self.risk_manager = risk_manager
        
        pc_params = self.settings.get_strategy_params('PortfolioConstructor')
        
        # Volatility Targeting Parameters
        self.volatility_targeting_enabled = pc_params.get('volatility_targeting_enable', False)
        self.target_volatility = float(pc_params.get('target_portfolio_volatility', 0.15))
        self.volatility_lookback_period = int(pc_params.get('volatility_targeting_lookback_period', 60))
        
        # Rebalancing Trigger Parameters
        self.rebalance_threshold_pct = float(pc_params.get('rebalance_threshold_pct', 0.05))

        self.equity_curve_df = pd.DataFrame(columns=['total_value_usd']).set_index(pd.to_datetime([]))

        logger.info(
            f"BasePortfolioConstructor initialized. Vol Targeting: {self.volatility_targeting_enabled} "
            f"(Target: {self.target_volatility:.2%}, Lookback: {self.volatility_lookback_period} bars). "
            f"Rebalance Threshold: {self.rebalance_threshold_pct:.2%}"
        )

    def update_equity_curve(self, timestamp: datetime, total_value: float):
        new_row = pd.DataFrame([{'total_value_usd': total_value}], index=[pd.to_datetime(timestamp, utc=True)])
        self.equity_curve_df = pd.concat([self.equity_curve_df, new_row])

    def _calculate_portfolio_volatility(self) -> float:
        if len(self.equity_curve_df) < self.volatility_lookback_period:
            return 0.0

        relevant_equity = self.equity_curve_df['total_value_usd'].iloc[-self.volatility_lookback_period:]
        returns = relevant_equity.pct_change().dropna()
        
        if returns.empty or returns.std() == 0:
            return 0.0
            
        period_volatility = returns.std()
        annualization_factor = app_settings.config.getint('BacktestingPerformance', 'AnnualizationFactor', fallback=252)
        annualized_volatility = period_volatility * np.sqrt(annualization_factor)
        
        return annualized_volatility

    def adjust_weights_for_volatility_target(self, target_weights: Dict[str, float]) -> Dict[str, float]:
        if not self.volatility_targeting_enabled:
            return target_weights

        current_vol = self._calculate_portfolio_volatility()
        if current_vol <= 1e-8:
            logger.warning("Current portfolio volatility is zero. Cannot apply volatility targeting.")
            return target_weights

        scaling_factor = self.target_volatility / current_vol
        min_scale, max_scale = 0.5, 2.0  # Should be in settings
        scaling_factor = max(min_scale, min(max_scale, scaling_factor))
        
        adjusted_weights = {asset: weight * scaling_factor for asset, weight in target_weights.items()}
        
        logger.info(f"Volatility Targeting: Current Vol={current_vol:.2%}, Target Vol={self.target_volatility:.2%}. Scaling Factor: {scaling_factor:.2f}")
        return adjusted_weights

    @abstractmethod
    def calculate_target_allocations(self, current_portfolio: PortfolioSnapshot, market_data: pd.DataFrame, trades_log: pd.DataFrame) -> Dict[str, float]:
        pass

    def rebalance_portfolio(
        self,
        current_portfolio: PortfolioSnapshot,
        market_data: pd.DataFrame,
        current_prices: Dict[str, float],
        portfolio_value: float,
        trades_log: Optional[pd.DataFrame] = None
    ) -> Dict[str, float]:
        
        target_allocations_pct = self.calculate_target_allocations(current_portfolio, market_data, trades_log)
        adjusted_allocations_pct = self.adjust_weights_for_volatility_target(target_allocations_pct)
        
        current_allocations_pct = {}
        if portfolio_value > 0:
            for asset, quantity in current_portfolio.positions.items():
                current_allocations_pct[asset] = (quantity * current_prices.get(asset, 0)) / portfolio_value
        
        rebalance_needed = False
        all_assets = set(current_allocations_pct.keys()) | set(adjusted_allocations_pct.keys())
        for asset in all_assets:
            current_pct = current_allocations_pct.get(asset, 0.0)
            target_pct = adjusted_allocations_pct.get(asset, 0.0)
            if abs(current_pct - target_pct) > self.rebalance_threshold_pct:
                rebalance_needed = True
                logger.info(f"Rebalancing triggered for {asset}. Current: {current_pct:.2%}, Target: {target_pct:.2%}")
                break
        
        if not rebalance_needed:
            return {}

        final_target_capital = {asset: portfolio_value * pct for asset, pct in adjusted_allocations_pct.items()}
        logger.info(f"Rebalance computed target capital allocations: {final_target_capital}")
        return final_target_capital
</code>

kamikaze_komodo/risk_control_module/triple_barrier_stop.py:
<code>
# FILE: kamikaze_komodo/risk_control_module/triple_barrier_stop.py
from typing import Optional, Dict, Any
from datetime import timedelta
from enum import Enum

from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager
from kamikaze_komodo.core.models import BarData, Trade
from kamikaze_komodo.core.enums import OrderSide
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class StopTriggerType(Enum):
    STOP_LOSS = "STOP_LOSS"
    TAKE_PROFIT = "TAKE_PROFIT"
    TIME_LIMIT = "TIME_LIMIT"

class TripleBarrierStop(BaseStopManager):
    """
    Implements De Prado's Triple-Barrier Method (stop-loss, take-profit, time limit).
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.profit_barrier_multiplier = float(self.params.get('triplebarrier_profitmultiplier', 2.0))
        self.loss_barrier_multiplier = float(self.params.get('triplebarrier_lossmultiplier', 1.0))
        self.time_barrier_days = int(self.params.get('triplebarrier_timelimitdays', 10))
        
        self.active_trade_barriers: Dict[str, Dict[str, Any]] = {} 

        logger.info(
            f"TripleBarrierStop initialized. "
            f"Profit Multiplier: {self.profit_barrier_multiplier}, "
            f"Loss Multiplier: {self.loss_barrier_multiplier}, "
            f"Time Limit: {self.time_barrier_days} days."
        )

    def calculate_barriers(self, trade: Trade, current_bar: BarData):
        if trade.id in self.active_trade_barriers:
            return

        atr_at_entry = trade.custom_fields.get("atr_at_entry")
        if atr_at_entry is None or atr_at_entry <= 0:
            logger.warning(f"ATR at entry not found for trade {trade.id}. Using 1% of entry price as risk unit.")
            risk_unit = trade.entry_price * 0.01
        else:
            risk_unit = atr_at_entry
        
        entry_price = trade.entry_price
        
        if trade.side == OrderSide.BUY:
            sl_price = entry_price - (self.loss_barrier_multiplier * risk_unit)
            tp_price = entry_price + (self.profit_barrier_multiplier * risk_unit)
        else: # SELL
            sl_price = entry_price + (self.loss_barrier_multiplier * risk_unit)
            tp_price = entry_price - (self.profit_barrier_multiplier * risk_unit)
            
        time_barrier_timestamp = trade.entry_timestamp + timedelta(days=self.time_barrier_days)

        self.active_trade_barriers[trade.id] = {
            "sl_price": sl_price,
            "tp_price": tp_price,
            "time_barrier_timestamp": time_barrier_timestamp
        }
        logger.info(f"Barriers calculated for trade {trade.id}: SL={sl_price:.4f}, TP={tp_price:.4f}, Time={time_barrier_timestamp}")

    def check_bar(self, trade: Trade, current_bar: BarData) -> Optional[StopTriggerType]:
        barriers = self.active_trade_barriers.get(trade.id)
        if not barriers:
            return None

        if trade.side == OrderSide.BUY:
            if current_bar.low <= barriers['sl_price']:
                return StopTriggerType.STOP_LOSS
            if current_bar.high >= barriers['tp_price']:
                return StopTriggerType.TAKE_PROFIT
        else: # SELL
            if current_bar.high >= barriers['sl_price']:
                return StopTriggerType.STOP_LOSS
            if current_bar.low <= barriers['tp_price']:
                return StopTriggerType.TAKE_PROFIT
        
        if current_bar.timestamp >= barriers['time_barrier_timestamp']:
            return StopTriggerType.TIME_LIMIT
            
        return None
    
    def reset_for_trade(self, trade_id: str):
        """Clears stored barriers for a specific trade."""
        if trade_id in self.active_trade_barriers:
            del self.active_trade_barriers[trade_id]

    def check_stop_loss(self, current_trade: Trade, latest_bar: BarData, bar_index: int, **kwargs) -> Optional[float]:
        trigger = self.check_bar(current_trade, latest_bar)
        if trigger in [StopTriggerType.STOP_LOSS, StopTriggerType.TIME_LIMIT]:
            # The engine will call reset_for_trade after processing
            return self.active_trade_barriers.get(current_trade.id, {}).get('sl_price', latest_bar.close)
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData, **kwargs) -> Optional[float]:
        trigger = self.check_bar(current_trade, latest_bar)
        if trigger == StopTriggerType.TAKE_PROFIT:
            # The engine will call reset_for_trade after processing
            return self.active_trade_barriers.get(current_trade.id, {}).get('tp_price', latest_bar.close)
        return None
</code>

kamikaze_komodo/risk_control_module/risk_manager.py:
<code>
# kamikaze_komodo/risk_control_module/risk_manager.py
from typing import Dict, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime

from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class RiskManager:
    """
    Manages overall portfolio risk, including drawdown control.
    """
    def __init__(self, settings: Any):
        self.settings = settings
        risk_params = self.settings.get_strategy_params('RiskManagement')
        self.max_portfolio_drawdown_pct = float(risk_params.get('maxportfoliodrawdownpct', 0.20))

        self._peak_equity: float = 0.0
        self._current_drawdown_pct: float = 0.0
        self._trading_halt_active: bool = False

        logger.info(f"RiskManager initialized. Max Portfolio Drawdown: {self.max_portfolio_drawdown_pct * 100:.2f}%")

    def update_portfolio_metrics(self, equity_curve_df: pd.DataFrame, current_timestamp: datetime):
        if equity_curve_df.empty:
            return

        # Ensure the 'total_value_usd' column name from the backtesting engine is used
        equity_values = equity_curve_df['total_value_usd']
        
        # Initialize peak equity with the first value if it's not set
        if self._peak_equity == 0.0 and not equity_values.empty:
            self._peak_equity = equity_values.iloc[0]

        latest_total_value = equity_values.iloc[-1]
        self._peak_equity = max(self._peak_equity, latest_total_value)

        if self._peak_equity > 0:
            self._current_drawdown_pct = (self._peak_equity - latest_total_value) / self._peak_equity
        else:
            self._current_drawdown_pct = 0.0

        logger.debug(
            f"RiskManager Metrics Updated: Equity=${latest_total_value:,.2f}, "
            f"Peak=${self._peak_equity:,.2f}, Drawdown={self._current_drawdown_pct:.2%}"
        )

    def check_portfolio_drawdown(self) -> bool:
        """
        Checks if the current portfolio drawdown exceeds the maximum allowed threshold.
        If breached, sets a trading halt flag. Returns True if breached.
        """
        if self._current_drawdown_pct > self.max_portfolio_drawdown_pct:
            if not self._trading_halt_active:
                self._trading_halt_active = True
                logger.critical(
                    f"PORTFOLIO DRAWDOWN LIMIT BREACHED! "
                    f"Current Drawdown: {self._current_drawdown_pct:.2%}, "
                    f"Max Allowed: {self.max_portfolio_drawdown_pct:.2%}. "
                    f"TRADING HALTED."
                )
            return True
        return False

    def is_trading_halted(self) -> bool:
        return self._trading_halt_active
</code>

kamikaze_komodo/risk_control_module/volatility_band_stop_manager.py:
<code>
# kamikaze_komodo/risk_control_module/volatility_band_stop_manager.py
from typing import Optional, Dict, Any
import pandas as pd
import pandas_ta as ta
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager
from kamikaze_komodo.core.models import BarData, Trade
from kamikaze_komodo.core.enums import OrderSide
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class VolatilityBandStopManager(BaseStopManager):
    """
    Manages stops based on volatility bands like Bollinger Bands or Keltner Channels.
    Can be used for trailing stops along the bands.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.band_type = self.params.get('volatilitybandstop_band_type', 'bollinger').lower() # 'bollinger' or 'keltner'
        
        # Bollinger Band params
        self.bb_period = int(self.params.get('volatilitybandstop_bb_period', 20))
        self.bb_std_dev = float(self.params.get('volatilitybandstop_bb_stddev', 2.0))
        
        # Keltner Channel params (if used)
        self.kc_period = int(self.params.get('volatilitybandstop_kc_period', 20)) # EMA period
        self.kc_atr_period = int(self.params.get('volatilitybandstop_kc_atr_period', 10))
        self.kc_atr_multiplier = float(self.params.get('volatilitybandstop_kc_atr_multiplier', 1.5))

        self.trail_type = self.params.get('volatilitybandstop_trailtype', 'none').lower() # e.g., 'trailing_bb_upper', 'trailing_bb_lower', 'none'
        
        # Store current stop levels if trailing
        self.current_trailing_stop_price: Optional[float] = None

        logger.info(f"VolatilityBandStopManager initialized. Band: {self.band_type}, Trail: {self.trail_type}")

    def _calculate_bands(self, data_history: pd.DataFrame) -> pd.DataFrame:
        df = data_history.copy()
        if df.empty or len(df) < max(self.bb_period, self.kc_period, self.kc_atr_period):
            return df # Not enough data

        if self.band_type == 'bollinger':
            if 'close' in df.columns and len(df) >= self.bb_period:
                try:
                    bbands = ta.bbands(df['close'], length=self.bb_period, std=self.bb_std_dev)
                    if bbands is not None and not bbands.empty:
                        # pandas_ta typically names columns like BBL_20_2.0, BBM_20_2.0, BBU_20_2.0
                        df['band_lower'] = bbands[f'BBL_{self.bb_period}_{self.bb_std_dev:.1f}']
                        df['band_middle'] = bbands[f'BBM_{self.bb_period}_{self.bb_std_dev:.1f}']
                        df['band_upper'] = bbands[f'BBU_{self.bb_period}_{self.bb_std_dev:.1f}']
                except Exception as e:
                    logger.error(f"Error calculating Bollinger Bands for VolStopManager: {e}")
                    df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
            else:
                 df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA

        elif self.band_type == 'keltner':
            if all(c in df.columns for c in ['high', 'low', 'close']) and len(df) >= max(self.kc_period, self.kc_atr_period):
                try:
                    kc = ta.kc(df['high'], df['low'], df['close'], length=self.kc_period, atr_length=self.kc_atr_period, mamode="EMA", multiplier=self.kc_atr_multiplier)
                    if kc is not None and not kc.empty:
                        # Column names from pandas_ta for Keltner might be like KCLer_20_10_1.5, KCMer_20_10_1.5, KCUer_20_10_1.5
                        # Need to verify exact names or use generic ones if possible. Let's assume standard:
                        df['band_lower'] = kc.iloc[:,0] # Lower band often first column
                        df['band_middle'] = kc.iloc[:,1] # Middle band
                        df['band_upper'] = kc.iloc[:,2] # Upper band
                except Exception as e:
                    logger.error(f"Error calculating Keltner Channels for VolStopManager: {e}")
                    df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
            else:
                 df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
        else:
            logger.warning(f"Unsupported band_type: {self.band_type} in VolatilityBandStopManager.")
            df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
        return df

    def check_stop_loss(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        data_history_for_bands: Optional[pd.DataFrame] = None # Pass full history for band calculation
    ) -> Optional[float]:
        if not data_history_for_bands or data_history_for_bands.empty:
            logger.warning("Data history for bands not provided to VolatilityBandStopManager.")
            return None

        df_with_bands = self._calculate_bands(data_history_for_bands)
        if df_with_bands.empty or 'band_lower' not in df_with_bands.columns or 'band_upper' not in df_with_bands.columns:
            logger.debug("Bands not available for stop loss check.")
            return None
        
        latest_band_lower = df_with_bands['band_lower'].iloc[-1]
        latest_band_upper = df_with_bands['band_upper'].iloc[-1]

        if pd.isna(latest_band_lower) or pd.isna(latest_band_upper):
            logger.debug("Latest band values are NaN.")
            return None

        stop_price = None

        if self.trail_type == 'none': # Fixed stop based on band at entry (requires band_at_entry)
            # This simple version will use current bands as stop.
            # For entry-based band stop, band value at entry should be stored in Trade.custom_fields
            if current_trade.side == OrderSide.BUY:
                stop_price = latest_band_lower # Simplistic: stop at current lower band
                if latest_bar.low <= stop_price:
                    logger.info(f"VOL_BAND STOP (BUY, fixed on current) for {current_trade.symbol} at {stop_price:.4f}")
                    return stop_price
            elif current_trade.side == OrderSide.SELL:
                stop_price = latest_band_upper # Simplistic: stop at current upper band
                if latest_bar.high >= stop_price:
                    logger.info(f"VOL_BAND STOP (SELL, fixed on current) for {current_trade.symbol} at {stop_price:.4f}")
                    return stop_price
        else: # Trailing stop logic
            if current_trade.side == OrderSide.BUY:
                # Trail stop along the lower band (or middle band)
                potential_stop = latest_band_lower # Default to lower band for long
                if self.trail_type == 'trailing_bb_middle' or self.trail_type == 'trailing_kc_middle':
                     if 'band_middle' in df_with_bands.columns and pd.notna(df_with_bands['band_middle'].iloc[-1]):
                        potential_stop = df_with_bands['band_middle'].iloc[-1]
                
                if self.current_trailing_stop_price is None or potential_stop > self.current_trailing_stop_price:
                    self.current_trailing_stop_price = potential_stop
                
                if self.current_trailing_stop_price and latest_bar.low <= self.current_trailing_stop_price:
                    logger.info(f"VOL_BAND TRAILING STOP (BUY) for {current_trade.symbol} at {self.current_trailing_stop_price:.4f}")
                    stop_price_to_return = self.current_trailing_stop_price
                    self.current_trailing_stop_price = None # Reset after hit
                    return stop_price_to_return

            elif current_trade.side == OrderSide.SELL:
                # Trail stop along the upper band (or middle band)
                potential_stop = latest_band_upper # Default to upper band for short
                if self.trail_type == 'trailing_bb_middle' or self.trail_type == 'trailing_kc_middle':
                     if 'band_middle' in df_with_bands.columns and pd.notna(df_with_bands['band_middle'].iloc[-1]):
                        potential_stop = df_with_bands['band_middle'].iloc[-1]

                if self.current_trailing_stop_price is None or potential_stop < self.current_trailing_stop_price:
                    self.current_trailing_stop_price = potential_stop

                if self.current_trailing_stop_price and latest_bar.high >= self.current_trailing_stop_price:
                    logger.info(f"VOL_BAND TRAILING STOP (SELL) for {current_trade.symbol} at {self.current_trailing_stop_price:.4f}")
                    stop_price_to_return = self.current_trailing_stop_price
                    self.current_trailing_stop_price = None # Reset after hit
                    return stop_price_to_return
        return None

    def check_take_profit(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        # data_history_for_bands: Optional[pd.DataFrame] = None # If TP uses bands
    ) -> Optional[float]:
        # Volatility bands are typically used for stops or dynamic exits, not fixed TP.
        # Could implement TP if price touches opposite band, e.g.
        # For now, this manager focuses on stop-loss.
        # Reset trailing stop if trade is closed by other means (e.g. strategy signal)
        if self.current_trailing_stop_price is not None and current_trade.exit_timestamp is not None:
             self.current_trailing_stop_price = None
        return None

    def reset_trailing_stop(self):
        """Called when a new trade is initiated or an old one is closed by other means."""
        self.current_trailing_stop_price = None
</code>

kamikaze_komodo/risk_control_module/__init__.py:
<code>
# kamikaze_komodo/risk_control_module/__init__.py
# This file makes the 'risk_control_module' directory a Python package.
from .risk_manager import RiskManager # Export RiskManager for easier import
from .optimal_f_position_sizer import OptimalFPositionSizer # Export new position sizers
from .ml_confidence_position_sizer import MLConfidencePositionSizer # Export new position sizers
from .parabolic_sar_stop import ParabolicSARStop # Export new stop managers
from .triple_barrier_stop import TripleBarrierStop, StopTriggerType # Export new stop managers
logger_name = "KamikazeKomodo.risk_control_module" # Satisfy linter
</code>

kamikaze_komodo/risk_control_module/stop_manager.py:
<code>
# FILE: kamikaze_komodo/risk_control_module/stop_manager.py
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from kamikaze_komodo.core.models import BarData, Trade
from kamikaze_komodo.core.enums import OrderSide
from kamikaze_komodo.app_logger import get_logger
from datetime import timedelta

logger = get_logger(__name__)

class BaseStopManager(ABC):
    """
    Abstract base class for stop-loss and take-profit management.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def check_stop_loss(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        bar_index: int,
        **kwargs  # FIX: Accept arbitrary keyword args for forward compatibility
    ) -> Optional[float]: # Returns stop price if triggered, else None
        """
        Checks if the stop-loss condition is met for the current trade.
        """
        pass

    @abstractmethod
    def check_take_profit(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        **kwargs # FIX: Accept arbitrary keyword args for forward compatibility
    ) -> Optional[float]: # Returns take profit price if triggered, else None
        """
        Checks if the take-profit condition is met for the current trade.
        """
        pass

class PercentageStopManager(BaseStopManager):
    """
    Manages stops based on a fixed percentage from the entry price.
    """
    def __init__(self, stop_loss_pct: Optional[float] = None, take_profit_pct: Optional[float] = None, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.stop_loss_pct = float(self.params.get('percentagestop_losspct', stop_loss_pct if stop_loss_pct is not None else 0))
        self.take_profit_pct = float(self.params.get('percentagestop_takeprofitpct', take_profit_pct if take_profit_pct is not None else 0))

        if self.stop_loss_pct < 0 or self.stop_loss_pct >= 1.0 :
                if self.stop_loss_pct != 0:
                        raise ValueError("stop_loss_pct must be between 0 (inclusive, to disable) and 1 (exclusive).")
        if self.take_profit_pct < 0:
                if self.take_profit_pct != 0:
                        raise ValueError("take_profit_pct must be non-negative (0 to disable).")
        
        self.stop_loss_pct = None if self.stop_loss_pct == 0 else self.stop_loss_pct
        self.take_profit_pct = None if self.take_profit_pct == 0 else self.take_profit_pct
        
        logger.info(f"PercentageStopManager initialized. SL: {self.stop_loss_pct*100 if self.stop_loss_pct else 'N/A'}%, TP: {self.take_profit_pct*100 if self.take_profit_pct else 'N/A'}%")

    def check_stop_loss(self, current_trade: Trade, latest_bar: BarData, bar_index: int, **kwargs) -> Optional[float]:
        if not self.stop_loss_pct or not current_trade or current_trade.entry_price <= 0:
            return None

        if current_trade.side == OrderSide.BUY:
            stop_price = current_trade.entry_price * (1 - self.stop_loss_pct)
            if latest_bar.low <= stop_price:
                logger.info(f"STOP LOSS (BUY) triggered for trade {current_trade.id} ({current_trade.symbol}) at SL price {stop_price:.4f} (Bar Low: {latest_bar.low:.4f}, Entry: {current_trade.entry_price:.4f})")
                return stop_price
        elif current_trade.side == OrderSide.SELL:
            stop_price = current_trade.entry_price * (1 + self.stop_loss_pct)
            if latest_bar.high >= stop_price:
                logger.info(f"STOP LOSS (SELL) triggered for trade {current_trade.id} ({current_trade.symbol}) at SL price {stop_price:.4f} (Bar High: {latest_bar.high:.4f}, Entry: {current_trade.entry_price:.4f})")
                return stop_price
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData, **kwargs) -> Optional[float]:
        if not self.take_profit_pct or not current_trade or current_trade.entry_price <= 0:
            return None

        if current_trade.side == OrderSide.BUY:
            profit_price = current_trade.entry_price * (1 + self.take_profit_pct)
            if latest_bar.high >= profit_price:
                logger.info(f"TAKE PROFIT (BUY) triggered for trade {current_trade.id} ({current_trade.symbol}) at TP price {profit_price:.4f} (Bar High: {latest_bar.high:.4f}, Entry: {current_trade.entry_price:.4f})")
                return profit_price
        elif current_trade.side == OrderSide.SELL:
            profit_price = current_trade.entry_price * (1 - self.take_profit_pct)
            if latest_bar.low <= profit_price:
                logger.info(f"TAKE PROFIT (SELL) triggered for trade {current_trade.id} ({current_trade.symbol}) at TP price {profit_price:.4f} (Bar Low: {latest_bar.low:.4f}, Entry: {current_trade.entry_price:.4f})")
                return profit_price
        return None

class ATRStopManager(BaseStopManager):
    """
    Manages stops based on ATR.
    """
    def __init__(self, atr_multiple: float = 2.0, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.atr_multiple = float(self.params.get('atrstop_atrmultiple', atr_multiple))
        logger.info(f"ATRStopManager initialized with ATR multiple: {self.atr_multiple}")

    def check_stop_loss(self, current_trade: Trade, latest_bar: BarData, bar_index: int, **kwargs) -> Optional[float]:
        atr_at_entry = current_trade.custom_fields.get("atr_at_entry") if hasattr(current_trade, 'custom_fields') and current_trade.custom_fields else None
        
        if atr_at_entry is None or atr_at_entry <= 1e-8:
            if latest_bar.atr is not None and latest_bar.atr > 1e-8:
                logger.debug(f"ATR at entry not available for trade {current_trade.id}. Using latest_bar.atr ({latest_bar.atr:.6f}) for ATR stop check.")
                atr_at_entry = latest_bar.atr
            else:
                logger.debug(f"ATR value not available or invalid for trade {current_trade.id}. Cannot apply ATR stop. ATR at entry: {atr_at_entry}, Latest bar ATR: {latest_bar.atr}")
                return None
        
        stop_distance = self.atr_multiple * atr_at_entry
        if current_trade.side == OrderSide.BUY:
            stop_price = current_trade.entry_price - stop_distance
            if latest_bar.low <= stop_price:
                logger.info(f"ATR STOP LOSS (BUY) for {current_trade.symbol} at {stop_price:.4f} (Entry: {current_trade.entry_price:.4f}, ATR Used: {atr_at_entry:.6f}, BarLow: {latest_bar.low:.4f})")
                return stop_price
        elif current_trade.side == OrderSide.SELL:
            stop_price = current_trade.entry_price + stop_distance
            if latest_bar.high >= stop_price:
                logger.info(f"ATR STOP LOSS (SELL) for {current_trade.symbol} at {stop_price:.4f} (Entry: {current_trade.entry_price:.4f}, ATR Used: {atr_at_entry:.6f}, BarHigh: {latest_bar.high:.4f})")
                return stop_price
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData, **kwargs) -> Optional[float]:
        return None
</code>

kamikaze_komodo/risk_control_module/optimal_f_position_sizer.py:
<code>
# FILE: kamikaze_komodo/risk_control_module/optimal_f_position_sizer.py
from typing import Optional, Dict, Any

from kamikaze_komodo.risk_control_module.position_sizer import BasePositionSizer
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData

logger = get_logger(__name__)

class OptimalFPositionSizer(BasePositionSizer):
    """
    Sizes positions based on a simplified Optimal f or fractional Kelly criterion.
    This requires estimates of win rate and the average win/loss ratio.
    """

    def __init__(self, params: Optional[Dict[str, Any]] = None):
        """
        Initializes the sizer with estimates from the parameters.
        """
        super().__init__(params)
        self.win_rate_estimate = float(self.params.get('optimalf_win_rate_estimate', 0.51))
        self.avg_win_loss_ratio_estimate = float(self.params.get('optimalf_avg_win_loss_ratio_estimate', 1.1))
        self.kelly_fraction = float(self.params.get('optimalf_kelly_fraction', 0.5))

        if self.avg_win_loss_ratio_estimate <= 0:
            raise ValueError("Average win/loss ratio estimate must be greater than zero.")
        
        logger.info(
            f"OptimalFPositionSizer initialized. "
            f"WinRateEstimate={self.win_rate_estimate}, "
            f"AvgWinLossRatio={self.avg_win_loss_ratio_estimate}, "
            f"KellyFraction={self.kelly_fraction}"
        )

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        trade_signal: SignalType,
        strategy_info: Dict[str, Any],
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        """
        Calculates the position size based on the Optimal f formula.
        """
        if current_price <= 0 or current_portfolio_value <= 0:
            return None

        # Kelly formula: f = W - (1-W)/R  (where W is win rate, R is payoff ratio)
        # Simplified variant: f = (W * (R + 1) - 1) / R
        optimal_f = (self.win_rate_estimate * (self.avg_win_loss_ratio_estimate + 1) - 1) / self.avg_win_loss_ratio_estimate
        
        # We only take positions if the edge is positive (f > 0)
        if optimal_f <= 0:
            logger.debug(f"Optimal f for {symbol} is not positive ({optimal_f:.4f}). No position taken.")
            return None

        # Apply the Kelly fraction to be more conservative
        allocation_fraction = optimal_f * self.kelly_fraction
        
        capital_to_allocate = current_portfolio_value * allocation_fraction
        
        # Ensure we don't allocate more than available cash for a new long position
        if trade_signal == SignalType.LONG and capital_to_allocate > available_capital:
            logger.warning(
                f"Optimal F allocation (${capital_to_allocate:,.2f}) exceeds available cash (${available_capital:,.2f}). "
                f"Sizing down to available cash."
            )
            capital_to_allocate = available_capital

        position_size = capital_to_allocate / current_price
        
        logger.info(
            f"OptimalF Sizing for {symbol}: Optimal_f={optimal_f:.4f}, Fraction={allocation_fraction:.4f}, "
            f"Allocating ${capital_to_allocate:,.2f}. Position Size: {position_size:.8f} units."
        )

        return position_size if position_size > 0 else None
</code>

kamikaze_komodo/risk_control_module/ml_confidence_position_sizer.py:
<code>
# FILE: kamikaze_komodo/risk_control_module/ml_confidence_position_sizer.py
from typing import Dict, Optional, Any
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.risk_control_module.position_sizer import BasePositionSizer
from kamikaze_komodo.core.enums import SignalType

logger = get_logger(__name__)

class MLConfidencePositionSizer(BasePositionSizer):
    """
    Sizes positions based on the confidence level of an ML prediction.
    Higher confidence leads to a larger position, within defined bounds.
    Assumes `strategy_info` will contain a 'confidence_score'.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.min_size_factor = float(self.params.get('mlconfidence_min_size_factor', 0.5))
        self.max_size_factor = float(self.params.get('mlconfidence_max_size_factor', 1.5))
        self.base_allocation_fraction = float(self.params.get('mlconfidence_base_allocation_fraction', 0.05))

        if not (0 <= self.min_size_factor <= self.max_size_factor):
            raise ValueError("min_size_factor must be >= 0 and <= max_size_factor.")
        if self.max_size_factor <= 0:
            raise ValueError("max_size_factor must be positive.")

        logger.info(
            f"MLConfidencePositionSizer initialized. "
            f"BaseAllocation={self.base_allocation_fraction*100:.2f}%, "
            f"MinFactor={self.min_size_factor}, MaxFactor={self.max_size_factor}"
        )

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        trade_signal: SignalType,
        strategy_info: Dict[str, Any],
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        if current_price <= 0 or current_portfolio_value <= 0:
            return None

        confidence_score = strategy_info.get('confidence_score')
        if confidence_score is None:
            logger.warning(f"ML 'confidence_score' missing for {symbol}. Cannot use MLConfidencePositionSizer. No trade.")
            return None
        
        confidence_score = max(0.0, min(1.0, float(confidence_score)))

        # Linearly scale the sizing factor based on confidence
        effective_scaling_factor = self.min_size_factor + (self.max_size_factor - self.min_size_factor) * confidence_score
        
        base_capital_to_allocate = current_portfolio_value * self.base_allocation_fraction
        capital_to_allocate = base_capital_to_allocate * effective_scaling_factor

        if trade_signal == SignalType.LONG and capital_to_allocate > available_capital:
            logger.warning(f"Calculated capital (${capital_to_allocate:,.2f}) for {symbol} exceeds cash (${available_capital:,.2f}). Sizing down.")
            capital_to_allocate = available_capital

        position_size = capital_to_allocate / current_price
        
        logger.info(
            f"MLConfidence Sizing for {symbol}: Confidence={confidence_score:.4f}, "
            f"ScalingFactor={effective_scaling_factor:.4f}. "
            f"Allocating ${capital_to_allocate:,.2f}. Position Size: {position_size:.8f} units."
        )
        
        return position_size if position_size > 0 else None
</code>

kamikaze_komodo/risk_control_module/parabolic_sar_stop.py:
<code>
# FILE: kamikaze_komodo/risk_control_module/parabolic_sar_stop.py
from typing import Optional, Dict, Any
import pandas as pd
import pandas_ta as ta
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager
from kamikaze_komodo.core.models import BarData, Trade
from kamikaze_komodo.core.enums import OrderSide
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class ParabolicSARStop(BaseStopManager):
    """
    Implements a dynamic stop-loss based on the Parabolic SAR (Stop and Reverse) indicator.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.acceleration_factor = float(self.params.get('parabolicsar_accelerationfactor', 0.02))
        self.max_acceleration = float(self.params.get('parabolicsar_maxacceleration', 0.2))
        
        logger.info(f"ParabolicSARStop initialized. AF={self.acceleration_factor}, Max AF={self.max_acceleration}")

    def check_stop_loss(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        bar_index: int,
        **kwargs # FIX: Accept kwargs
    ) -> Optional[float]:
        """
        Checks if the Parabolic SAR stop-loss is triggered for the current trade.
        """
        data_history_for_sar = kwargs.get('data_history_for_sar') # FIX: Get data from kwargs
        if data_history_for_sar is None or data_history_for_sar.empty:
            logger.warning("Data history not provided for Parabolic SAR calculation.")
            return None
        
        if len(data_history_for_sar) < 3: # SAR needs at least a few bars
            return None

        try:
            sar_values = ta.sar(
                high=data_history_for_sar['high'],
                low=data_history_for_sar['low'],
                af=self.acceleration_factor,
                max_af=self.max_acceleration
            )
            if sar_values is None or sar_values.empty:
                return None
            
            latest_sar = sar_values.iloc[-1]
            if pd.isna(latest_sar):
                return None
            
            stop_price = float(latest_sar)
            
            if current_trade.side == OrderSide.BUY and latest_bar.low <= stop_price:
                logger.info(f"Parabolic SAR stop (LONG) triggered for {current_trade.symbol} at {stop_price:.4f}")
                return stop_price
            elif current_trade.side == OrderSide.SELL and latest_bar.high >= stop_price:
                logger.info(f"Parabolic SAR stop (SHORT) triggered for {current_trade.symbol} at {stop_price:.4f}")
                return stop_price

        except Exception as e:
            logger.error(f"Error calculating Parabolic SAR: {e}", exc_info=True)
        
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData, **kwargs) -> Optional[float]:
        return None
</code>

kamikaze_komodo/risk_control_module/position_sizer.py:
<code>
# FILE: kamikaze_komodo/risk_control_module/position_sizer.py
from abc import ABC, abstractmethod
from typing import Dict, Optional, Any, Tuple
import numpy as np
from kamikaze_komodo.core.models import BarData # For ATR based sizers potentially
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.core.enums import SignalType # Added for OptimalF and MLConfidence sizers
logger = get_logger(__name__)

class BasePositionSizer(ABC):
    """
    Abstract base class for position sizing strategies.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float, # Total equity
        trade_signal: SignalType, # Added for OptimalF and MLConfidence sizers
        strategy_info: Dict[str, Any], # Added for MLConfidence sizer (e.g. ML confidence)
        latest_bar: Optional[BarData] = None, # For ATR or volatility based
        atr_value: Optional[float] = None # Explicit ATR if available
    ) -> Optional[float]: # Returns position size in asset units, or None if no trade
        """
        Calculates the size of the position to take.
        """
        pass

class FixedFractionalPositionSizer(BasePositionSizer):
    """
    Sizes positions based on a fixed fraction of the total portfolio equity.
    """
    def __init__(self, fraction: float = 0.01, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.fraction_to_allocate = float(self.params.get('fixedfractional_allocationfraction', fraction))
        if not 0 < self.fraction_to_allocate <= 1.0:
            logger.error(f"Fraction must be between 0 (exclusive) and 1 (inclusive). Got {self.fraction_to_allocate}")
            raise ValueError("Fraction must be > 0 and <= 1.")
        logger.info(f"FixedFractionalPositionSizer initialized with fraction: {self.fraction_to_allocate}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float, # Cash
        current_portfolio_value: float, # Equity
        trade_signal: SignalType,
        strategy_info: Dict[str, Any],
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        if current_price <= 0 or current_portfolio_value <= 0:
            return None
        
        capital_to_allocate = current_portfolio_value * self.fraction_to_allocate
        
        if trade_signal == SignalType.LONG and capital_to_allocate > available_capital :
            capital_to_allocate = available_capital
        
        if capital_to_allocate <= 1.0: # Minimum capital to allocate (e.g. $1)
            return None

        position_size = capital_to_allocate / current_price
        # FIX: Change log level to DEBUG to reduce noise
        logger.debug(f"FixedFractional Sizing for {symbol}: Allocating ${capital_to_allocate:.2f}. Position Size: {position_size:.8f} units.")
        return position_size

class ATRBasedPositionSizer(BasePositionSizer):
    """
    Sizes positions based on Average True Range (ATR) to normalize risk per trade.
    """
    def __init__(self, risk_per_trade_fraction: float = 0.01, atr_multiple_for_stop: float = 2.0, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.risk_per_trade_fraction = float(self.params.get('atrbased_riskpertradefraction', risk_per_trade_fraction))
        self.atr_multiple_for_stop = float(self.params.get('atrbased_atrmultipleforstop', atr_multiple_for_stop))
        logger.info(f"ATRBasedPositionSizer initialized with risk_fraction: {self.risk_per_trade_fraction}, atr_multiple: {self.atr_multiple_for_stop}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        trade_signal: SignalType,
        strategy_info: Dict[str, Any],
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        
        effective_atr = atr_value if atr_value is not None else getattr(latest_bar, 'atr', None)
        
        if effective_atr is None or not np.isfinite(effective_atr) or effective_atr <= 1e-8:
            logger.warning(f"ATR value for {symbol} is invalid ({effective_atr}). Cannot size using ATRBasedPositionSizer.")
            return None
        
        if current_price <= 0 or current_portfolio_value <= 0:
            return None

        capital_to_risk = current_portfolio_value * self.risk_per_trade_fraction
        
        stop_distance_per_unit = self.atr_multiple_for_stop * effective_atr
        if stop_distance_per_unit <= 1e-8:
            logger.warning(f"Stop distance per unit is zero or too small for {symbol}. Cannot size position.")
            return None
            
        position_size = capital_to_risk / stop_distance_per_unit
        
        position_cost = position_size * current_price
        if trade_signal == SignalType.LONG and position_cost > available_capital:
            logger.warning(f"Calculated position cost (${position_cost:.2f}) for {symbol} exceeds available cash (${available_capital:.2f}). Reducing size.")
            position_size = available_capital / current_price 
            if position_size <= 1e-8 : return None

        # FIX: Change log level to DEBUG to reduce noise
        logger.debug(f"ATRBased Sizing for {symbol}: Risking ${capital_to_risk:.2f}. "
                    f"ATR: {effective_atr:.6f}, StopDist: ${stop_distance_per_unit:.4f}. "
                    f"Calculated Size: {position_size:.8f} units.")
        return position_size

class PairTradingPositionSizer(BasePositionSizer):
    """
    Sizes positions for a pair trade, aiming for dollar neutrality.
    """
    def __init__(self, dollar_neutral: bool = True, fraction_of_equity_for_pair: float = 0.1, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.dollar_neutral = str(self.params.get('pairtradingpositionsizer_dollarneutral', dollar_neutral)).lower() == 'true'
        self.fraction_of_equity_for_pair = float(self.params.get('fraction_of_equity_for_pair', fraction_of_equity_for_pair))
        logger.info(f"PairTradingPositionSizer initialized. Dollar Neutral: {self.dollar_neutral}, Fraction for Pair: {self.fraction_of_equity_for_pair}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        trade_signal: SignalType,
        strategy_info: Dict[str, Any],
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None,
        other_leg_price: Optional[float] = None,
        hedge_ratio: Optional[float] = None
    ) -> Optional[float]:
        
        if current_price <= 0 or current_portfolio_value <=0: return None

        total_capital_for_pair_trade = current_portfolio_value * self.fraction_of_equity_for_pair

        if self.dollar_neutral:
            capital_for_this_leg = total_capital_for_pair_trade / 2.0
            
            if total_capital_for_pair_trade > available_capital:
                reduction_factor = available_capital / total_capital_for_pair_trade if total_capital_for_pair_trade > 0 else 0
                capital_for_this_leg *= reduction_factor

            if capital_for_this_leg <= 1.0:
                return None
            
            position_size = capital_for_this_leg / current_price
            logger.debug(f"PairTrading Sizing (Dollar Neutral) for leg {symbol}: Capital for leg ${capital_for_this_leg:.2f}. Size: {position_size:.8f} units.")
            return position_size
        else:
            if total_capital_for_pair_trade > available_capital:
                total_capital_for_pair_trade = available_capital

            if total_capital_for_pair_trade <= 1.0: return None
            
            position_size = total_capital_for_pair_trade / current_price
            logger.warning(f"PairTrading Sizing (Non-Dollar Neutral) for leg {symbol} is simplified. Allocating ${total_capital_for_pair_trade:.2f}. Size: {position_size:.8f} units.")
            return position_size
</code>

