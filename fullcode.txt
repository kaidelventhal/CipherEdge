kamikaze_komodo/app_logger.py:
<code>
# kamikaze_komodo/app_logger.py
import logging
import os
from logging.handlers import RotatingFileHandler

LOG_DIR = "logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

log_file_path = os.path.join(LOG_DIR, "kamikaze_komodo.log")

# Configure logging
logger = logging.getLogger("KamikazeKomodo")
logger.setLevel(logging.DEBUG)  # Set to DEBUG to capture all levels of messages

# Create handlers
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)  # Console logs info and above

file_handler = RotatingFileHandler(
    log_file_path, maxBytes=10*1024*1024, backupCount=5  # 10MB per file, 5 backups
)
file_handler.setLevel(logging.DEBUG)  # File logs debug and above

# Create formatters and add it to handlers
log_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s.%(funcName)s - %(message)s')
console_handler.setFormatter(log_format)
file_handler.setFormatter(log_format)

# Add handlers to the logger
if not logger.handlers:
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

def get_logger(module_name: str) -> logging.Logger:
    """
    Returns a logger instance for a specific module.
    """
    return logging.getLogger(f"KamikazeKomodo.{module_name}")
</code>

kamikaze_komodo/main.py:
<code>
# FILE: kamikaze_komodo/main.py
import asyncio
import os
import pandas as pd
from typing import List, Dict, Any, Optional

from kamikaze_komodo.app_logger import get_logger, logger as root_logger
from kamikaze_komodo.config.settings import settings
from kamikaze_komodo.orchestration.scheduler import TaskScheduler
from kamikaze_komodo.orchestration.portfolio_manager import PortfolioManager
from kamikaze_komodo.backtesting_engine.engine import BacktestingEngine
from kamikaze_komodo.backtesting_engine.performance_analyzer import PerformanceAnalyzer

logger = get_logger(__name__)

async def run_portfolio_backtest():
    """
    Initializes and runs the backtesting engine, then prints performance results.
    """
    root_logger.info("--- Initializing Portfolio Backtest ---")

    # First, create the PortfolioManager instance that will be tested.
    # By default, it loads its configuration from the settings file.
    portfolio_manager = PortfolioManager()

    # Use the asynchronous factory `create` to instantiate the engine
    backtest_engine = await BacktestingEngine.create(
        portfolio_manager=portfolio_manager,
        initial_capital=float(settings.config.get('Backtesting', 'InitialCapital', fallback=10000.0)),
        commission_bps=float(settings.config.get('Trading', 'CommissionBPS', fallback=0.0)),
        slippage_bps=float(settings.config.get('Trading', 'SlippageBPS', fallback=0.0)),
    )

    trades_log, final_portfolio, equity_curve_df = await backtest_engine.run()

    # Analyze and print performance
    logger.info("--- Portfolio Backtest Finished ---")
    if final_portfolio:
        logger.info(f"Initial Capital: ${final_portfolio.get('initial_capital', 0):,.2f}")
        logger.info(f"Final Portfolio Value: ${final_portfolio.get('final_portfolio_value', 0):,.2f}")

    if equity_curve_df is not None and not equity_curve_df.empty:
        logger.info("Equity curve generated. See logs or plotting output for details.")

        performance_analyzer = PerformanceAnalyzer(
            trades=trades_log,
            initial_capital=final_portfolio.get('initial_capital', 0),
            final_capital=final_portfolio.get('final_portfolio_value', 0),
            equity_curve_df=equity_curve_df,
            risk_free_rate_annual=float(settings.config.get('BacktestingPerformance', 'RiskFreeRateAnnual')),
            annualization_factor=int(settings.config.get('BacktestingPerformance', 'AnnualizationFactor'))
        )
        metrics = performance_analyzer.calculate_metrics()
        performance_analyzer.print_summary(metrics)

async def run_live_trading():
    """
    Initializes the PortfolioManager and TaskScheduler for live trading.
    """
    logger.info("--- Initializing for LIVE TRADING ---")

    portfolio_manager = PortfolioManager() # ExchangeAPI is initialized internally for live mode
    scheduler_manager = TaskScheduler()

    # Schedule the portfolio manager's main execution cycle
    # The interval should match the strategy's timeframe or desired frequency
    run_interval_minutes = int(settings.config.get('Scheduler', 'RunIntervalMinutes', fallback=240))
    logger.info(f"Scheduling portfolio manager to run every {run_interval_minutes} minutes.")

    scheduler_manager.add_job(
        portfolio_manager.run_cycle,
        'interval',
        minutes=run_interval_minutes,
        id='portfolio_run_cycle'
    )

    try:
        scheduler_manager.start()
        # Keep the application running
        logger.info("Live trading scheduler started. Press Ctrl+C to stop.")
        while True:
            await asyncio.sleep(60)
    except (KeyboardInterrupt, SystemExit):
        logger.info("Shutting down live trading...")
        # await portfolio_manager.close() # Implement close method if needed for cleanup
        scheduler_manager.shutdown()
    logger.info("Live trading shut down.")


async def main():
    """
    Main entry point for the Kamikaze Komodo trading bot.
    Initializes and runs the bot in the mode specified in the config.
    """
    root_logger.info(">>> Kamikaze Komodo Program Starting <<<")
    if not settings:
        root_logger.critical("Settings failed to load. Application cannot start.")
        return

    run_mode = settings.config.get('General', 'RunMode', fallback='backtest').lower()
    root_logger.info(f"Running in '{run_mode}' mode.")

    if run_mode == 'backtest':
        await run_portfolio_backtest()
    elif run_mode == 'live':
        await run_live_trading()
    else:
        logger.error(f"Unknown RunMode '{run_mode}' in config.ini. Use 'live' or 'backtest'.")

    root_logger.info(">>> Kamikaze Komodo Program Finished <<<")

if __name__ == "__main__":
    try:
        # Ensure log directory exists
        if not os.path.exists("logs"):
            os.makedirs("logs")

        asyncio.run(main())
    except KeyboardInterrupt:
        root_logger.info("Kamikaze Komodo program terminated by user.")
    except Exception as e:
        root_logger.critical(f"Critical error in main execution: {e}", exc_info=True)
</code>

kamikaze_komodo/__init__.py:
<code>
# kamikaze_komodo/__init__.py
# This file makes the 'root' directory a Python package.
</code>

kamikaze_komodo/optimizer_main.py:
<code>
# FILE: optimizer_main.py
import asyncio
import pandas as pd
from kamikaze_komodo.backtesting_engine.optimizer import StrategyOptimizer
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.config.settings import settings
from datetime import datetime, timedelta, timezone
from kamikaze_komodo.app_logger import get_logger

# Import all strategies to be tested
from kamikaze_komodo.strategy_framework.strategies.ewmac import EWMACStrategy
from kamikaze_komodo.strategy_framework.strategies.bollinger_band_breakout_strategy import BollingerBandBreakoutStrategy
from kamikaze_komodo.strategy_framework.strategies.ehlers_instantaneous_trendline import EhlersInstantaneousTrendlineStrategy


logger = get_logger("Optimizer")

async def main():
    """
    Main function to set up and run a comparative optimization across multiple strategies.
    """
    if not settings:
        logger.critical("Settings not loaded. Cannot run optimizer.")
        return

    # --- 1. Load Data for a Single Asset ---
    asset_to_optimize = "PF_XBTUSD"
    timeframe = settings.default_timeframe

    logger.info(f"Loading data for optimization: {asset_to_optimize} ({timeframe})")
    db_manager = DatabaseManager()
    data_fetcher = DataFetcher()
    start_date = datetime.now(timezone.utc) - timedelta(days=settings.historical_data_days)

    bars = db_manager.retrieve_bar_data(asset_to_optimize, timeframe, start_date=start_date)
    if not bars or len(bars) < 200:
        logger.info(f"Fetching fresh data for {asset_to_optimize}...")
        bars = await data_fetcher.fetch_historical_data_for_period(asset_to_optimize, timeframe, start_date)
        if bars: db_manager.store_bar_data(bars)

    await data_fetcher.close()
    db_manager.close()

    if not bars:
        logger.error(f"Could not load data for {asset_to_optimize}. Aborting optimization.")
        return

    data_df = pd.DataFrame([bar.model_dump() for bar in bars])
    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
    data_df.set_index('timestamp', inplace=True)
    logger.info(f"Loaded {len(data_df)} bars for {asset_to_optimize}.")

    # --- 2. Define Strategies and Parameter Grids for Optimization ---
    # Common risk parameters to be tested with each strategy
    risk_params = {
        'triplebarrier_tp_multiple': [2.0, 3.0],
        'triplebarrier_sl_multiple': [1.5, 2.0],
        'triplebarrier_time_limit_bars': [20, 30],
    }

    # Dictionary mapping strategy classes to their specific parameter grids
    optimization_tasks = {
        EWMACStrategy: {
            'shortwindow': [20, 30],
            'longwindow': [50, 60],
            **risk_params
        },
        BollingerBandBreakoutStrategy: {
            'bb_period': [20, 30],
            'bb_std_dev': [2.0, 2.5],
            **risk_params
        },
        EhlersInstantaneousTrendlineStrategy: {
            'it_lag_trigger': [1, 2],
            **risk_params
        },
    }

    overall_best_result = {
        'strategy': None,
        'params': None,
        'metric_value': -float('inf')
    }

    optimization_metric = 'sharpe_ratio'

    # --- 3. Loop Through and Optimize Each Strategy ---
    for strategy_class, param_grid in optimization_tasks.items():
        logger.info(f"\n--- Starting Optimization for {strategy_class.__name__} ---")

        optimizer = StrategyOptimizer(
            strategy_class=strategy_class,
            data_feed_df=data_df,
            param_grid=param_grid,
            optimization_metric=optimization_metric,
            initial_capital=float(settings.config.get('Backtesting', 'InitialCapital')),
            commission_bps=float(settings.config.get('Trading', 'CommissionBPS')),
            slippage_bps=float(settings.config.get('Trading', 'SlippageBPS')),
            symbol=asset_to_optimize,
            timeframe=timeframe,
            position_sizer_class_name="FixedFractionalPositionSizer",
            stop_manager_class_name="TripleBarrierStopManager"
        )

        logger.info(f"Starting Grid Search for '{strategy_class.__name__}'...")
        best_params, best_metric, results_df = await optimizer.grid_search()

        logger.info(f"--- Optimization for {strategy_class.__name__} Finished ---")
        if best_params:
            logger.info(f"Best Metric ({optimization_metric}): {best_metric:.4f}")
            logger.info(f"Best Parameters: {best_params}")

            if best_metric > overall_best_result['metric_value']:
                overall_best_result['strategy'] = strategy_class.__name__
                overall_best_result['params'] = best_params
                overall_best_result['metric_value'] = best_metric
                logger.info(f"*** New Overall Best Found: {strategy_class.__name__} with metric {best_metric:.4f} ***")
        else:
            logger.warning(f"Optimization for {strategy_class.__name__} did not yield any successful results.")

    # --- 4. Report Final Winner ---
    logger.info("\n\n---=== OVERALL OPTIMIZATION COMPLETE ===---")
    if overall_best_result['strategy']:
        logger.info(f"The best performing strategy is: {overall_best_result['strategy']}")
        logger.info(f"Best overall metric ({optimization_metric}): {overall_best_result['metric_value']:.4f}")
        logger.info(f"Best overall parameters: {overall_best_result['params']}")
        logger.info("\nUpdate your config.ini with these new 'best' parameters to improve performance.")
    else:
        logger.warning("No successful results found across all strategies.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        logger.critical(f"Critical error in optimizer execution: {e}", exc_info=True)
</code>

kamikaze_komodo/ml_models/__init__.py:
<code>
# kamikaze_komodo/ml_models/__init__.py
# This file makes the 'ml_models' directory a Python package.
</code>

kamikaze_komodo/ml_models/training_pipelines/lightgbm_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/lightgbm_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
from kamikaze_komodo.ml_models.price_forecasting.lightgbm_forecaster import LightGBMForecaster
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class LightGBMTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LightGBM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"lgbm_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path

        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            logger.info(f"Created directory for trained models: {self.model_save_path_dir}")
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.forecaster = LightGBMForecaster(params=self.model_params)
        logger.info(f"LightGBM Training Pipeline initialized for {symbol} ({timeframe}). Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int = 730) -> pd.DataFrame:
        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        logger.info(f"Attempting to retrieve data from DB for {self.symbol} ({self.timeframe}) from {start_date} to {end_date}")
        historical_bars = db_manager.retrieve_bar_data(self.symbol, self.timeframe, start_date, end_date)
        
        required_bars = int(self.model_params.get('minbarsfortraining', 200))
        if not historical_bars or len(historical_bars) < required_bars:
            logger.info(f"Insufficient data in DB ({len(historical_bars)} bars found). Fetching fresh data from exchange...")
            historical_bars = await data_fetcher.fetch_historical_data_for_period(self.symbol, self.timeframe, start_date, end_date)
            if historical_bars:
                db_manager.store_bar_data(historical_bars)
            else:
                logger.error("Failed to fetch training data.")
                await data_fetcher.close()
                db_manager.close()
                return pd.DataFrame()
        
        await data_fetcher.close()
        db_manager.close()
        if not historical_bars:
            logger.error("No historical bars available for training.")
            return pd.DataFrame()
        data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
        data_df.set_index('timestamp', inplace=True)
        data_df.sort_index(inplace=True)
        
        if settings.enable_sentiment_analysis and settings.simulated_sentiment_data_path:
            sentiment_path = settings.simulated_sentiment_data_path
            if os.path.exists(sentiment_path):
                logger.info(f"Loading sentiment data from {sentiment_path} to merge for training.")
                sentiment_df = pd.read_csv(sentiment_path, parse_dates=['timestamp'], index_col='timestamp')
                if not sentiment_df.index.tz:
                    sentiment_df.index = sentiment_df.index.tz_localize('UTC')
                else:
                    sentiment_df.index = sentiment_df.index.tz_convert('UTC')
                
                # --- FIX: Drop existing sentiment_score column to prevent overlap error ---
                if 'sentiment_score' in data_df.columns:
                    data_df = data_df.drop(columns=['sentiment_score'])
                
                data_df = data_df.join(sentiment_df['sentiment_score'], how='left')
                data_df['sentiment_score'].ffill(inplace=True)
                data_df['sentiment_score'].fillna(0.0, inplace=True)
                logger.info("Successfully merged sentiment data into the training set.")
            else:
                logger.warning(f"Sentiment data file not found at {sentiment_path}. Training without sentiment feature.")
                data_df['sentiment_score'] = 0.0
        else:
            logger.info("Sentiment analysis not enabled or no data path provided. Training without sentiment feature.")
            data_df['sentiment_score'] = 0.0
        
        logger.info(f"Fetched and prepared {len(data_df)} bars for training {self.symbol} ({self.timeframe}).")
        return data_df

    async def run_training(self):
        days_history = int(self.model_params.get('trainingdayshistory', 730))
        historical_df = await self.fetch_training_data(days_history=days_history)
        if historical_df.empty:
            logger.error("Cannot run training, no historical data.")
            return
        target_col_name = self.model_params.get('targetcolumnname', 'close_change_lag_1_future')
        
        feature_cols_str = self.model_params.get('feature_columns')
        feature_columns = [col.strip() for col in feature_cols_str.split(',')] if feature_cols_str else None
        
        logger.info(f"Starting training with target: '{target_col_name}', features: {feature_columns if feature_columns else 'default in forecaster'}")
        self.forecaster.train(historical_df, target_column=target_col_name, feature_columns_to_use=feature_columns)
        
        if self.forecaster.model:
            self.forecaster.save_model(self.model_full_save_path)
        else:
            logger.error("Training did not produce a model. Model not saved.")
</code>

kamikaze_komodo/ml_models/training_pipelines/xgboost_classifier_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/xgboost_classifier_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
from kamikaze_komodo.ml_models.price_forecasting.xgboost_classifier_forecaster import XGBoostClassifierForecaster
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class XGBoostClassifierTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "XGBoost_Classifier_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"xgb_classifier_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path
            
        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            logger.info(f"Created directory for trained XGBoost models: {self.model_save_path_dir}")
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.forecaster = XGBoostClassifierForecaster(params=self.model_params)
        logger.info(f"XGBoost Training Pipeline initialized for {symbol} ({timeframe}). Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int = 730) -> pd.DataFrame:
        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        logger.info(f"Attempting to retrieve data from DB for {self.symbol} ({self.timeframe}) from {start_date} to {end_date}")
        historical_bars = db_manager.retrieve_bar_data(self.symbol, self.timeframe, start_date, end_date)
        
        required_bars = int(self.model_params.get('minbarsfortraining', 200))
        if not historical_bars or len(historical_bars) < required_bars:
            logger.info(f"Insufficient data in DB ({len(historical_bars)} bars found). Fetching fresh data for XGBoost training...")
            historical_bars = await data_fetcher.fetch_historical_data_for_period(self.symbol, self.timeframe, start_date, end_date)
            if historical_bars:
                db_manager.store_bar_data(historical_bars)
            else:
                logger.error("Failed to fetch training data for XGBoost.")
                await data_fetcher.close()
                db_manager.close()
                return pd.DataFrame()
        
        await data_fetcher.close()
        db_manager.close()
        if not historical_bars:
            logger.error("No historical bars available for XGBoost training.")
            return pd.DataFrame()
            
        data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
        data_df.set_index('timestamp', inplace=True)
        data_df.sort_index(inplace=True)
        
        if settings.enable_sentiment_analysis and settings.simulated_sentiment_data_path:
            sentiment_path = settings.simulated_sentiment_data_path
            if os.path.exists(sentiment_path):
                logger.info(f"Loading sentiment data from {sentiment_path} to merge for training.")
                sentiment_df = pd.read_csv(sentiment_path, parse_dates=['timestamp'], index_col='timestamp')
                if not sentiment_df.index.tz:
                    sentiment_df.index = sentiment_df.index.tz_localize('UTC')
                else:
                    sentiment_df.index = sentiment_df.index.tz_convert('UTC')
                
                # --- FIX: Drop existing sentiment_score column to prevent overlap error ---
                if 'sentiment_score' in data_df.columns:
                    data_df = data_df.drop(columns=['sentiment_score'])
                    
                data_df = data_df.join(sentiment_df['sentiment_score'], how='left')
                data_df['sentiment_score'].ffill(inplace=True)
                data_df['sentiment_score'].fillna(0.0, inplace=True)
                logger.info("Successfully merged sentiment data into the training set.")
            else:
                logger.warning(f"Sentiment data file not found at {sentiment_path}. Training without sentiment feature.")
                data_df['sentiment_score'] = 0.0
        else:
            logger.info("Sentiment analysis not enabled or no data path provided. Training without sentiment feature.")
            data_df['sentiment_score'] = 0.0

        logger.info(f"Fetched and prepared {len(data_df)} bars for XGBoost training {self.symbol} ({self.timeframe}).")
        return data_df

    async def run_training(self):
        days_history = int(self.model_params.get('trainingdayshistory', 730))
        historical_df = await self.fetch_training_data(days_history=days_history)
        if historical_df.empty:
            logger.error("Cannot run XGBoost training, no historical data.")
            return

        target_def = self.model_params.get('targetdefinition', 'next_bar_direction')
        feature_cols_str = self.model_params.get('feature_columns')
        feature_columns = [col.strip() for col in feature_cols_str.split(',')] if feature_cols_str else None
        
        logger.info(f"Starting XGBoost training with target definition: '{target_def}', features: {feature_columns if feature_columns else 'default in forecaster'}")
        self.forecaster.train(historical_df, target_definition=target_def, feature_columns=feature_columns)
        
        if self.forecaster.model:
            self.forecaster.save_model(self.model_full_save_path)
        else:
            logger.error("XGBoost training did not produce a model. Model not saved.")
</code>

kamikaze_komodo/ml_models/training_pipelines/lstm_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/lstm_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
from kamikaze_komodo.ml_models.price_forecasting.lstm_model import LSTMForecaster
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class LSTMTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LSTM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"lstm_{symbol.replace('/', '_').lower()}_{timeframe}.pth")
        
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path

        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.forecaster = LSTMForecaster(params=self.model_params)
        logger.info(f"LSTM Training Pipeline initialized. Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int) -> pd.DataFrame:
        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        historical_bars = db_manager.retrieve_bar_data(self.symbol, self.timeframe, start_date, end_date)
        
        required_bars = int(self.model_params.get('minbarsfortraining', 200))
        if not historical_bars or len(historical_bars) < required_bars:
            logger.info("Insufficient data in DB. Fetching fresh data for LSTM training...")
            historical_bars = await data_fetcher.fetch_historical_data_for_period(self.symbol, self.timeframe, start_date, end_date)
            if historical_bars:
                db_manager.store_bar_data(historical_bars)
            else:
                logger.error("Failed to fetch training data for LSTM.")
                await data_fetcher.close()
                db_manager.close()
                return pd.DataFrame()
        
        await data_fetcher.close()
        db_manager.close()

        if not historical_bars:
            return pd.DataFrame()
            
        data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
        data_df.set_index('timestamp', inplace=True)
        data_df.sort_index(inplace=True)
        
        if settings.enable_sentiment_analysis and settings.simulated_sentiment_data_path:
            sentiment_path = settings.simulated_sentiment_data_path
            if os.path.exists(sentiment_path):
                logger.info(f"Loading sentiment data from {sentiment_path} to merge for training.")
                sentiment_df = pd.read_csv(sentiment_path, parse_dates=['timestamp'], index_col='timestamp')
                if not sentiment_df.index.tz:
                    sentiment_df.index = sentiment_df.index.tz_localize('UTC')
                else:
                    sentiment_df.index = sentiment_df.index.tz_convert('UTC')
                
                # --- FIX: Drop existing sentiment_score column to prevent overlap error ---
                if 'sentiment_score' in data_df.columns:
                    data_df = data_df.drop(columns=['sentiment_score'])

                data_df = data_df.join(sentiment_df['sentiment_score'], how='left')
                data_df['sentiment_score'].ffill(inplace=True)
                data_df['sentiment_score'].fillna(0.0, inplace=True)
                logger.info("Successfully merged sentiment data into the training set.")
            else:
                logger.warning(f"Sentiment data file not found at {sentiment_path}. Training without sentiment feature.")
                data_df['sentiment_score'] = 0.0
        else:
            logger.info("Sentiment analysis not enabled or no data path provided. Training without sentiment feature.")
            data_df['sentiment_score'] = 0.0

        logger.info(f"Fetched and prepared {len(data_df)} bars for LSTM training.")
        return data_df

    async def run_training(self):
        days_history = int(self.model_params.get('trainingdayshistory', 730))
        historical_df = await self.fetch_training_data(days_history=days_history)

        if historical_df.empty:
            logger.error("Cannot run LSTM training, no historical data.")
            return

        target_col_name = self.model_params.get('targetcolumnname', 'close_change_lag_1_future')
        feature_cols_str = self.model_params.get('featurecolumns')
        feature_columns = [col.strip() for col in feature_cols_str.split(',')] if feature_cols_str else None
        
        self.forecaster.train(historical_df, target_column=target_col_name, feature_columns=feature_columns)
        
        if self.forecaster.model:
            self.forecaster.save_model(self.model_full_save_path)
        else:
            logger.error("LSTM training did not produce a model. Model not saved.")
</code>

kamikaze_komodo/ml_models/training_pipelines/kmeans_regime_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/kmeans_regime_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os

from kamikaze_komodo.ml_models.regime_detection.kmeans_regime_model import KMeansRegimeModel
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class KMeansRegimeTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "KMeans_Regime_Model"):
        if not settings:
            logger.critical("Settings not loaded. KMeansRegimeTrainingPipeline cannot be initialized.")
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        if not self.model_params:
            logger.warning(f"No parameters found for config section [{model_config_section}]. Using defaults for KMeansRegimeModel if any.")
            self.model_params = {} # Ensure it's a dict
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"kmeans_regime_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        # FIX: Use the consistent PROJECT_ROOT from settings.py
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path
            
        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            logger.info(f"Created directory for trained KMeans regime models: {self.model_save_path_dir}")
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.regime_model = KMeansRegimeModel(model_path=None, params=self.model_params) # Don't load, we are training
        logger.info(f"KMeansRegimeTrainingPipeline initialized for {symbol} ({timeframe}). Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int) -> pd.DataFrame:
        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        logger.info(f"Attempting to retrieve data from DB for KMeans training: {self.symbol} ({self.timeframe}) from {start_date} to {end_date}")
        historical_bars = db_manager.retrieve_bar_data(self.symbol, self.timeframe, start_date, end_date)
        
        min_bars_for_features = int(self.model_params.get('minbarsfortraining', 100)) 

        if not historical_bars or len(historical_bars) < min_bars_for_features:
            logger.info(f"Insufficient data in DB ({len(historical_bars)} bars found, need {min_bars_for_features}). Fetching fresh data for KMeans training...")
            historical_bars = await data_fetcher.fetch_historical_data_for_period(self.symbol, self.timeframe, start_date, end_date)
            if historical_bars:
                db_manager.store_bar_data(historical_bars)
            else:
                logger.error("Failed to fetch training data for KMeans.")
                await data_fetcher.close()
                db_manager.close()
                return pd.DataFrame()
        
        await data_fetcher.close()
        db_manager.close()

        if not historical_bars or len(historical_bars) < min_bars_for_features:
            logger.error(f"Still not enough data ({len(historical_bars)} bars) for KMeans training after fetch attempt. Need {min_bars_for_features}.")
            return pd.DataFrame()
            
        data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
        if data_df.empty:
            logger.error("DataFrame is empty after converting BarData list.")
            return pd.DataFrame()
            
        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
        data_df.set_index('timestamp', inplace=True)
        data_df.sort_index(inplace=True)
        
        logger.info(f"Fetched {len(data_df)} bars for KMeans training {self.symbol} ({self.timeframe}).")
        return data_df

    async def run_training(self):
        """
        Fetches data, trains the KMeans regime model, and saves it.
        """
        days_history = int(self.model_params.get('trainingdayshistory', 1095))
        if days_history <=0:
            logger.error(f"TrainingDaysHistory ({days_history}) must be positive. Cannot run training.")
            return

        historical_df = await self.fetch_training_data(days_history=days_history)

        if historical_df.empty:
            logger.error("Cannot run KMeans training, no historical data was retrieved or processed.")
            return

        logger.info(f"Starting KMeans regime model training using {self.regime_model.__class__.__name__}...")
        
        self.regime_model.train(historical_df) 
        
        if self.regime_model.model and self.regime_model.scaler:
            self.regime_model.save_model(self.model_full_save_path)
            logger.info(f"KMeans regime model training completed and model saved to {self.model_full_save_path}.")
        else:
            logger.error("KMeans regime model training did not produce a valid model or scaler. Model not saved.")
</code>

kamikaze_komodo/ml_models/training_pipelines/__init__.py:
<code>
# kamikaze_komodo/ml_models/training_pipelines/__init__.py
# This file makes the 'training_pipelines' directory a Python package.
</code>

kamikaze_komodo/ml_models/training_pipelines/meta_labeling_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/meta_labeling_pipeline.py
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import RandomForestClassifier # Example meta-model
import joblib
import os
from typing import Tuple, Dict, Any, Optional

from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from datetime import datetime, timedelta, timezone

logger = get_logger(__name__)

def get_triple_barrier_events(
    close_prices: pd.Series,
    t_events: pd.DatetimeIndex,
    pt_sl: Tuple[float, float],
    target_vol: pd.Series,
    min_ret: float,
    num_threads: int,
    vertical_barrier_times: Optional[pd.Series] = None,
    side: Optional[pd.Series] = None
) -> pd.DataFrame:
    """
    Generates labels for the triple-barrier method.
    This is a simplified adaptation of De Prado's method.
    """
    if vertical_barrier_times is None:
        raise ValueError("vertical_barrier_times must be provided.")
        
    out = pd.DataFrame(index=t_events)
    out['t1'] = vertical_barrier_times
    out['trgt'] = target_vol

    for loc, t1 in vertical_barrier_times.items():
        df0 = close_prices[loc:t1]
        df0 = (df0 / close_prices[loc] - 1) * (1 if side is None or side.loc[loc] == 1 else -1)
        
        out.loc[loc, 'sl'] = df0[df0 < -pt_sl[1]].index.min()
        out.loc[loc, 'pt'] = df0[df0 > pt_sl[0]].index.min()

    # Determine first touch
    df1 = out.dropna(subset=['sl', 'pt'])
    df1 = df1.copy()
    df1['t1'] = pd.to_datetime(df1['t1'])
    df1['sl'] = pd.to_datetime(df1['sl'])
    df1['pt'] = pd.to_datetime(df1['pt'])
    
    first_touch = df1[['t1', 'sl', 'pt']].min(axis=1)
    
    events = pd.DataFrame(index=t_events)
    events['t1'] = first_touch
    events['trgt'] = out['trgt']
    if side is not None:
        events['side'] = side
    
    return events

def get_bins(events: pd.DataFrame, close_prices: pd.Series) -> pd.DataFrame:
    """
    Computes labels {0, 1} from events.
    1 = take profit was hit, 0 = stop loss or time barrier was hit.
    """
    events_ = events.dropna(subset=['t1'])
    px = events_.index.union(events_['t1'].values).drop_duplicates()
    px = close_prices.reindex(px, method='bfill')
    
    out = pd.DataFrame(index=events_.index)
    out['ret'] = px.loc[events_['t1'].values].values / px.loc[events_.index] - 1
    
    if 'side' in events:
        out['ret'] *= events_['side']
    
    # Simple binary outcome: 1 for positive return, 0 for non-positive
    out['bin'] = np.sign(out['ret'])
    out.loc[out['bin'] <= 0, 'bin'] = 0
    
    return out


class MetaLabelingTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str):
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_params = settings.get_strategy_params('MLForecaster_Strategy')
        
        # Paths for saving models
        base_path = os.path.join(PROJECT_ROOT, "ml_models/trained_models")
        os.makedirs(base_path, exist_ok=True)
        self.primary_model_path = os.path.join(base_path, f"primary_{symbol.replace('/', '_')}.joblib")
        self.meta_model_path = os.path.join(base_path, f"meta_{symbol.replace('/', '_')}.joblib")

    async def fetch_data(self, days: int = 1000) -> pd.DataFrame:
        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        start_date = datetime.now(timezone.utc) - timedelta(days=days)
        bars = db_manager.retrieve_bar_data(self.symbol, self.timeframe, start_date=start_date)
        if not bars or len(bars) < 200:
            bars = await data_fetcher.fetch_historical_data_for_period(self.symbol, self.timeframe, start_date)
            if bars: db_manager.store_bar_data(bars)
        await data_fetcher.close()
        db_manager.close()
        
        if not bars: return pd.DataFrame()
        
        df = pd.DataFrame([b.model_dump() for b in bars])
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df.set_index('timestamp', inplace=True)
        return df

    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        df = data.copy()
        for lag in [1, 3, 5, 10]:
            df[f'log_return_lag_{lag}'] = np.log(df['close'] / df['close'].shift(lag))
        df['volatility_20'] = df['log_return_lag_1'].rolling(window=20).std()
        return df.dropna()

    async def run_training(self):
        logger.info(f"Starting meta-labeling training pipeline for {self.symbol}...")
        
        # 1. Fetch and prepare data
        data = await self.fetch_data()
        if data.empty:
            logger.error("No data available for training.")
            return
        
        features_df = self.create_features(data)
        X = features_df
        
        # 2. Define barriers and generate labels
        daily_vol = X['volatility_20'] # Simplified volatility measure
        pt_sl_multipliers = (1.0, 1.0) # Symmetric 1:1 risk-reward
        vertical_barrier_bars = 10 # 10 bars hold time
        
        vertical_barrier_times = pd.Series(X.index, index=X.index).shift(-vertical_barrier_bars)
        
        # Primary model will predict side, let's assume it predicts return > 0 or < 0
        # For simplicity, we'll generate side predictions based on a simple momentum indicator
        side = pd.Series(np.nan, index=X.index)
        side[X['log_return_lag_1'] > 0] = 1
        side[X['log_return_lag_1'] < 0] = -1
        side = side.ffill().dropna()
        
        # Align indexes
        aligned_idx = side.index.intersection(vertical_barrier_times.index).intersection(daily_vol.index)
        
        events = get_triple_barrier_events(
            close_prices=X['close'],
            t_events=aligned_idx,
            pt_sl=pt_sl_multipliers,
            target_vol=daily_vol.loc[aligned_idx],
            min_ret=0,
            num_threads=1,
            vertical_barrier_times=vertical_barrier_times.loc[aligned_idx],
            side=side.loc[aligned_idx]
        )
        
        labels = get_bins(events, X['close'])
        
        # Align features with labels
        X = X.loc[labels.index]
        
        # 3. Train Primary Model (predicts side)
        # Using a simple LightGBM model to predict the sign of the next return
        y_primary = np.sign(X['close'].pct_change(1).shift(-1).dropna())
        X_primary = X.loc[y_primary.index]
        
        primary_model = lgb.LGBMClassifier()
        primary_model.fit(X_primary, y_primary)
        joblib.dump(primary_model, self.primary_model_path)
        logger.info(f"Primary model trained and saved to {self.primary_model_path}")
        
        # 4. Get Primary Model Predictions (as feature for meta model)
        primary_predictions_proba = primary_model.predict_proba(X)[:, 1] # Probability of class 1 (up)
        
        # 5. Train Meta Model (predicts success based on primary model's confidence)
        X_meta = pd.DataFrame({'primary_pred_proba': primary_predictions_proba}, index=X.index)
        y_meta = labels['bin']
        
        # Using a calibrated classifier for the meta model
        meta_model_base = RandomForestClassifier(n_estimators=100, max_depth=4)
        meta_model = CalibratedClassifierCV(meta_model_base, method='isotonic', cv=5)
        meta_model.fit(X_meta, y_meta)
        joblib.dump(meta_model, self.meta_model_path)
        logger.info(f"Meta model trained and saved to {self.meta_model_path}")

        logger.info("Meta-labeling training pipeline finished.")
</code>

kamikaze_komodo/ml_models/price_forecasting/base_forecaster.py:
<code>
# FILE: kamikaze_komodo/ml_models/price_forecasting/base_forecaster.py
from abc import ABC, abstractmethod
import pandas as pd
from typing import Optional, Dict, Any, Union
from kamikaze_komodo.app_logger import get_logger
logger = get_logger(__name__)
class BasePriceForecaster(ABC):
    """
    Abstract base class for price forecasting models.
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        self.model_path = model_path
        self.params = params if params is not None else {}
        self.model: Any = None
        # FIX: The call to load_model is removed from the base class.
        # Subclasses are now responsible for calling it at the appropriate time
        # (i.e., after the model architecture has been defined).
        logger.info(f"{self.__class__.__name__} initialized with model_path: {model_path}, params: {self.params}")
    @abstractmethod
    def train(self, historical_data: pd.DataFrame, target_column: str = 'close', feature_columns: Optional[list] = None):
        """
        Trains the forecasting model.
        Args:
            historical_data (pd.DataFrame): DataFrame with historical OHLCV and feature data.
            target_column (str): The name of the column to predict.
            feature_columns (Optional[list]): List of column names to be used as features. If None, uses defaults.
        """
        pass
    @abstractmethod
    def predict(self, new_data: pd.DataFrame, feature_columns: Optional[list] = None) -> Union[pd.Series, float, None]:
        """
        Makes predictions on new data.
        Args:
            new_data (pd.DataFrame): DataFrame with the latest data for prediction.
                                     For bar-by-bar, this might be a single row or a lookback window.
            feature_columns (Optional[list]): List of column names to be used as features, must match training.
        Returns:
            Union[pd.Series, float, None]: Predicted value(s) or None if prediction fails.
                                           Could be a series for multi-step or single float for next step.
        """
        pass
    @abstractmethod
    def save_model(self, path: str):
        """
        Saves the trained model to the specified path.
        """
        pass
    @abstractmethod
    def load_model(self, path: str):
        """
        Loads a trained model from the specified path.
        """
        pass
    @abstractmethod
    def create_features(self, data: pd.DataFrame, feature_columns: Optional[list] = None) -> pd.DataFrame:
        """
        Creates features for the model from raw data.
        """
        pass
</code>

kamikaze_komodo/ml_models/price_forecasting/lightgbm_forecaster.py:
<code>
# kamikaze_komodo/ml_models/price_forecasting/lightgbm_forecaster.py
import lightgbm as lgb
import pandas as pd
import numpy as np
import joblib # For saving/loading model
from typing import Optional, Dict, Any, List, Union
from kamikaze_komodo.ml_models.price_forecasting.base_forecaster import BasePriceForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings as app_settings # Use app_settings to avoid conflict
logger = get_logger(__name__)
class LightGBMForecaster(BasePriceForecaster):
    """
    LightGBM-based price forecaster.
    Predicts price movement (e.g., next bar's close relative to current).
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        super().__init__(model_path, params) # This calls load_model if model_path is provided
        self.default_lgbm_params = {
            'objective': 'regression_l1', # Or 'regression_l2'
            'metric': 'rmse', # Root Mean Squared Error
            'n_estimators': 100,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'n_jobs': -1,
            'seed': 42,
            'boosting_type': 'gbdt',
        }
        # Update with any params passed from config for 'lgbm_params' specifically
        config_lgbm_params = {}
        if self.params: # self.params comes from config section like [LightGBM_Forecaster]
            for key, value in self.params.items():
                if key.startswith('lgbm_params_'): # e.g. lgbm_params_n_estimators
                    param_name = key.replace('lgbm_params_', '').lower()
                    config_lgbm_params[param_name] = value
        
        self.lgbm_params = {**self.default_lgbm_params, **config_lgbm_params}
        # If model_path was passed to super() and model loaded, self.model is set.
        # If model_path is in params (e.g. from config) but not passed directly to init, load it.
        if not self.model and self.model_path: # self.model_path is set by super if path given
             self.load_model(self.model_path)
        elif not self.model and self.params.get('modelfilename'): # Check if model path is in params from config
            # Construct full path if model_path is not set by direct argument to __init__
            # This logic is typically handled by the training/inference pipeline that instantiates this.
            # For direct use, ensure model_path is passed or params correctly configure it.
            pass
    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Creates a standard set of known features from the input data.
        This method generates a superset of features; selection for training/prediction happens elsewhere.
        """
        if data.empty:
            logger.warning("Data for feature creation is empty.")
            return pd.DataFrame()
        df = data.copy() 
        if 'close' not in df.columns:
            logger.error("'close' column not found in data for feature creation.")
            return df 
        for lag in [1, 3, 5, 10]:
            df[f'log_return_lag_{lag}'] = np.log(df['close'] / df['close'].shift(lag))
            df[f'close_change_lag_{lag}'] = df['close'].pct_change(lag)
        
        if 'log_return_lag_1' in df.columns: # Check if base lag feature was created
            df['volatility_5'] = df['log_return_lag_1'].rolling(window=5).std()
            df['volatility_10'] = df['log_return_lag_1'].rolling(window=10).std()
        else:
            df['volatility_5'] = np.nan
            df['volatility_10'] = np.nan
        if all(col in df.columns for col in ['high', 'low', 'close']):
            try:
                import pandas_ta as ta
                df.ta.rsi(close=df['close'], length=14, append=True, col_names=('RSI_14',))
                df.ta.macd(close=df['close'], append=True, col_names=('MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9'))
            except ImportError:
                logger.warning("pandas_ta not installed. Skipping TA features for LightGBM.")
            except Exception as e_ta: 
                logger.warning(f"Error during pandas_ta feature creation: {e_ta}. TA features might be missing or incomplete.")
        else:
            logger.warning("Missing 'high', 'low', or 'close' columns. Skipping TA features.")
        df = df.replace([np.inf, -np.inf], np.nan)
        return df # Return DataFrame with ALL generated features, NaNs from shifts/calcs are expected here
    def train(self, historical_data: pd.DataFrame, target_column: str = 'close_change_lag_1_future', feature_columns_to_use: Optional[List[str]] = None):
        logger.info(f"Starting LightGBM training for target '{target_column}'. Data shape: {historical_data.shape}")
        df = historical_data.copy()
        if target_column == 'close_change_lag_1_future':
            df['target'] = (df['close'].shift(-1) / df['close']) - 1
        elif target_column.startswith('log_return_lag_') and target_column.endswith('_future'):
            try:
                shift_val = int(target_column.split('_')[3])
                df['target'] = np.log(df['close'].shift(-shift_val) / df['close'])
            except Exception:
                logger.error(f"Could not parse shift value from target_column: {target_column}. Using default log return shift -1.")
                df['target'] = np.log(df['close'].shift(-1) / df['close']) # Default to next bar log return
        else:
            if target_column not in df.columns:
                logger.error(f"Target column '{target_column}' not in data or not a recognized dynamic target format.")
                return
            df['target'] = df[target_column]
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(subset=['target'], inplace=True) 
        df_with_all_features = self.create_features(df) 
        
        X_final_features = pd.DataFrame()
        if feature_columns_to_use:
            actual_features_present = [col for col in feature_columns_to_use if col in df_with_all_features.columns]
            missing = set(feature_columns_to_use) - set(actual_features_present)
            if missing:
                logger.warning(f"During training, specified feature_columns not all found/generated: {missing}. Using available: {actual_features_present}")
            if not actual_features_present:
                 logger.error("None of the specified_feature_columns_to_use are present after generation. Cannot train.")
                 return
            X_final_features = df_with_all_features[actual_features_present].copy()
        else:
            # Default: use a predefined list of potentially generated features
            default_feature_set = [
                col for col in df_with_all_features.columns if col.startswith('log_return_lag_') or \
                col.startswith('close_change_lag_') or \
                col.startswith('volatility_') or \
                col in ['RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9']
            ]
            # Filter this default set by what's actually in df_with_all_features
            default_features_present = [col for col in default_feature_set if col in df_with_all_features.columns]
            if not default_features_present:
                logger.error("No default features could be found/generated. Cannot train.")
                return
            X_final_features = df_with_all_features[default_features_present].copy()
        
        X_final_features.dropna(inplace=True) 
        y_train = df.loc[X_final_features.index, 'target'] 
        X_train = X_final_features
        if X_train.empty or y_train.empty:
            logger.error("Feature matrix X_train or target vector y_train is empty after processing. Training cannot proceed.")
            return
        self.model = lgb.LGBMRegressor(**self.lgbm_params)
        logger.info(f"Training LightGBM model with {len(X_train)} samples. Features: {list(X_train.columns)}")
        try:
            self.model.fit(X_train, y_train)
            logger.info("LightGBM model training completed.")
            self.trained_feature_columns_ = list(X_train.columns) # Store actual columns used
        except Exception as e:
            logger.error(f"Error during LightGBM model training: {e}", exc_info=True)
            self.model = None
    def predict(self, new_data: pd.DataFrame, feature_columns_to_use: Optional[list] = None) -> Union[pd.Series, float, None]:
        if self.model is None:
            logger.error("Model not loaded or trained. Cannot make predictions.")
            return None
        
        df_with_all_features = self.create_features(new_data)
        cols_for_prediction = None
        # Priority: 1. Explicitly passed `feature_columns_to_use`, 2. `self.trained_feature_columns_`
        if feature_columns_to_use:
            cols_for_prediction = [col for col in feature_columns_to_use if col in df_with_all_features.columns]
            missing_explicit = set(feature_columns_to_use) - set(cols_for_prediction)
            if missing_explicit: logger.warning(f"Explicitly requested prediction features not found: {missing_explicit}")
        elif hasattr(self, 'trained_feature_columns_') and self.trained_feature_columns_:
            cols_for_prediction = [col for col in self.trained_feature_columns_ if col in df_with_all_features.columns]
            missing_trained = set(self.trained_feature_columns_) - set(cols_for_prediction)
            if missing_trained: logger.warning(f"Features model was trained on are not all available for prediction: {missing_trained}")
        
        if not cols_for_prediction:
            logger.error("No feature columns determined for prediction. Make sure model is trained or features are specified and generatable.")
            return None
        
        X_new = df_with_all_features[cols_for_prediction].copy()
        if X_new.empty:
            logger.warning("Feature matrix X_new is empty after selection. Cannot predict.")
            return None
        
        # Handle potential NaNs in the very last row for prediction
        # LightGBM can handle NaNs internally if not too many, but for the last row it's critical.
        # If X_new is just one row (latest data point), ensure it's complete or handle.
        if len(X_new) == 1 and X_new.isnull().values.any():
            logger.warning(f"Latest data row for prediction contains NaNs in selected features: {X_new[X_new.isnull().any(axis=1)].columns[X_new.isnull().any(axis=1)[0]]}. Prediction may fail or be zero.")
            # Depending on LightGBM's setup and data, it might predict 0 or error.
            # Option: return None or fill specific ways if this is an issue.
            # For now, let LightGBM try.
        try:
            predictions = self.model.predict(X_new)
            logger.debug(f"Made {len(predictions)} predictions. Latest prediction input shape: {X_new.shape}. Prediction output: {predictions[-1] if len(predictions)>0 else 'N/A'}")
            if len(predictions) == 0: return None
            # We usually want the prediction for the last row of input `new_data`
            return predictions[-1] if isinstance(predictions, np.ndarray) else predictions # if it was a single value already
        except Exception as e:
            logger.error(f"Error during LightGBM prediction: {e}", exc_info=True)
            return None
    def save_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if self.model is None:
            logger.error("No model to save.")
            return
        if not _path:
            logger.error("No path specified for saving the model.")
            return
        try:
            model_and_features = {
                'model': self.model,
                'feature_columns': getattr(self, 'trained_feature_columns_', None)
            }
            joblib.dump(model_and_features, _path)
            logger.info(f"LightGBM model and feature columns saved to {_path}")
        except Exception as e:
            logger.error(f"Error saving LightGBM model to {_path}: {e}", exc_info=True)
    def load_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if not _path:
            # This case might happen if model_path is None and params don't specify it.
            # Initialization of LightGBMForecaster should handle this (e.g. by not setting self.model)
            logger.debug("No path specified for loading the model during load_model call.")
            return
        try:
            model_and_features = joblib.load(_path)
            self.model = model_and_features['model']
            self.trained_feature_columns_ = model_and_features.get('feature_columns') 
            self.model_path = _path 
            logger.info(f"LightGBM model and feature columns loaded from {_path}. Trained features: {self.trained_feature_columns_}")
        except FileNotFoundError:
            logger.error(f"LightGBM model file not found at {_path}.")
            self.model = None
            self.trained_feature_columns_ = None
        except Exception as e:
            logger.error(f"Error loading LightGBM model from {_path}: {e}", exc_info=True)
            self.model = None
            self.trained_feature_columns_ = None
</code>

kamikaze_komodo/ml_models/price_forecasting/__init__.py:
<code>
# kamikaze_komodo/ml_models/price_forecasting/__init__.py
# This file makes the 'price_forecasting' directory a Python package.
</code>

kamikaze_komodo/ml_models/price_forecasting/lstm_model.py:
<code>
# FILE: kamikaze_komodo/ml_models/price_forecasting/lstm_model.py
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import joblib
from typing import Optional, Dict, Any, List, Union, Tuple
from sklearn.preprocessing import MinMaxScaler

from kamikaze_komodo.ml_models.price_forecasting.base_forecaster import BasePriceForecaster
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

# Define the PyTorch LSTM Model
class LSTMNetwork(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int, dropout: float):
        super(LSTMNetwork, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)
        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :]) # Get the output from the last time step
        return out

class LSTMForecaster(BasePriceForecaster):
    """
    LSTM-based price forecaster using PyTorch.
    Predicts future price movement based on a sequence of historical data.
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        super().__init__(model_path, params)
        self.scaler = MinMaxScaler(feature_range=(-1, 1))
        self.sequence_length = int(self.params.get('sequencelength', 60))
        self.feature_columns_str = self.params.get('featurecolumns', 'close,log_return_lag_1,close_change_lag_1,volatility_5,RSI_14')
        self.feature_columns = [col.strip() for col in self.feature_columns_str.split(',')]
        self.num_features = len(self.feature_columns)


        # Model hyperparameters
        self.hidden_size = int(self.params.get('hiddensize', 50))
        self.num_layers = int(self.params.get('numlayers', 2))
        self.dropout = float(self.params.get('dropout', 0.2))
        self.num_epochs = int(self.params.get('numepochs', 20))
        self.batch_size = int(self.params.get('batchsize', 32))
        self.learning_rate = float(self.params.get('learningrate', 0.001))

        # Initialize model architecture but don't load weights from super() as it's not implemented there for torch
        self.model = LSTMNetwork(
            input_size=self.num_features,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            output_size=1,
            dropout=self.dropout
        )
        if self.model_path:
            self.load_model(self.model_path)

    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        if data.empty:
            return pd.DataFrame()
        df = data.copy()

        import pandas_ta as ta
        for lag in [1, 3, 5]:
            df[f'log_return_lag_{lag}'] = np.log(df['close'] / df['close'].shift(lag))
            df[f'close_change_lag_{lag}'] = df['close'].pct_change(lag)
        df['volatility_5'] = df['log_return_lag_1'].rolling(window=5).std()
        df.ta.rsi(length=14, append=True, col_names=('RSI_14',))

        df = df.replace([np.inf, -np.inf], np.nan)
        return df

    def _create_sequences(self, data: np.ndarray, target_idx: int) -> Tuple[np.ndarray, np.ndarray]:
        X, y = [], []
        # Exclude target from feature set
        feature_indices = [i for i in range(data.shape[1]) if i != target_idx]
        for i in range(len(data) - self.sequence_length):
            X.append(data[i:(i + self.sequence_length), feature_indices])
            y.append(data[i + self.sequence_length, target_idx])
        return np.array(X), np.array(y)


    def train(self, historical_data: pd.DataFrame, target_column: str = 'close_change_lag_1_future', feature_columns: Optional[list] = None):
        logger.info("Starting LSTM training...")
        df_features = self.create_features(historical_data)

        if feature_columns is None:
            feature_columns = self.feature_columns

        df_features['target'] = df_features['close'].pct_change(1).shift(-1)

        # --- FIX: Select final columns BEFORE dropping NaN values ---
        final_columns_to_use = feature_columns + ['target']
        
        # Check if all columns are present
        missing_cols = set(final_columns_to_use) - set(df_features.columns)
        if missing_cols:
            logger.error(f"Missing required columns for LSTM training: {missing_cols}. Cannot proceed.")
            return

        features_with_target = df_features[final_columns_to_use]
        features_with_target = features_with_target.dropna()

        if features_with_target.empty:
            logger.error("DataFrame is empty after selecting features and dropping NaNs. Not enough data to create complete feature/target rows. Cannot train LSTM model.")
            return
        # --- END FIX ---

        # Scale data
        scaled_data = self.scaler.fit_transform(features_with_target)
        
        # Find index of target column for sequence creation
        target_idx = features_with_target.columns.get_loc('target')
        
        X, y = self._create_sequences(scaled_data, target_idx)
        X_train = torch.from_numpy(X).float()
        y_train = torch.from_numpy(y).float().view(-1, 1)

        # Training Loop
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)

        for epoch in range(self.num_epochs):
            self.model.train()
            outputs = self.model(X_train)
            optimizer.zero_grad()
            loss = criterion(outputs, y_train)
            loss.backward()
            optimizer.step()
            if (epoch + 1) % 5 == 0:
                logger.info(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {loss.item():.6f}')

        self.trained_feature_columns_ = feature_columns
        logger.info("LSTM model training completed.")

    def predict(self, new_data: pd.DataFrame, feature_columns: Optional[list] = None) -> Union[pd.Series, float, None]:
        if self.model is None or self.scaler is None:
            logger.error("Model or scaler not available.")
            return None

        self.model.eval()
        df_features = self.create_features(new_data)

        if feature_columns is None:
            feature_columns = self.trained_feature_columns_ or self.feature_columns

        if len(df_features) < self.sequence_length:
            return None

        last_sequence_unscaled = df_features[feature_columns].iloc[-self.sequence_length:]
        if last_sequence_unscaled.isnull().values.any():
            return None # Cannot predict with NaNs

        # Use a temporary scaler fitted on the feature columns only to avoid data leakage
        temp_scaler = MinMaxScaler(feature_range=(-1,1))
        # This scaler should ideally be fitted on the same training distribution
        # For simplicity here, we transform based on the scaler fitted during training, which included the target
        # A more robust solution involves saving separate scalers for features and target.
        
        # We need to create a dummy dataframe with all original columns to use the fitted scaler
        all_cols = self.trained_feature_columns_ + ['target']
        dummy_df_for_scaling = pd.DataFrame(columns=all_cols)
        for col in feature_columns:
            dummy_df_for_scaling[col] = last_sequence_unscaled[col]
        dummy_df_for_scaling = dummy_df_for_scaling.fillna(0) # fill target with dummy values
        
        scaled_data = self.scaler.transform(dummy_df_for_scaling)
        
        # Extract only the feature columns from the scaled data
        target_idx = all_cols.index('target')
        feature_indices = [i for i, col in enumerate(all_cols) if i != target_idx]
        scaled_sequence = scaled_data[:, feature_indices]


        with torch.no_grad():
            input_tensor = torch.from_numpy(scaled_sequence).float().unsqueeze(0) # Add batch dimension
            prediction_scaled = self.model(input_tensor)

        # We need to inverse transform the prediction. This requires a dummy array.
        dummy_array = np.zeros((1, len(all_cols)))
        dummy_array[0, target_idx] = prediction_scaled.item()
        prediction_unscaled = self.scaler.inverse_transform(dummy_array)[0, target_idx]

        return float(prediction_unscaled)

    def save_model(self, path: str):
        try:
            state = {
                'model_state_dict': self.model.state_dict(),
                'scaler': self.scaler,
                'params': self.params,
                'trained_feature_columns': getattr(self, 'trained_feature_columns_', None)
            }
            torch.save(state, path)
            logger.info(f"LSTM model saved to {path}")
        except Exception as e:
            logger.error(f"Error saving LSTM model: {e}", exc_info=True)

    def load_model(self, path: str):
        try:
            # FIX: Set weights_only=False to allow loading sklearn scaler object
            # This is safe as we are loading a file we just saved in a trusted environment.
            state = torch.load(path, weights_only=False)
            self.model.load_state_dict(state['model_state_dict'])
            self.scaler = state['scaler']
            self.params = state['params']
            self.trained_feature_columns_ = state.get('trained_feature_columns')
            self.model.eval()
            self.model_path = path
            logger.info(f"LSTM model loaded from {path}")
        except FileNotFoundError:
            logger.error(f"LSTM model file not found at {path}.")
            self.model = None
        except Exception as e:
            logger.error(f"Error loading LSTM model: {e}", exc_info=True)
            self.model = None
</code>

kamikaze_komodo/ml_models/price_forecasting/xgboost_classifier_forecaster.py:
<code>
# FILE: kamikaze_komodo/ml_models/price_forecasting/xgboost_classifier_forecaster.py
import xgboost as xgb
import pandas as pd
import numpy as np
import joblib
from typing import Optional, Dict, Any, List, Tuple
from sklearn.preprocessing import LabelEncoder

from kamikaze_komodo.ml_models.price_forecasting.base_forecaster import BasePriceForecaster
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class XGBoostClassifierForecaster(BasePriceForecaster):
    """
    XGBoost-based classifier for price movement prediction (UP, DOWN, SIDEWAYS).
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        # FIX: Initialize attributes BEFORE calling super().__init__
        # This prevents the super constructor's call to load_model from being overwritten.
        self.label_encoder = LabelEncoder()
        self.trained_feature_columns_: Optional[List[str]] = None

        super().__init__(model_path, params) # Handles loading model if path provided

        self.default_xgb_params = {
            'objective': 'multi:softprob', # For multiclass classification, outputs probabilities
            'eval_metric': 'mlogloss', # Multiclass logloss
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 3,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'use_label_encoder': False, # Deprecated, handle encoding manually if needed
            'seed': 42,
        }
        config_xgb_params = {k.replace('xgb_params_', ''): v for k, v in self.params.items() if k.startswith('xgb_params_')}
        self.xgb_params = {**self.default_xgb_params, **config_xgb_params}

        self.num_class = int(self.params.get('num_classes', 3)) # UP, DOWN, SIDEWAYS
        self.xgb_params['num_class'] = self.num_class

        if not self.model and self.model_path:
            self.load_model(self.model_path)

    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        if data.empty:
            return pd.DataFrame()
        df = data.copy()
        if 'close' not in df.columns:
            logger.error("'close' column missing for feature creation.")
            return df

        for lag in [1, 2, 3, 5, 10, 20]:
            df[f'log_return_lag_{lag}'] = np.log(df['close'] / df['close'].shift(lag))
            df[f'close_change_lag_{lag}'] = df['close'].pct_change(lag)

        if 'log_return_lag_1' in df.columns:
            df['volatility_5'] = df['log_return_lag_1'].rolling(window=5).std()
            df['volatility_10'] = df['log_return_lag_1'].rolling(window=10).std()
            df['volatility_20'] = df['log_return_lag_1'].rolling(window=20).std()

        if all(col in df.columns for col in ['high', 'low', 'close']):
            try:
                import pandas_ta as ta
                df.ta.rsi(length=14, append=True, col_names=('RSI_14',))
                df.ta.macd(append=True, col_names=('MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9'))
                df.ta.atr(length=14, append=True, col_names=('ATR_14',))
            except ImportError:
                logger.warning("pandas_ta not installed. Skipping TA features for XGBoost.")
            except Exception as e:
                logger.warning(f"Error creating TA features: {e}")

        df = df.replace([np.inf, -np.inf], np.nan)
        return df

    def _define_target(self, data: pd.DataFrame, thresholds: Optional[Tuple[float, float]] = (-0.001, 0.001)) -> pd.Series:
        """Defines target classes: 0 (UP), 1 (DOWN), 2 (SIDEWAYS)."""
        future_returns = data['close'].pct_change(1).shift(-1) # Next bar's return
        if thresholds is None:
            thresholds = (-0.001, 0.001) # Default if not provided
        lower_thresh, upper_thresh = thresholds

        target = pd.Series(2, index=data.index) # Default to SIDEWAYS
        target[future_returns > upper_thresh] = 0 # UP
        target[future_returns < lower_thresh] = 1 # DOWN
        return target.astype(int)

    def train(self, historical_data: pd.DataFrame, target_definition: str = 'next_bar_direction', feature_columns: Optional[list] = None):
        logger.info(f"Starting XGBoost Classifier training. Data shape: {historical_data.shape}")
        df = historical_data.copy()

        return_thresholds_str = self.params.get('returnthresholds_percent', "-0.001,0.001")
        try:
            thresholds_list = [float(x.strip()) for x in return_thresholds_str.split(',')]
            if len(thresholds_list) != 2:
                raise ValueError("ReturnThresholds_Percent must be two comma-separated floats.")
            return_thresholds = tuple(thresholds_list)
        except Exception as e:
            logger.warning(f"Invalid ReturnThresholds_Percent '{return_thresholds_str}', using defaults (-0.001, 0.001). Error: {e}")
            return_thresholds = (-0.001, 0.001)

        if target_definition == 'next_bar_direction':
            df['target'] = self._define_target(df, return_thresholds)
        else:
            logger.error(f"Unsupported target_definition: {target_definition}")
            return

        df.dropna(subset=['target'], inplace=True)
        df_with_features = self.create_features(df)

        if feature_columns:
            actual_features = [col for col in feature_columns if col in df_with_features.columns]
        else:
            default_feature_set = [
                col for col in df_with_features.columns if
                col.startswith('log_return_lag_') or
                col.startswith('close_change_lag_') or
                col.startswith('volatility_') or
                col in ['RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'ATR_14']
            ]
            actual_features = [col for col in default_feature_set if col in df_with_features.columns]


        if not actual_features:
            logger.error("No features selected for training XGBoost.")
            return

        X = df_with_features[actual_features].copy()
        y = df.loc[X.index, 'target'].astype(int)

        logger.debug(f"XGBoost - Shape of df_with_features: {df_with_features.shape}")
        logger.debug(f"XGBoost - Features selected for X: {actual_features}")
        logger.debug(f"XGBoost - Sample of X before dropna (head):\n{X.head()}")
        logger.debug(f"XGBoost - NaN counts in X before dropna:\n{X.isnull().sum().sort_values(ascending=False)}")
        logger.debug(f"XGBoost - Shape of X before dropna: {X.shape}")

        X.dropna(inplace=True)

        logger.debug(f"XGBoost - Shape of X after dropna: {X.shape}")
        logger.debug(f"XGBoost - Shape of y before aligning with X: {y.shape}")

        y = y.loc[X.index]

        logger.debug(f"XGBoost - Shape of y after aligning with X: {y.shape}")
        if not X.empty:
            logger.debug(f"XGBoost - Sample of X after dropna & alignment (head):\n{X.head()}")
        if not y.empty:
            logger.debug(f"XGBoost - Sample of y after dropna & alignment (head):\n{y.head()}")
            logger.debug(f"XGBoost - Value counts of y: \n{y.value_counts(dropna=False)}")


        if X.empty or y.empty:
            logger.error("Feature matrix X or target vector y is empty after processing. Training cannot proceed.")
            if X.empty:
                logger.error(f"XGBoost - X is empty. Columns previously in X (before dropna): {str(actual_features)}")
                logger.error(f"XGBoost - df_with_features had columns: {list(df_with_features.columns)}")
            if y.empty:
                logger.error("XGBoost - y is empty.")
            return

        self.label_encoder.fit(y) # Fit encoder on the integer labels (0, 1, 2)
        y_encoded = self.label_encoder.transform(y)

        self.model = xgb.XGBClassifier(**self.xgb_params)
        logger.info(f"Training XGBoostClassifier with {len(X)} samples. Features: {actual_features}")
        try:
            self.model.fit(X, y_encoded)
            self.trained_feature_columns_ = list(X.columns)
            logger.info("XGBoostClassifier training completed.")
        except Exception as e:
            logger.error(f"Error during XGBoostClassifier training: {e}", exc_info=True)
            self.model = None

    def predict(self, new_data: pd.DataFrame, feature_columns: Optional[list] = None) -> Optional[Dict[str, Any]]:
        if self.model is None:
            logger.error("XGBoost model not loaded/trained. Cannot predict.")
            return None

        df_with_features = self.create_features(new_data)
        cols_for_pred = feature_columns if feature_columns else self.trained_feature_columns_
        if not cols_for_pred:
            logger.error("No feature columns determined for XGBoost prediction.")
            return None

        # Ensure all required columns are present, even if with NaNs, before selection
        missing_cols = set(cols_for_pred) - set(df_with_features.columns)
        if missing_cols:
            logger.warning(f"Columns required for prediction are missing from generated features: {missing_cols}")
            return None

        X_new = df_with_features[cols_for_pred].copy()
        if X_new.empty:
            logger.warning("Feature matrix for prediction is empty.")
            return None

        if X_new.iloc[-1].isnull().any():
            logger.warning(f"Last row for XGBoost prediction contains NaNs in features: {X_new.columns[X_new.iloc[-1].isnull()].tolist()}. Prediction might be unreliable.")

        try:
            # Predict probabilities for the last row
            last_row_features = X_new.iloc[[-1]] # Keep as DataFrame
            probabilities = self.model.predict_proba(last_row_features)[0]
            predicted_class_encoded = np.argmax(probabilities)
            predicted_class_label = self.label_encoder.inverse_transform([predicted_class_encoded])[0] # Original label (0, 1, 2)
            confidence = probabilities[predicted_class_encoded]

            return {
                "predicted_class": int(predicted_class_label), # 0:UP, 1:DOWN, 2:SIDEWAYS
                "confidence": float(confidence),
                "probabilities": [float(p) for p in probabilities] # Prob for each class
            }
        except Exception as e:
            logger.error(f"Error during XGBoostClassifier prediction: {e}", exc_info=True)
            return None

    def save_model(self, path: str):
        if self.model is None:
            logger.error("No XGBoost model to save.")
            return
        try:
            model_data = {
                'model': self.model,
                'label_encoder': self.label_encoder,
                'feature_columns': self.trained_feature_columns_
            }
            joblib.dump(model_data, path)
            logger.info(f"XGBoostClassifier model saved to {path}")
        except Exception as e:
            logger.error(f"Error saving XGBoostClassifier model to {path}: {e}", exc_info=True)

    def load_model(self, path: str):
        try:
            model_data = joblib.load(path)
            self.model = model_data['model']
            self.label_encoder = model_data['label_encoder']
            self.trained_feature_columns_ = model_data.get('feature_columns')
            self.model_path = path
            logger.info(f"XGBoostClassifier model loaded from {path}. Trained features: {self.trained_feature_columns_}")
        except FileNotFoundError:
            logger.error(f"XGBoostClassifier model file not found at {path}.")
            self.model = None
        except Exception as e:
            logger.error(f"Error loading XGBoostClassifier model from {path}: {e}", exc_info=True)
            self.model = None
</code>

kamikaze_komodo/ml_models/regime_detection/kmeans_regime_model.py:
<code>
# kamikaze_komodo/ml_models/regime_detection/kmeans_regime_model.py
import pandas as pd
import numpy as np
import joblib
import os
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from typing import Optional, Dict, Any, List

from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from datetime import datetime, timedelta, timezone

logger = get_logger(__name__)

class KMeansRegimeModel:
    """
    Identifies market regimes using K-Means clustering on specified features.
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        self.model_path = model_path
        
        self.n_clusters = int(self.params.get('num_clusters', 3))
        # Features string from config, e.g., "volatility_20d,atr_14d_percentage"
        features_str = self.params.get('featuresforclustering', 'volatility_20d,atr_14d_percentage')
        self.features_for_clustering = [f.strip() for f in features_str.split(',')]

        self.model: Optional[KMeans] = None
        self.scaler: Optional[StandardScaler] = None
        self.cluster_centers_: Optional[np.ndarray] = None # To store cluster centers post-training for interpretation

        if model_path:
            self.load_model(model_path)
        logger.info(f"KMeansRegimeModel initialized. Clusters: {self.n_clusters}, Features: {self.features_for_clustering}, Model Path: {model_path}")

    def _calculate_feature_volatility_X_day(self, data: pd.DataFrame, window: int = 20) -> pd.Series:
        """Calculates X-day rolling volatility of log returns."""
        if 'close' not in data.columns or len(data) < window:
            return pd.Series(np.nan, index=data.index)
        log_returns = np.log(data['close'] / data['close'].shift(1))
        return log_returns.rolling(window=window).std() * np.sqrt(window) # Annualize for context if daily, or use raw

    def _calculate_feature_atr_X_day_percentage(self, data: pd.DataFrame, window: int = 14) -> pd.Series:
        """Calculates X-day ATR as a percentage of closing price."""
        if not all(col in data.columns for col in ['high', 'low', 'close']) or len(data) < window:
            return pd.Series(np.nan, index=data.index)
        try:
            import pandas_ta as ta
            atr = ta.atr(high=data['high'], low=data['low'], close=data['close'], length=window)
            if atr is None or data['close'].rolling(window=window).min().eq(0).any(): # Avoid division by zero
                 return pd.Series(np.nan, index=data.index)
            atr_percentage = (atr / data['close']) * 100
            return atr_percentage
        except ImportError:
            logger.warning("pandas_ta not found for ATR calculation in KMeansRegimeModel.")
            return pd.Series(np.nan, index=data.index)
        except Exception as e:
            logger.error(f"Error calculating ATR%: {e}")
            return pd.Series(np.nan, index=data.index)


    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        if data.empty:
            return pd.DataFrame()
        df = data.copy()
        
        generated_features = pd.DataFrame(index=df.index)

        for feature_name in self.features_for_clustering:
            if feature_name.startswith('volatility_') and feature_name.endswith('d'):
                try:
                    window = int(feature_name.split('_')[1][:-1])
                    generated_features[feature_name] = self._calculate_feature_volatility_X_day(df, window)
                except ValueError:
                    logger.warning(f"Could not parse window for volatility feature: {feature_name}")
            elif feature_name.startswith('atr_') and feature_name.endswith('d_percentage'):
                try:
                    window = int(feature_name.split('_')[1][:-1]) # atr_14d -> 14
                    generated_features[feature_name] = self._calculate_feature_atr_X_day_percentage(df, window)
                except ValueError:
                     logger.warning(f"Could not parse window for ATR feature: {feature_name}")
            else:
                logger.warning(f"Unsupported feature definition for Kmeans clustering: {feature_name}")
        
        generated_features.dropna(inplace=True)
        return generated_features

    def train(self, historical_data: pd.DataFrame):
        logger.info(f"Starting KMeans Regime Model training. Data shape: {historical_data.shape}")
        feature_df = self.create_features(historical_data)

        if feature_df.empty or len(feature_df) < self.n_clusters:
            logger.error("Not enough data points after feature creation to train KMeans model.")
            return

        self.scaler = StandardScaler()
        scaled_features = self.scaler.fit_transform(feature_df)

        self.model = KMeans(n_clusters=self.n_clusters, random_state=42, n_init='auto')
        try:
            self.model.fit(scaled_features)
            self.cluster_centers_ = self.scaler.inverse_transform(self.model.cluster_centers_) # Store unscaled centers
            logger.info(f"KMeans Regime Model training completed. Inertia: {self.model.inertia_:.2f}")
            logger.info(f"Unscaled Cluster Centers:\n{self.cluster_centers_}")
            # Interpret clusters (e.g., by examining center values for volatility, atr_percentage)
            # For example, cluster with highest volatility could be "high volatility regime"
        except Exception as e:
            logger.error(f"Error during KMeans model training: {e}", exc_info=True)
            self.model = None
            self.scaler = None

    def predict(self, new_data: pd.DataFrame) -> Optional[int]:
        if self.model is None or self.scaler is None:
            logger.error("KMeans model or scaler not trained/loaded. Cannot predict regime.")
            return None
        
        feature_df = self.create_features(new_data)
        if feature_df.empty:
            logger.warning("No features could be created from new_data for KMeans prediction.")
            return None

        # We need to predict for the last row of feature_df
        last_features = feature_df.iloc[[-1]]
        if last_features.isnull().values.any():
            logger.warning(f"Latest features for KMeans prediction contain NaNs: {last_features}. Cannot predict.")
            return None

        scaled_features = self.scaler.transform(last_features)
        try:
            regime = self.model.predict(scaled_features)[0]
            logger.debug(f"Predicted regime for latest data: {regime}")
            return int(regime)
        except Exception as e:
            logger.error(f"Error during KMeans regime prediction: {e}", exc_info=True)
            return None

    def predict_series(self, data: pd.DataFrame) -> Optional[pd.Series]:
        """
        Predicts the regime for an entire DataFrame of historical data.
        Returns a Series of regimes aligned with the input DataFrame's index.
        """
        if self.model is None or self.scaler is None:
            logger.error("KMeans model or scaler not trained/loaded. Cannot predict regime series.")
            return None
        
        feature_df = self.create_features(data)
        if feature_df.empty:
            logger.warning("No features could be created from data for KMeans series prediction.")
            return pd.Series(np.nan, index=data.index)

        # Align feature_df with original data index to handle NaNs from feature creation
        aligned_feature_df = feature_df.reindex(data.index)
        
        # We can only predict where we have features. Get the valid indices.
        valid_indices = aligned_feature_df.dropna().index
        if valid_indices.empty:
            logger.warning("No valid feature rows to predict on.")
            return pd.Series(np.nan, index=data.index)

        scaled_features = self.scaler.transform(aligned_feature_df.loc[valid_indices])
        
        try:
            regimes = self.model.predict(scaled_features)
            # Create a series with the predictions, indexed correctly
            regime_series = pd.Series(regimes, index=valid_indices)
            # Reindex to match the original data's index, and forward-fill missing values
            return regime_series.reindex(data.index).ffill()
        except Exception as e:
            logger.error(f"Error during KMeans regime series prediction: {e}", exc_info=True)
            return None

    def save_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if self.model is None or self.scaler is None:
            logger.error("No KMeans model or scaler to save.")
            return
        if not _path:
            logger.error("No path specified for saving KMeans model.")
            return
        try:
            model_data = {
                'model': self.model,
                'scaler': self.scaler,
                'features_for_clustering': self.features_for_clustering,
                'n_clusters': self.n_clusters,
                'cluster_centers_': self.cluster_centers_
            }
            joblib.dump(model_data, _path)
            logger.info(f"KMeans Regime model saved to {_path}")
        except Exception as e:
            logger.error(f"Error saving KMeans model to {_path}: {e}", exc_info=True)

    def load_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if not _path:
            logger.debug("No path specified for loading KMeans model.")
            return
        try:
            model_data = joblib.load(_path)
            self.model = model_data['model']
            self.scaler = model_data['scaler']
            self.features_for_clustering = model_data.get('features_for_clustering', self.features_for_clustering)
            self.n_clusters = model_data.get('n_clusters', self.n_clusters)
            self.cluster_centers_ = model_data.get('cluster_centers_')
            self.model_path = _path
            logger.info(f"KMeans Regime model loaded from {_path}. Clusters: {self.n_clusters}, Features: {self.features_for_clustering}")
            if self.cluster_centers_ is not None:
                 logger.info(f"Loaded Unscaled Cluster Centers:\n{self.cluster_centers_}")
        except FileNotFoundError:
            logger.error(f"KMeans model file not found at {_path}.")
            self.model = None
            self.scaler = None
        except Exception as e:
            logger.error(f"Error loading KMeans model from {_path}: {e}", exc_info=True)
            self.model = None
            self.scaler = None

async def main_kmeans_regime_example():
    if not settings:
        print("Settings not loaded, cannot run KMeans Regime model example.")
        return

    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    config_section = "KMeans_Regime_Model" # Must match config.ini section name

    if not settings.config.has_section(config_section):
        logger.error(f"Config section [{config_section}] not found. Cannot run KMeans Regime example.")
        return

    model_params = settings.get_strategy_params(config_section)
    _model_base_path = model_params.get('modelsavepath', 'kamkaze_komodo/ml_models/trained_models/regime')
    _model_filename = model_params.get('modelfilename', f"kmeans_regime_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
    
    script_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) 
    if not os.path.isabs(_model_base_path):
        model_save_path_dir = os.path.join(script_dir, _model_base_path)
    else:
        model_save_path_dir = _model_base_path
    if not os.path.exists(model_save_path_dir):
        os.makedirs(model_save_path_dir, exist_ok=True)
    model_full_path = os.path.join(model_save_path_dir, _model_filename)

    # --- Training ---
    logger.info("--- KMeans Regime Model Training Example ---")
    regime_model_trainer = KMeansRegimeModel(params=model_params)
    
    db_manager = DatabaseManager()
    data_fetcher = DataFetcher()
    training_days = int(model_params.get('trainingdayshistory', 1095))
    start_dt = datetime.now(timezone.utc) - timedelta(days=training_days)
    end_dt = datetime.now(timezone.utc)
    
    bars = db_manager.retrieve_bar_data(symbol, timeframe, start_dt, end_dt)
    if not bars or len(bars) < 100: # Need substantial data for regime features
        logger.info(f"Fetching fresh data for KMeans training for {symbol}...")
        bars = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_dt, end_dt)
        if bars: db_manager.store_bar_data(bars)
    await data_fetcher.close()
    db_manager.close()

    if not bars or len(bars) < 100:
        logger.error(f"Not enough data ({len(bars)} bars) for KMeans training.")
        return

    training_df = pd.DataFrame([b.model_dump() for b in bars])
    training_df['timestamp'] = pd.to_datetime(training_df['timestamp'])
    training_df.set_index('timestamp', inplace=True)
    training_df.sort_index(inplace=True)

    regime_model_trainer.train(training_df)
    if regime_model_trainer.model:
        regime_model_trainer.save_model(model_full_path)

    # --- Prediction ---
    logger.info("--- KMeans Regime Model Prediction Example ---")
    if not os.path.exists(model_full_path):
        logger.error("Trained KMeans model not found. Skipping prediction example.")
        return

    regime_model_predictor = KMeansRegimeModel(model_path=model_full_path, params=model_params)
    if regime_model_predictor.model is None:
        logger.error("Could not load KMeans model for prediction.")
        return

    # Use the last N bars of training_df for prediction example (or fetch fresh small segment)
    if len(training_df) > 50:
        prediction_data_segment = training_df.tail(50) # Use recent history to get features for the last point
        predicted_regime = regime_model_predictor.predict(prediction_data_segment)
        if predicted_regime is not None:
            logger.info(f"Predicted regime for {symbol} ({timeframe}) on latest data: {predicted_regime}")
        else:
            logger.warning(f"Could not get KMeans regime prediction for {symbol} ({timeframe}).")
    else:
        logger.warning("Not enough data in training_df to run prediction example.")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main_kmeans_regime_example())
</code>

kamikaze_komodo/ml_models/regime_detection/__init__.py:
<code>
# kamikaze_komodo/ml_models/regime_detection/__init__.py
# This file makes the 'regime_detection' directory a Python package.
</code>

kamikaze_komodo/ml_models/inference_pipelines/lstm_inference.py:
<code>
# FILE: kamikaze_komodo/ml_models/inference_pipelines/lstm_inference.py
import pandas as pd
from typing import Optional
import os
from kamikaze_komodo.ml_models.price_forecasting.lstm_model import LSTMForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class LSTMInference:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LSTM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        
        model_params_config = settings.get_strategy_params(model_config_section)
        
        _model_base_path = model_params_config.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = model_params_config.get('modelfilename', f"lstm_{symbol.replace('/', '_').lower()}_{timeframe}.pth")
        
        if not os.path.isabs(_model_base_path):
            self.model_load_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_load_path_dir = _model_base_path
            
        self.model_full_load_path = os.path.join(self.model_load_path_dir, _model_filename)
        self.forecaster = LSTMForecaster(model_path=self.model_full_load_path, params=model_params_config)
        
        if self.forecaster.model is None:
            logger.warning(f"LSTMInference: Model could not be loaded from {self.model_full_load_path}. Predictions will not be available.")

    def get_prediction(self, current_data_history: pd.DataFrame) -> Optional[float]:
        """
        Gets a single prediction based on the current data history.
        """
        if self.forecaster.model is None:
            logger.warning("No model loaded, cannot get prediction.")
            return None
        if current_data_history.empty:
            logger.warning("Data history is empty, cannot get prediction.")
            return None
            
        prediction_output = self.forecaster.predict(current_data_history)
        
        if prediction_output is None:
            return None
            
        return float(prediction_output)
</code>

kamikaze_komodo/ml_models/inference_pipelines/__init__.py:
<code>
# kamikaze_komodo/ml_models/inference_pipelines/__init__.py
# This file makes the 'inference_pipelines' directory a Python package.
</code>

kamikaze_komodo/ml_models/inference_pipelines/xgboost_classifier_inference.py:
<code>
# FILE: kamikaze_komodo/ml_models/inference_pipelines/xgboost_classifier_inference.py
import pandas as pd
from typing import Optional, Dict, Any
import os
import numpy as np
from kamikaze_komodo.ml_models.price_forecasting.xgboost_classifier_forecaster import XGBoostClassifierForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings

logger = get_logger(__name__)

class XGBoostClassifierInference:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "XGBoost_Classifier_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        
        model_params_config = settings.get_strategy_params(model_config_section)
        
        _model_base_path = model_params_config.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = model_params_config.get('modelfilename', f"xgb_classifier_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        script_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) 
        
        if not os.path.isabs(_model_base_path):
            self.model_load_path_dir = os.path.join(script_dir, _model_base_path)
        else:
            self.model_load_path_dir = _model_base_path
            
        self.model_full_load_path = os.path.join(self.model_load_path_dir, _model_filename)
        self.forecaster = XGBoostClassifierForecaster(model_path=self.model_full_load_path, params=model_params_config)
        
        if self.forecaster.model is None:
            logger.warning(f"XGBoostClassifierInference: Model could not be loaded from {self.model_full_load_path}. Predictions will not be available.")

    def get_prediction(self, current_data_history: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        Gets a single classification prediction based on the current data history.
        Returns a dictionary with 'predicted_class', 'confidence', and 'probabilities'.
        """
        if self.forecaster.model is None:
            logger.warning("XGBoost model not loaded, cannot get prediction.")
            return None
        if current_data_history.empty:
            logger.warning("Data history for XGBoost prediction is empty.")
            return None
            
        prediction_output = self.forecaster.predict(current_data_history) # XGBoostClassifierForecaster.predict returns a dict
        
        if prediction_output and isinstance(prediction_output, dict):
            return prediction_output
        else:
            logger.warning(f"Unexpected prediction output type from XGBoost forecaster: {type(prediction_output)}")
            return None

async def main_xgboost_inference_example():
    from kamikaze_komodo.data_handling.database_manager import DatabaseManager
    from datetime import datetime, timedelta, timezone
    
    if not settings:
        print("Settings not loaded, cannot run XGBoost Classifier inference example.")
        return
        
    symbol_to_predict = settings.default_symbol
    timeframe_to_predict = settings.default_timeframe
    
    if not settings.config.has_section("XGBoost_Classifier_Forecaster"):
        logger.error("Config section [XGBoost_Classifier_Forecaster] not found. Cannot run inference example.")
        return
        
    inference_engine = XGBoostClassifierInference(symbol=symbol_to_predict, timeframe=timeframe_to_predict)
    if inference_engine.forecaster.model is None:
        logger.error("Failed to load XGBoost model for inference. Exiting example.")
        return
        
    db_manager = DatabaseManager()
    hist_days = 30 
    start_dt = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_dt = datetime.now(timezone.utc)
    
    bars = db_manager.retrieve_bar_data(symbol_to_predict, timeframe_to_predict, start_dt, end_dt)
    db_manager.close()
    
    if not bars or len(bars) < 50: # Need enough for feature generation
        logger.error(f"Not enough recent data ({len(bars)} bars) to make an XGBoost prediction example.")
        return
        
    data_df = pd.DataFrame([b.model_dump() for b in bars])
    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
    data_df.set_index('timestamp', inplace=True)
    data_df.sort_index(inplace=True)
    
    logger.info(f"Making XGBoost prediction for {symbol_to_predict} ({timeframe_to_predict}) using last {len(data_df)} bars.")
    prediction_dict = inference_engine.get_prediction(data_df)
    
    if prediction_dict:
        logger.info(f"XGBoost Prediction for {symbol_to_predict} ({timeframe_to_predict}): {prediction_dict}")
    else:
        logger.warning(f"Could not get an XGBoost prediction for {symbol_to_predict} ({timeframe_to_predict}).")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main_xgboost_inference_example())
</code>

kamikaze_komodo/ml_models/inference_pipelines/lightgbm_inference.py:
<code>
# kamikaze_komodo/ml_models/inference_pipelines/lightgbm_inference.py
import pandas as pd
from typing import Optional, Union
import os
import numpy as np 
from kamikaze_komodo.ml_models.price_forecasting.lightgbm_forecaster import LightGBMForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings
logger = get_logger(__name__)
class LightGBMInference:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LightGBM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        
        model_params_config = settings.get_strategy_params(model_config_section) 
        
        _model_base_path = model_params_config.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = model_params_config.get('modelfilename', f"lgbm_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        script_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) 
        
        if not os.path.isabs(_model_base_path):
            self.model_load_path_dir = os.path.join(script_dir, _model_base_path)
        else:
            self.model_load_path_dir = _model_base_path
            
        self.model_full_load_path = os.path.join(self.model_load_path_dir, _model_filename)
        self.forecaster = LightGBMForecaster(model_path=self.model_full_load_path, params=model_params_config)
        if self.forecaster.model is None:
            logger.warning(f"LightGBMInference: Model could not be loaded from {self.model_full_load_path}. Predictions will not be available.")
    def get_prediction(self, current_data_history: pd.DataFrame) -> Optional[float]:
        """
        Gets a single prediction based on the current data history.
        Assumes current_data_history has enough data to form features for the last point.
        """
        if self.forecaster.model is None:
            logger.warning("No model loaded, cannot get prediction.")
            return None
        if current_data_history.empty:
            logger.warning("Data history is empty, cannot get prediction.")
            return None
        # The LightGBMForecaster's predict method expects a DataFrame, even for a single prediction.
        # It will use its create_features method, which typically uses lags.
        # So, pass the recent history. `predict` will internally call `create_features`.
        
        # Determine feature columns to use for prediction
        feature_cols_to_pass = None
        # Check if 'feature_columns' is in the forecaster's parameters (e.g., from config)
        feature_cols_str_from_params = self.forecaster.params.get('feature_columns')
        if isinstance(feature_cols_str_from_params, str) and feature_cols_str_from_params:
            feature_cols_to_pass = [col.strip() for col in feature_cols_str_from_params.split(',')]
        elif isinstance(feature_cols_str_from_params, list):
             feature_cols_to_pass = feature_cols_str_from_params
        # If not in params, `LightGBMForecaster.predict` will use `self.trained_feature_columns_` if available.
        # Or, `feature_columns_to_use` can be None if the model should use its internal defaults/trained features.
        prediction_output = self.forecaster.predict(current_data_history, feature_columns_to_use=feature_cols_to_pass)
        
        if prediction_output is None:
            return None
        
        if isinstance(prediction_output, pd.Series):
            if not prediction_output.empty:
                return prediction_output.iloc[-1] 
            else:
                logger.warning("Prediction series is empty.")
                return None
        elif isinstance(prediction_output, (float, np.float64)):
            return float(prediction_output)
        else:
            logger.warning(f"Unexpected prediction output type: {type(prediction_output)}")
            return None
async def main_inference_example():
    from kamikaze_komodo.data_handling.database_manager import DatabaseManager
    from datetime import datetime, timedelta, timezone
    
    if not settings:
        print("Settings not loaded, cannot run LightGBM inference example.")
        return
    symbol_to_predict = settings.default_symbol
    timeframe_to_predict = settings.default_timeframe
    if not settings.config.has_section("LightGBM_Forecaster"):
        logger.error("Config section [LightGBM_Forecaster] not found. Cannot run inference.")
        return
    inference_engine = LightGBMInference(symbol=symbol_to_predict, timeframe=timeframe_to_predict)
    if inference_engine.forecaster.model is None:
        logger.error("Failed to load model for inference. Exiting example.")
        return
    db_manager = DatabaseManager()
    hist_days = 30 
    start_dt = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_dt = datetime.now(timezone.utc)
    
    bars = db_manager.retrieve_bar_data(symbol_to_predict, timeframe_to_predict, start_dt, end_dt)
    db_manager.close()
    if not bars or len(bars) < 20: 
        logger.error(f"Not enough recent data ({len(bars)} bars) to make a prediction example.")
        return
        
    data_df = pd.DataFrame([b.model_dump() for b in bars])
    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
    data_df.set_index('timestamp', inplace=True)
    data_df.sort_index(inplace=True)
    
    logger.info(f"Making prediction for {symbol_to_predict} ({timeframe_to_predict}) using last {len(data_df)} bars.")
    prediction = inference_engine.get_prediction(data_df)
    if prediction is not None:
        logger.info(f"Prediction for {symbol_to_predict} ({timeframe_to_predict}): {prediction:.6f}")
    else:
        logger.warning(f"Could not get a prediction for {symbol_to_predict} ({timeframe_to_predict}).")
if __name__ == "__main__":
    import asyncio
    asyncio.run(main_inference_example())
</code>

kamikaze_komodo/orchestration/portfolio_manager.py:
<code>
# FILE: kamikaze_komodo/orchestration/portfolio_manager.py
import pandas as pd
from typing import Dict, List, Optional, Any, Type

from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.strategy_framework.strategy_manager import StrategyManager
# --- FIX: Import the Ehlers strategy ---
from kamikaze_komodo.strategy_framework.strategies.ehlers_instantaneous_trendline import EhlersInstantaneousTrendlineStrategy
from kamikaze_komodo.strategy_framework.strategies.ewmac import EWMACStrategy
from kamikaze_komodo.strategy_framework.strategies.bollinger_band_breakout_strategy import BollingerBandBreakoutStrategy
from kamikaze_komodo.strategy_framework.strategies.funding_rate_arb_strategy import FundingRateArbStrategy
from kamikaze_komodo.strategy_framework.strategies.ml_forecaster_strategy import MLForecasterStrategy
from kamikaze_komodo.portfolio_constructor.asset_allocator import HRPAllocator, BaseAssetAllocator, FixedWeightAssetAllocator
from kamikaze_komodo.portfolio_constructor.rebalancer import BasicRebalancer
from kamikaze_komodo.exchange_interaction.exchange_api import ExchangeAPI
from kamikaze_komodo.core.models import BarData, PortfolioSnapshot
from kamikaze_komodo.core.enums import SignalType

logger = get_logger(__name__)

# --- FIX: Add the Ehlers strategy to the class map ---
STRATEGY_CLASS_MAP: Dict[str, Type[BaseStrategy]] = {
    "EWMAC_Strategy": EWMACStrategy,
    "BollingerBandBreakout_Strategy": BollingerBandBreakoutStrategy,
    "FundingRateArb_Strategy": FundingRateArbStrategy,
    "MLForecaster_Strategy": MLForecasterStrategy,
    "EhlersInstantaneousTrendline_Strategy": EhlersInstantaneousTrendlineStrategy, # Add this line
}


class PortfolioManager:
    """
    The central orchestrator for the trading bot.
    Manages data, strategies, and execution for a portfolio of assets.
    """

    def __init__(
        self,
        exchange_api: Optional[Any] = None,
        trading_universe: Optional[List[str]] = None,
        strategy_instances: Optional[List[BaseStrategy]] = None,
        asset_allocator: Optional[BaseAssetAllocator] = None,
    ):
        if not settings:
            raise ValueError("Settings not loaded.")

        self.exchange_api = exchange_api if exchange_api else ExchangeAPI()
        self.is_backtest = exchange_api is not None
        self.timeframe = settings.default_timeframe

        self.strategy_manager = StrategyManager()
        self.portfolio_config = settings.get_strategy_params('Portfolio')

        if trading_universe is not None and strategy_instances is not None:
            # Programmatic configuration (for optimizer)
            self.trading_universe = trading_universe
            for strategy in strategy_instances:
                self.strategy_manager.add_strategy(strategy)
            self.asset_allocator = asset_allocator if asset_allocator is not None else HRPAllocator(params=settings.get_strategy_params('HRPAllocator'))
            self.rebalancer = BasicRebalancer(params=settings.get_strategy_params('Rebalancer'))
        else:
            # Config-file based configuration (for main backtest/live)
            self.trading_universe: List[str] = [s.strip() for s in self.portfolio_config.get('tradinguniverse', '').split(',')]
            self._load_strategies()
            self._initialize_constructor_components(self.portfolio_config)


        logger.info(f"Managing portfolio for universe: {self.trading_universe}")
        logger.info(f"Initialized Asset Allocator: {self.asset_allocator.__class__.__name__}")
        logger.info(f"Initialized Rebalancer: {self.rebalancer.__class__.__name__}")

        self.portfolio_snapshot = PortfolioSnapshot(
            total_value_usd=10000.0, # Will be updated by engine
            cash_balance_usd=10000.0,
            positions={asset: 0.0 for asset in self.trading_universe}
        )


    def _load_strategies(self):
        logger.info("Dynamically loading strategies from config...")
        active_strategies_str = self.portfolio_config.get('activestrategies', '')
        if not active_strategies_str:
            logger.warning("No 'ActiveStrategies' defined in [Portfolio] section of config.")
            return

        active_strategy_names = [s.strip() for s in active_strategies_str.split(',')]

        for strategy_name in active_strategy_names:
            strategy_class = STRATEGY_CLASS_MAP.get(strategy_name)
            if strategy_class:
                strategy_params = settings.get_strategy_params(strategy_name)
                for asset in self.trading_universe:
                    strategy_instance = strategy_class(symbol=asset, timeframe=self.timeframe, params=strategy_params)
                    self.strategy_manager.add_strategy(strategy_instance)
            else:
                logger.error(f"Strategy '{strategy_name}' is active but not found in STRATEGY_CLASS_MAP.")

    def _initialize_constructor_components(self, config: Dict[str, Any]):
        allocator_name = str(config.get('assetallocator', 'HRPAllocator')).lower()
        if allocator_name == 'hrpallocator':
            self.asset_allocator: BaseAssetAllocator = HRPAllocator(params=settings.get_strategy_params('HRPAllocator'))
        else:
            fixed_weights = {asset: 1.0/len(self.trading_universe) for asset in self.trading_universe}
            self.asset_allocator = FixedWeightAssetAllocator(target_weights=fixed_weights)

        self.rebalancer = BasicRebalancer(params=settings.get_strategy_params('Rebalancer'))

    async def run_cycle(self, historical_data_for_cycle: Optional[Dict[str, pd.DataFrame]] = None):
        if not historical_data_for_cycle:
            if not self.is_backtest:
                logger.warning("Live data fetching not yet implemented in PortfolioManager.run_cycle.")
            return

        # 1. Generate Signals from all active strategies
        signals: Dict[str, SignalType] = {}
        for strategy in self.strategy_manager.get_all_strategies():
            if strategy.symbol in historical_data_for_cycle:
                strategy.data_history = historical_data_for_cycle[strategy.symbol]
                
                bar_data_dict = strategy.data_history.iloc[-1].to_dict()
                bar_data_dict.pop('symbol', None)
                bar_data_dict.pop('timeframe', None)

                latest_bar_data = BarData(
                    timestamp=strategy.data_history.index[-1], 
                    symbol=strategy.symbol, 
                    timeframe=self.timeframe, 
                    **bar_data_dict
                )

                signal = strategy.on_bar_data(latest_bar_data)
                if isinstance(signal, list): signal = signal[0].signal_type if signal else SignalType.HOLD
                if signal != SignalType.HOLD:
                    signals[strategy.symbol] = signal

        logger.info(f"Consolidated signals: { {k: v.value for k, v in signals.items()} }")

        # 2. Determine Target Allocation
        target_allocations_pct: Dict[str, float] = {}

        assets_with_entry_signal = [asset for asset, sig in signals.items() if sig in [SignalType.LONG, SignalType.SHORT]]
        if assets_with_entry_signal:
            allocation_data = {asset: data for asset, data in historical_data_for_cycle.items() if asset in assets_with_entry_signal}
            weights = self.asset_allocator.allocate(
                assets=assets_with_entry_signal,
                portfolio_value=self.portfolio_snapshot.total_value_usd,
                historical_data=allocation_data
            )
            for asset, weight in weights.items():
                if signals.get(asset) == SignalType.SHORT:
                    target_allocations_pct[asset] = -abs(weight)
                elif signals.get(asset) == SignalType.LONG:
                    target_allocations_pct[asset] = abs(weight)

        for asset, sig in signals.items():
            if sig in [SignalType.CLOSE_LONG, SignalType.CLOSE_SHORT]:
                target_allocations_pct[asset] = 0.0

        logger.info(f"Target allocations (%): { {k: f'{v*100:.2f}%' for k,v in target_allocations_pct.items()} }")

        # 3. Generate Rebalancing Orders
        asset_prices = {asset: data.iloc[-1]['close'] for asset, data in historical_data_for_cycle.items() if not data.empty}
        rebalancing_orders = self.rebalancer.generate_rebalancing_orders(
            current_portfolio=self.portfolio_snapshot,
            target_allocations_pct=target_allocations_pct,
            asset_prices=asset_prices,
        )

        # 4. Execute Orders
        if not rebalancing_orders:
            logger.info("No rebalancing orders to execute.")
        else:
            logger.info(f"Executing {len(rebalancing_orders)} rebalancing orders...")
            for order_params in rebalancing_orders:
                logger.info(f"Placing order: {order_params}")
                await self.exchange_api.create_order(**order_params)
</code>

kamikaze_komodo/orchestration/scheduler.py:
<code>
# kamikaze_komodo/orchestration/scheduler.py

from typing import Optional
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.executors.pool import ThreadPoolExecutor, ProcessPoolExecutor
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings
import os

logger = get_logger(__name__)

class TaskScheduler:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(TaskScheduler, cls).__new__(cls, *args, **kwargs)
        return cls._instance

    def __init__(self, db_path: Optional[str] = "logs/scheduler_jobs.sqlite"):
        if hasattr(self, '_initialized') and self._initialized: # Ensure __init__ runs only once for singleton
            return
        
        if not settings:
            logger.critical("Settings not loaded. TaskScheduler cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.db_path = db_path
        if self.db_path and not os.path.isabs(self.db_path):
             # Get project root based on this file's location: kamikaze_komodo/orchestration/scheduler.py
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            self.db_path = os.path.join(project_root, self.db_path)
        
        db_dir = os.path.dirname(self.db_path)
        if not os.path.exists(db_dir):
            os.makedirs(db_dir, exist_ok=True)
            logger.info(f"Created directory for scheduler database: {db_dir}")

        jobstores = {
            'default': SQLAlchemyJobStore(url=f'sqlite:///{self.db_path}')
        }
        executors = {
            'default': ThreadPoolExecutor(10), # For I/O bound tasks
            'processpool': ProcessPoolExecutor(3) # For CPU bound tasks
        }
        job_defaults = {
            'coalesce': False, # Run missed jobs if scheduler was down (be careful with this)
            'max_instances': 3 # Max parallel instances of the same job
        }
        
        self.scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            executors=executors,
            job_defaults=job_defaults,
            timezone='UTC' # Explicitly set timezone
        )
        self._initialized = True
        logger.info(f"TaskScheduler initialized with SQLite job store at: {self.db_path}")

    def start(self):
        if not self.scheduler.running:
            try:
                self.scheduler.start()
                logger.info("APScheduler started.")
            except Exception as e:
                logger.error(f"Failed to start APScheduler: {e}", exc_info=True)
        else:
            logger.info("APScheduler is already running.")

    def shutdown(self, wait: bool = True):
        if self.scheduler.running:
            try:
                self.scheduler.shutdown(wait=wait)
                logger.info("APScheduler shut down.")
            except Exception as e:
                logger.error(f"Error shutting down APScheduler: {e}", exc_info=True)

    def add_job(self, func, trigger: str = 'interval', **kwargs):
        """
        Adds a job to the scheduler.
        Args:
            func: The function to execute.
            trigger: The trigger type (e.g., 'interval', 'cron', 'date').
            **kwargs: Arguments for the trigger and job (e.g., minutes=1, id='my_job').
        """
        try:
            job = self.scheduler.add_job(func, trigger, **kwargs)
            logger.info(f"Job '{kwargs.get('id', func.__name__)}' added with trigger: {trigger}, params: {kwargs}")
            return job
        except Exception as e:
            logger.error(f"Failed to add job '{kwargs.get('id', func.__name__)}': {e}", exc_info=True)
            return None

    def remove_job(self, job_id: str):
        try:
            self.scheduler.remove_job(job_id)
            logger.info(f"Job '{job_id}' removed.")
        except Exception as e: # Specific exception: JobLookupError
            logger.warning(f"Failed to remove job '{job_id}': {e}")

# --- Example Scheduled Tasks (Conceptual for Phase 6 "Begin Integration") ---
async def example_data_polling_task():
    logger.info("Scheduler: Running example_data_polling_task...")
    # In a real scenario, this would call a method in DataFetcher or a dedicated data polling module.
    # from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
    # fetcher = DataFetcher()
    # await fetcher.fetch_latest_data_for_active_symbols() # Fictional method
    # await fetcher.close()
    await asyncio.sleep(2) # Simulate work
    logger.info("Scheduler: example_data_polling_task finished.")

async def example_news_scraping_task():
    logger.info("Scheduler: Running example_news_scraping_task...")
    # from kamikaze_komodo.ai_news_analysis_agent_module.news_scraper import NewsScraper
    # scraper = NewsScraper()
    # articles = await scraper.scrape_all(limit_per_source=5, since_hours_rss=12)
    # if articles:
    #     logger.info(f"Scheduler: Scraped {len(articles)} news articles.")
        # Further processing: store, analyze sentiment, etc.
    await asyncio.sleep(5) # Simulate work
    logger.info("Scheduler: example_news_scraping_task finished.")

async def example_model_retraining_check_task():
    logger.info("Scheduler: Running example_model_retraining_check_task...")
    # This task would check conditions for retraining ML models
    # e.g., time since last training, performance degradation, new data volume
    # from kamikaze_komodo.ml_models.training_pipelines.lightgbm_pipeline import LightGBMTrainingPipeline
    # if conditions_met_for_retraining("LightGBM_BTCUSD_1h"):
    #     pipeline = LightGBMTrainingPipeline(symbol="BTC/USD", timeframe="1h")
    #     await pipeline.run_training()
    await asyncio.sleep(3)
    logger.info("Scheduler: example_model_retraining_check_task finished.")


async def main_scheduler_example():
    """Demonstrates basic scheduler setup and job addition."""
    scheduler_manager = TaskScheduler()

    # Add example jobs (these won't run unless scheduler is started and loop runs)
    scheduler_manager.add_job(example_data_polling_task, 'interval', minutes=15, id='data_poll_main')
    scheduler_manager.add_job(example_news_scraping_task, 'cron', hour='*/2', id='news_scrape_bi_hourly') # Every 2 hours
    scheduler_manager.add_job(example_model_retraining_check_task, 'cron', day_of_week='sun', hour='3', minute='0', id='weekly_retrain_check')

    try:
        scheduler_manager.start()
        # Keep the main thread alive to allow scheduler to run, or integrate into main application loop
        # For this example, we'll just let it run for a short period.
        # In a real app, asyncio.get_event_loop().run_forever() or similar would be used.
        if settings and settings.log_level.upper() == "DEBUG": # Only run for a bit in debug
            logger.info("Scheduler example running for 30 seconds (DEBUG mode)...")
            await asyncio.sleep(30)
        else:
            logger.info("Scheduler example configured. In a full app, it would run continuously.")
            # For non-debug, don't block indefinitely here in a simple example.
            # Real app would have its own main loop.
            
    except (KeyboardInterrupt, SystemExit):
        logger.info("Scheduler example interrupted.")
    finally:
        scheduler_manager.shutdown()

if __name__ == "__main__":
    import asyncio
    # This example shows how to set up the scheduler.
    # It's best integrated into the main application's async loop (e.g., in main.py).
    # Run with: python -m kamikaze_komodo.orchestration.scheduler
    
    # To see jobs persist and get reloaded, run, stop, then run again.
    # Check the logs/scheduler_jobs.sqlite file.
    
    # Note: Running this standalone will schedule jobs. If you run it multiple times
    # without clearing the scheduler_jobs.sqlite, it might try to add duplicate jobs
    # if the `replace_existing=True` option is not used in add_job and IDs are the same.
    # The current setup will log errors for duplicate job IDs if they are not replaced.
    asyncio.run(main_scheduler_example())
</code>

kamikaze_komodo/orchestration/__init__.py:
<code>
# kamikaze_komodo/orchestration/__init__.py
# This file makes the 'orchestration' directory a Python package.
</code>

kamikaze_komodo/data_handling/database_manager.py:
<code>
# kamikaze_komodo/data_handling/database_manager.py
# Updated to include store/retrieve for NewsArticle
# Phase 6: Minor modification to bar_data table to include market_regime.
import sqlite3
from typing import List, Optional, Dict, Any # Added Dict, Any
from kamikaze_komodo.core.models import BarData, NewsArticle, FundingRate # Added FundingRate
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone, UTC 
import json # For storing dicts/lists like related_symbols or key_themes

logger = get_logger(__name__)

class DatabaseManager:
    """
    Manages local storage of data (initially SQLite).
    Timestamps are stored as ISO 8601 TEXT.
    Lists/Dicts are stored as JSON TEXT.
    """
    def __init__(self, db_name: str = "kamikaze_komodo_data.db"):
        self.db_name = db_name
        self.conn: Optional[sqlite3.Connection] = None
        self._connect()
        self._create_tables()

    def _connect(self):
        try:
            self.conn = sqlite3.connect(self.db_name, detect_types=sqlite3.PARSE_COLNAMES)
            self.conn.row_factory = sqlite3.Row 
            logger.info(f"Successfully connected to database: {self.db_name}")
        except sqlite3.Error as e:
            logger.error(f"Error connecting to database {self.db_name}: {e}")
            self.conn = None

    def _create_tables(self):
        if not self.conn:
            logger.error("Cannot create tables, no database connection.")
            return
        try:
            cursor = self.conn.cursor()
            # BarData Table
            # Add funding_rate column
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS bar_data (
                    timestamp TEXT NOT NULL, 
                    symbol TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    open REAL NOT NULL,
                    high REAL NOT NULL,
                    low REAL NOT NULL,
                    close REAL NOT NULL,
                    volume REAL NOT NULL,
                    atr REAL, 
                    sentiment_score REAL,
                    prediction_value REAL,
                    prediction_confidence REAL,
                    market_regime INTEGER,
                    funding_rate REAL,
                    PRIMARY KEY (timestamp, symbol, timeframe)
                )
            """)
            # Funding Rate Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS funding_rates (
                    timestamp TEXT NOT NULL,
                    symbol TEXT NOT NULL,
                    funding_rate REAL NOT NULL,
                    mark_price REAL,
                    PRIMARY KEY (timestamp, symbol)
                )
            """)
            # NewsArticle Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS news_articles (
                    id TEXT PRIMARY KEY,
                    url TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    publication_date TEXT, 
                    retrieval_date TEXT NOT NULL, 
                    source TEXT NOT NULL,
                    content TEXT,
                    summary TEXT,
                    sentiment_score REAL,
                    sentiment_label TEXT,
                    sentiment_confidence REAL,
                    key_themes TEXT,
                    related_symbols TEXT,
                    raw_llm_response TEXT 
                )
            """)
            self.conn.commit()
            logger.info("Tables checked/created successfully.")
        except sqlite3.Error as e:
            logger.error(f"Error creating tables: {e}")

    def _to_iso_format(self, dt: Optional[datetime]) -> Optional[str]:
        if dt is None: return None
        if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:
            dt = dt.replace(tzinfo=UTC)
        else:
            dt = dt.astimezone(UTC)
        return dt.isoformat()

    def _from_iso_format(self, iso_str: Optional[str]) -> Optional[datetime]:
        if iso_str is None: return None
        try:
            dt = datetime.fromisoformat(iso_str)
            if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:
                return dt.replace(tzinfo=UTC)
            return dt.astimezone(UTC)
        except ValueError:
            logger.warning(f"Could not parse ISO timestamp string: {iso_str}")
            return None

    def store_bar_data(self, bar_data_list: List[BarData]):
        if not self.conn: logger.error("No DB connection for bar data."); return False
        if not bar_data_list: logger.info("No bar data to store."); return True
        try:
            cursor = self.conn.cursor()
            data_to_insert = [
                (
                    self._to_iso_format(bd.timestamp), bd.symbol, bd.timeframe,
                    bd.open, bd.high, bd.low, bd.close, bd.volume,
                    bd.atr, bd.sentiment_score,
                    bd.prediction_value, bd.prediction_confidence,
                    bd.market_regime, bd.funding_rate
                ) for bd in bar_data_list
            ]
            cursor.executemany("""
                INSERT OR REPLACE INTO bar_data 
                (timestamp, symbol, timeframe, open, high, low, close, volume, atr, sentiment_score,
                 prediction_value, prediction_confidence, market_regime, funding_rate)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?) 
            """, data_to_insert) 
            self.conn.commit()
            logger.info(f"Stored/Replaced {len(data_to_insert)} bar data entries. ({cursor.rowcount} affected)")
            return True
        except Exception as e:
            logger.error(f"Error storing bar data: {e}", exc_info=True); return False

    def retrieve_bar_data(self, symbol: str, timeframe: str, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> List[BarData]:
        if not self.conn: logger.error("No DB connection for bar data."); return []
        try:
            cursor = self.conn.cursor()
            query = "SELECT * FROM bar_data WHERE symbol = ? AND timeframe = ?"
            params = [symbol, timeframe]
            if start_date: query += " AND timestamp >= ?"; params.append(self._to_iso_format(start_date))
            if end_date: query += " AND timestamp <= ?"; params.append(self._to_iso_format(end_date))
            query += " ORDER BY timestamp ASC"

            cursor.execute(query, tuple(params))
            rows = cursor.fetchall()
            bar_data_list = [BarData(**row) for row in (dict(row) for row in rows)]
            logger.info(f"Retrieved {len(bar_data_list)} bar data entries for {symbol} ({timeframe}).")
            return bar_data_list
        except Exception as e:
            logger.error(f"Error retrieving bar data for {symbol} ({timeframe}): {e}", exc_info=True); return []

    def store_funding_rates(self, funding_rates: List[FundingRate]):
        if not self.conn: logger.error("No DB connection for funding rates."); return False
        if not funding_rates: logger.info("No funding rates to store."); return True
        try:
            cursor = self.conn.cursor()
            data_to_insert = [
                (self._to_iso_format(fr.timestamp), fr.symbol, fr.funding_rate, fr.mark_price) for fr in funding_rates
            ]
            cursor.executemany("""
                INSERT OR REPLACE INTO funding_rates
                (timestamp, symbol, funding_rate, mark_price)
                VALUES (?, ?, ?, ?)
            """, data_to_insert)
            self.conn.commit()
            logger.info(f"Stored/Replaced {len(data_to_insert)} funding rate entries.")
            return True
        except Exception as e:
            logger.error(f"Error storing funding rates: {e}", exc_info=True); return False

    def retrieve_funding_rates(self, symbol: str, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> List[FundingRate]:
        if not self.conn: logger.error("No DB connection for funding rates."); return []
        try:
            cursor = self.conn.cursor()
            query = "SELECT * FROM funding_rates WHERE symbol = ?"
            params = [symbol]
            if start_date: query += " AND timestamp >= ?"; params.append(self._to_iso_format(start_date))
            if end_date: query += " AND timestamp <= ?"; params.append(self._to_iso_format(end_date))
            query += " ORDER BY timestamp ASC"
            cursor.execute(query, tuple(params))
            rows = cursor.fetchall()
            rates_list = [FundingRate(**row) for row in (dict(row) for row in rows)]
            logger.info(f"Retrieved {len(rates_list)} funding rate entries for {symbol}.")
            return rates_list
        except Exception as e:
            logger.error(f"Error retrieving funding rates for {symbol}: {e}", exc_info=True); return []
    
    def store_news_articles(self, articles: List[NewsArticle]):
        if not self.conn: logger.error("No DB connection for news articles."); return False
        if not articles: logger.info("No news articles to store."); return True
        try:
            cursor = self.conn.cursor()
            data_to_insert = [
                (
                    article.id, article.url, article.title,
                    self._to_iso_format(article.publication_date),
                    self._to_iso_format(article.retrieval_date),
                    article.source, article.content, article.summary,
                    article.sentiment_score, article.sentiment_label,
                    article.sentiment_confidence,
                    json.dumps(article.key_themes) if article.key_themes else None,
                    json.dumps(article.related_symbols) if article.related_symbols else None,
                    json.dumps(article.raw_llm_response) if article.raw_llm_response else None
                ) for article in articles
            ]
            
            cursor.executemany("""
                INSERT OR REPLACE INTO news_articles
                (id, url, title, publication_date, retrieval_date, source, content, summary,
                 sentiment_score, sentiment_label, sentiment_confidence, key_themes, related_symbols, raw_llm_response)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, data_to_insert)
            self.conn.commit()
            logger.info(f"Stored/Replaced {len(data_to_insert)} news articles. ({cursor.rowcount} affected)")
            return True
        except Exception as e:
            logger.error(f"Error storing news articles: {e}", exc_info=True); return False

    def retrieve_news_articles(self, symbol: Optional[str] = None, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None, source: Optional[str] = None, limit: int = 100) -> List[NewsArticle]:
        if not self.conn: logger.error("No DB connection for news articles."); return []
        try:
            cursor = self.conn.cursor()
            query = "SELECT * FROM news_articles WHERE 1=1"
            params = []

            if symbol:
                query += " AND related_symbols LIKE ?"
                params.append(f'%"{symbol}"%')
            if start_date:
                query += " AND publication_date >= ?"
                params.append(self._to_iso_format(start_date))
            if end_date:
                query += " AND publication_date <= ?"
                params.append(self._to_iso_format(end_date))
            if source:
                query += " AND source = ?"
                params.append(source)
            
            query += " ORDER BY publication_date DESC, retrieval_date DESC LIMIT ?"
            params.append(limit)

            cursor.execute(query, tuple(params))
            rows = cursor.fetchall()
            articles_list = [NewsArticle(**dict(row)) for row in rows]
            logger.info(f"Retrieved {len(articles_list)} news articles with given criteria.")
            return articles_list
        except Exception as e:
            logger.error(f"Error retrieving news articles: {e}", exc_info=True); return []

    def close(self):
        if self.conn: self.conn.close(); logger.info("Database connection closed."); self.conn = None

    def __del__(self): self.close()
</code>

kamikaze_komodo/data_handling/__init__.py:
<code>
# kamikaze_komodo/data_handling/__init__.py
# This file makes the 'data_handling' directory a Python package.
</code>

kamikaze_komodo/data_handling/data_fetcher.py:
<code>
# kamikaze_komodo/data_handling/data_fetcher.py
import ccxt.async_support as ccxt # Use async version for future compatibility
import asyncio
from typing import List, Optional, Tuple
from datetime import datetime, timedelta, timezone
# Assuming these are correctly located relative to this file for your project structure
from kamikaze_komodo.core.models import BarData, FundingRate
from kamikaze_komodo.core.utils import ohlcv_to_bardata
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Ensure settings is loaded globally

logger = get_logger(__name__)

class DataFetcher:
    """
    Fetches historical and real-time market data using CCXT.
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. DataFetcher cannot be initialized.")
            raise ValueError("Settings not loaded. Ensure config files are present and correct.")

        self.exchange_id = settings.exchange_id_to_use
        exchange_class = getattr(ccxt, self.exchange_id, None)
        
        if not exchange_class:
            logger.error(f"Exchange '{self.exchange_id}' is not supported by CCXT.")
            raise ValueError(f"Exchange '{self.exchange_id}' is not supported by CCXT.")

        config = {
            'apiKey': settings.kraken_api_key,
            'secret': settings.kraken_secret_key,
            'enableRateLimit': True,
        }
        
        self.exchange = exchange_class(config)
        logger.info(f"Instantiated CCXT exchange class: {self.exchange_id}")

        if settings.kraken_testnet:
            if hasattr(self.exchange, 'set_sandbox_mode') and callable(self.exchange.set_sandbox_mode):
                try:
                    self.exchange.set_sandbox_mode(True)
                    logger.info(f"CCXT sandbox mode successfully enabled for {self.exchange_id}.")
                except ccxt.NotSupported:
                    logger.warning(f"{self.exchange_id} does not support unified set_sandbox_mode via method.")
                except Exception as e:
                    logger.error(f"An error occurred while trying to set sandbox mode for {self.exchange_id}: {e}", exc_info=True)
            else:
                logger.warning(f"{self.exchange_id} CCXT class does not have a 'set_sandbox_mode' method.")
        
        # self.exchange.verbose = True # Uncomment for debugging API calls
        logger.info(f"Initialized DataFetcher for '{self.exchange_id}'. Configured Testnet (Sandbox) from settings: {settings.kraken_testnet}")

    async def fetch_historical_ohlcv(self, symbol: str, timeframe: str, since: Optional[datetime] = None, limit: Optional[int] = None, params: Optional[dict] = None) -> List[BarData]:
        if not self.exchange.has['fetchOHLCV']:
            logger.error(f"{self.exchange_id} does not support fetchOHLCV.")
            return []

        since_timestamp_ms = int(since.timestamp() * 1000) if since else None

        ohlcv_data_list: List[BarData] = []
        try:
            logger.info(f"Fetching historical OHLCV for {symbol} ({timeframe}) since {since} with limit {limit}")
            raw_ohlcv = await self.exchange.fetch_ohlcv(symbol, timeframe, since_timestamp_ms, limit, params or {})
            
            if raw_ohlcv:
                for entry in raw_ohlcv:
                    try:
                        bar = ohlcv_to_bardata(entry, symbol, timeframe)
                        ohlcv_data_list.append(bar)
                    except ValueError as e_bar:
                        logger.warning(f"Skipping invalid OHLCV entry for {symbol} ({timeframe}): {entry}. Error: {e_bar}")
                logger.info(f"Successfully fetched {len(ohlcv_data_list)} candles for {symbol} ({timeframe}).")
        except Exception as e:
            logger.error(f"An unexpected error occurred in fetch_historical_ohlcv for {symbol}: {e}", exc_info=True)
        return ohlcv_data_list

    async def fetch_historical_data_for_period(self, symbol: str, timeframe: str, start_date: datetime, end_date: datetime = datetime.now(timezone.utc)) -> List[BarData]:
        all_bars: List[BarData] = []
        current_start_date = start_date
        
        timeframe_duration_seconds = self.exchange.parse_timeframe(timeframe)
        if timeframe_duration_seconds is None:
            logger.error(f"Could not parse timeframe: {timeframe} for {self.exchange_id}.")
            return []

        logger.info(f"Fetching historical period data for {symbol} ({timeframe}) from {start_date} to {end_date}")

        while current_start_date < end_date:
            limit_per_call = 500
            bars = await self.fetch_historical_ohlcv(symbol, timeframe, since=current_start_date, limit=limit_per_call)
            
            if not bars: break 
            
            relevant_bars = [b for b in bars if b.timestamp < end_date]
            if not relevant_bars: break

            all_bars.extend(relevant_bars)
            last_fetched_timestamp = relevant_bars[-1].timestamp
            current_start_date = last_fetched_timestamp + timedelta(seconds=timeframe_duration_seconds)
            
            if current_start_date >= end_date: break
            
            await asyncio.sleep(self.exchange.rateLimit / 1000.0 if isinstance(self.exchange.rateLimit, (int, float)) and self.exchange.rateLimit > 0 else 0.2)

        if all_bars:
            unique_bars_dict = {bar.timestamp: bar for bar in all_bars}
            all_bars = sorted(list(unique_bars_dict.values()), key=lambda b: b.timestamp)
            logger.info(f"Total unique historical bars fetched for {symbol} ({timeframe}) in period: {len(all_bars)}")
        
        return all_bars

    async def fetch_funding_rate_history(self, symbol: str, since: Optional[datetime] = None, limit: Optional[int] = 100) -> List[FundingRate]:
        """Fetches historical funding rates for a perpetual futures symbol."""
        if not self.exchange.has['fetchFundingRateHistory']:
            logger.warning(f"Exchange {self.exchange_id} does not support fetchFundingRateHistory.")
            return []
        
        since_ms = int(since.timestamp() * 1000) if since else None
        logger.info(f"Fetching funding rate history for {symbol} since {since}.")
        try:
            raw_rates = await self.exchange.fetch_funding_rate_history(symbol, since=since_ms, limit=limit)
            funding_rates = [
                FundingRate(
                    symbol=rate['symbol'],
                    timestamp=datetime.fromtimestamp(rate['timestamp'] / 1000, tz=timezone.utc),
                    funding_rate=rate['fundingRate'],
                    mark_price=rate.get('markPrice')
                ) for rate in raw_rates
            ]
            logger.info(f"Fetched {len(funding_rates)} funding rate entries for {symbol}.")
            return funding_rates
        except Exception as e:
            logger.error(f"Error fetching funding rate history for {symbol}: {e}", exc_info=True)
            return []


    async def close(self):
        """Closes the CCXT exchange connection."""
        try:
            if hasattr(self.exchange, 'close') and callable(self.exchange.close):
                await self.exchange.close()
            logger.info(f"CCXT exchange connection for {self.exchange_id} closed.")
        except Exception as e:
            logger.error(f"Error closing CCXT exchange connection for {self.exchange_id}: {e}", exc_info=True)
</code>

kamikaze_komodo/exchange_interaction/__init__.py:
<code>
# kamikaze_komodo/exchange_interaction/__init__.py
# This file makes the 'exchange_interaction' directory a Python package.
</code>

kamikaze_komodo/exchange_interaction/exchange_api.py:
<code>
# kamikaze_komodo/exchange_interaction/exchange_api.py
import ccxt.async_support as ccxt
import asyncio
from typing import Dict, Optional, List
from kamikaze_komodo.core.enums import OrderType, OrderSide
from kamikaze_komodo.core.models import Order
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Import global settings
from datetime import datetime, timezone # Ensure timezone is imported

logger = get_logger(__name__)

class ExchangeAPI:
    """
    Handles interactions with the cryptocurrency exchange.
    Manages order placement, cancellation, and fetching account information.
    Phase 6: Added explicit check for short selling capability (though CCXT often handles this implicitly for derivative exchanges).
    """
    def __init__(self, exchange_id: Optional[str] = None): # exchange_id is now optional
        if not settings:
            logger.critical("Settings not loaded. ExchangeAPI cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.exchange_id = exchange_id if exchange_id else settings.exchange_id_to_use
        exchange_class = getattr(ccxt, self.exchange_id, None)

        if not exchange_class:
            logger.error(f"Exchange {self.exchange_id} is not supported by CCXT.")
            raise ValueError(f"Exchange {self.exchange_id} is not supported by CCXT.")

        # Determine API keys based on the exchange_id
        # This example assumes a single set of keys in settings (e.g., KRAKEN_API)
        # For a multi-exchange system, you'd fetch keys specific to self.exchange_id
        api_key = settings.kraken_api_key # Defaulting to Kraken keys for now
        secret_key = settings.kraken_secret_key # Defaulting to Kraken keys
        use_testnet = settings.kraken_testnet # Defaulting to Kraken testnet setting

        # Example for specific exchange key loading (if settings were structured differently)
        # if self.exchange_id == 'binance':
        #     api_key = settings.binance_api_key
        #     secret_key = settings.binance_secret_key
        #     use_testnet = settings.binance_testnet
        # elif self.exchange_id == 'krakenfutures':
        #     api_key = settings.kraken_futures_api_key # etc.

        config = {
            'apiKey': api_key,
            'secret': secret_key,
            'enableRateLimit': True,
        }
        self.exchange = exchange_class(config)
        logger.info(f"Initialized ExchangeAPI for {self.exchange_id}.")

        if use_testnet:
            if hasattr(self.exchange, 'set_sandbox_mode') and callable(self.exchange.set_sandbox_mode):
                try:
                    self.exchange.set_sandbox_mode(True)
                    logger.info(f"Sandbox mode enabled for {self.exchange_id}.")
                except ccxt.NotSupported:
                    logger.warning(f"{self.exchange_id} does not support unified set_sandbox_mode. Testnet functionality depends on API keys/URL.")
                except Exception as e_sandbox:
                    logger.error(f"Error setting sandbox mode for {self.exchange_id}: {e_sandbox}")
            else:
                logger.warning(f"{self.exchange_id} does not have set_sandbox_mode. Testnet relies on specific API keys or default URL pointing to sandbox.")
        else:
            logger.info(f"Running in live mode for {self.exchange_id}.")

        if not api_key or "YOUR_API_KEY" in str(api_key).upper() or (isinstance(api_key, str) and "D27PYGI95TLS" in api_key.upper()): # Check specific placeholder
            logger.warning(f"API key for {self.exchange_id} appears to be a placeholder or is not configured. Authenticated calls may fail.")

    async def fetch_balance(self) -> Optional[Dict]:
        if not self.exchange.has['fetchBalance']:
            logger.error(f"{self.exchange_id} does not support fetchBalance.")
            return None
        try:
            balance = await self.exchange.fetch_balance()
            logger.info(f"Successfully fetched balance from {self.exchange_id}.")
            return balance
        except ccxt.NetworkError as e:
            logger.error(f"Network error fetching balance: {e}", exc_info=True)
        except ccxt.AuthenticationError as e_auth:
            logger.error(f"Authentication error fetching balance from {self.exchange_id}. Check API keys and permissions: {e_auth}", exc_info=True)
            return None # Explicitly return None on auth error
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error fetching balance: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred fetching balance: {e}", exc_info=True)
        return None

    async def create_order(
        self,
        symbol: str,
        order_type: OrderType,
        side: OrderSide,
        amount: float,
        price: Optional[float] = None,
        params: Optional[Dict] = None
    ) -> Optional[Order]:
        if order_type == OrderType.LIMIT and price is None:
            logger.error("Price must be specified for a LIMIT order.")
            return None

        if not self.exchange.has['createOrder']:
            logger.error(f"{self.exchange_id} does not support createOrder.")
            return None

        order_type_str = order_type.value
        side_str = side.value

        # Phase 6: Check for short selling specific capabilities (conceptual for CCXT futures)
        if side == OrderSide.SELL: # This could be opening a short or closing a long
            # For many futures exchanges, 'sell' with no existing position implies short.
            # CCXT often handles this implicitly. Some exchanges might need specific params for short.
            # e.g., params = {'reduceOnly': False} if it was to ensure opening a new position.
            # We assume for now that a simple SELL order will open a short if no long position exists.
            # If the exchange has explicit shorting methods (less common in CCXT unified API), that'd be different.
            logger.info(f"Preparing to place a SELL order for {symbol}. This may open a short position.")

        try:
            logger.info(f"Attempting to place {side_str} {order_type_str} order for {amount} {symbol} at price {price if price else 'market'} on {self.exchange_id}")
            
            # Check for placeholder API keys again before actual call
            is_placeholder_key = not self.exchange.apiKey or "YOUR_API_KEY" in self.exchange.apiKey.upper() or "D27PYGI95TLS" in self.exchange.apiKey.upper()
            if settings.kraken_testnet and is_placeholder_key : # Use general testnet flag
                logger.warning(f"Simulating order creation for {self.exchange_id} due to testnet mode and placeholder API keys.")
                simulated_order_id = f"sim_{self.exchange_id}_{ccxt.Exchange.uuid()}"
                return Order(
                    id=simulated_order_id,
                    symbol=symbol,
                    type=order_type,
                    side=side,
                    amount=amount,
                    price=price if order_type == OrderType.LIMIT else None,
                    timestamp=datetime.now(timezone.utc), # Use timezone.utc
                    status="open", # Simulate as open
                    exchange_id=simulated_order_id
                )

            exchange_order_response = await self.exchange.create_order(symbol, order_type_str, side_str, amount, price, params or {})
            logger.info(f"Successfully placed order on {self.exchange_id}. Order ID: {exchange_order_response.get('id')}")
            
            created_order = Order(
                id=str(exchange_order_response.get('id')),
                symbol=exchange_order_response.get('symbol'),
                type=OrderType(exchange_order_response.get('type', order_type_str).lower()),
                side=OrderSide(exchange_order_response.get('side', side_str).lower()),
                amount=float(exchange_order_response.get('amount', amount)),
                price=float(exchange_order_response['price']) if exchange_order_response.get('price') else None,
                timestamp=datetime.fromtimestamp(exchange_order_response['timestamp'] / 1000, tz=timezone.utc) if exchange_order_response.get('timestamp') else datetime.now(timezone.utc),
                status=exchange_order_response.get('status', 'open'),
                filled_amount=float(exchange_order_response.get('filled', 0.0)),
                average_fill_price=float(exchange_order_response.get('average')) if exchange_order_response.get('average') else None,
                exchange_id=str(exchange_order_response.get('id'))
            )
            return created_order

        except ccxt.InsufficientFunds as e:
            logger.error(f"Insufficient funds to place order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.InvalidOrder as e:
            logger.error(f"Invalid order parameters for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.AuthenticationError as e_auth:
            logger.error(f"Authentication error placing order for {symbol} on {self.exchange_id}. Check API keys: {e_auth}", exc_info=True)
        except ccxt.NetworkError as e:
            logger.error(f"Network error placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e: # Catch specific exchange errors before generic Exception
            logger.error(f"Exchange error placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        return None

    async def cancel_order(self, order_id: str, symbol: Optional[str] = None, params: Optional[Dict] = None) -> bool:
        if not self.exchange.has['cancelOrder']:
            logger.error(f"{self.exchange_id} does not support cancelOrder.")
            return False
        try:
            await self.exchange.cancel_order(order_id, symbol, params or {})
            logger.info(f"Successfully requested cancellation for order ID {order_id} on {self.exchange_id}.")
            return True
        except ccxt.OrderNotFound as e:
            logger.error(f"Order ID {order_id} not found for cancellation on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.NetworkError as e:
            logger.error(f"Network error canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        return False

    async def fetch_order(self, order_id: str, symbol: Optional[str] = None) -> Optional[Order]:
        if not self.exchange.has['fetchOrder']:
            logger.warning(f"{self.exchange_id} does not support fetching individual orders directly.")
            return None
        try:
            exchange_order_response = await self.exchange.fetch_order(order_id, symbol)
            fetched_order = Order(
                id=str(exchange_order_response.get('id')),
                symbol=exchange_order_response.get('symbol'),
                type=OrderType(exchange_order_response.get('type').lower()),
                side=OrderSide(exchange_order_response.get('side').lower()),
                amount=float(exchange_order_response.get('amount')),
                price=float(exchange_order_response['price']) if exchange_order_response.get('price') else None,
                timestamp=datetime.fromtimestamp(exchange_order_response['timestamp'] / 1000, tz=timezone.utc) if exchange_order_response.get('timestamp') else datetime.now(timezone.utc),
                status=exchange_order_response.get('status'),
                filled_amount=float(exchange_order_response.get('filled', 0.0)),
                average_fill_price=float(exchange_order_response.get('average')) if exchange_order_response.get('average') else None,
                exchange_id=str(exchange_order_response.get('id'))
            )
            return fetched_order
        except ccxt.OrderNotFound:
            logger.warning(f"Order {order_id} not found on {self.exchange_id}.")
        except Exception as e:
            logger.error(f"Error fetching order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        return None

    async def fetch_open_orders(self, symbol: Optional[str] = None, since: Optional[datetime] = None, limit: Optional[int] = None) -> List[Order]: # Changed since to datetime
        open_orders_list = []
        if not self.exchange.has['fetchOpenOrders']:
            logger.warning(f"{self.exchange_id} does not support fetching open orders.")
            return open_orders_list

        try:
            since_timestamp_ms = int(since.timestamp() * 1000) if since else None
            raw_orders = await self.exchange.fetch_open_orders(symbol, since_timestamp_ms, limit)
            for ex_order in raw_orders:
                order = Order(
                    id=str(ex_order.get('id')),
                    symbol=ex_order.get('symbol'),
                    type=OrderType(ex_order.get('type').lower()),
                    side=OrderSide(ex_order.get('side').lower()),
                    amount=float(ex_order.get('amount')),
                    price=float(ex_order['price']) if ex_order.get('price') else None,
                    timestamp=datetime.fromtimestamp(ex_order['timestamp'] / 1000, tz=timezone.utc) if ex_order.get('timestamp') else datetime.now(timezone.utc),
                    status=ex_order.get('status', 'open'),
                    filled_amount=float(ex_order.get('filled', 0.0)),
                    average_fill_price=float(ex_order.get('average')) if ex_order.get('average') else None,
                    exchange_id=str(ex_order.get('id'))
                )
                open_orders_list.append(order)
            logger.info(f"Fetched {len(open_orders_list)} open orders for symbol {symbol if symbol else 'all'} on {self.exchange_id}.")
        except Exception as e:
            logger.error(f"Error fetching open orders on {self.exchange_id}: {e}", exc_info=True)
        return open_orders_list

    async def close(self):
        try:
            if hasattr(self.exchange, 'close') and callable(self.exchange.close):
                await self.exchange.close()
                logger.info(f"CCXT exchange connection for {self.exchange_id} closed.")
        except Exception as e:
            logger.error(f"Error closing CCXT exchange connection for {self.exchange_id}: {e}", exc_info=True)

# Example Usage (run within an asyncio event loop):
async def main_exchange_api_example():
    if not settings:
        print("Settings could not be loaded. Exiting example.")
        return

    exchange_api = ExchangeAPI() # Uses exchange_id from settings
    balance = await exchange_api.fetch_balance()
    if balance:
        logger.info(f"Free USD Balance: {balance.get('USD', {}).get('free', 'N/A')}")
        logger.info(f"Free {settings.default_symbol.split('/')[0]} Balance: {balance.get(settings.default_symbol.split('/')[0], {}).get('free', 'N/A')}")

    # target_symbol = settings.default_symbol
    # order_to_place = await exchange_api.create_order(
    #     symbol=target_symbol,
    #     order_type=OrderType.LIMIT,
    #     side=OrderSide.BUY,
    #     amount=0.0001,
    #     price=15000.0
    # )
    # if order_to_place:
    #     logger.info(f"Practice order placed/simulated: ID {order_to_place.id}, Status {order_to_place.status}")
    # else:
    #     logger.warning("Practice order placement failed or was not attempted.")

    await exchange_api.close()
</code>

kamikaze_komodo/portfolio_constructor/asset_allocator.py:
<code>
# kamikaze_komodo/portfolio_constructor/asset_allocator.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
import numpy as np
import pandas as pd
from kamikaze_komodo.core.models import BarData # Or other relevant models
from kamikaze_komodo.app_logger import get_logger

# For HRP
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from scipy.spatial.distance import squareform
import scipy.stats

logger = get_logger(__name__)

class BaseAssetAllocator(ABC):
    """
    Abstract base class for asset allocation strategies.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def allocate(
        self,
        assets: List[str], # List of available asset symbols
        portfolio_value: float, # Total current portfolio value
        historical_data: Optional[Dict[str, pd.DataFrame]] = None, # Dict of DataFrames {symbol: df_ohlcv}
        trade_history: Optional[pd.DataFrame] = None # For OptimalF
    ) -> Dict[str, float]: # Target allocation in terms of percentage
        """
        Determines the target allocation for assets.
        Args:
            assets (List[str]): List of asset symbols to consider for allocation.
            portfolio_value (float): Total capital available for allocation.
            historical_data (Optional[Dict[str, pd.DataFrame]]): Historical OHLCV data for assets.
            trade_history (Optional[pd.DataFrame]): For OptimalF, needs past trade performance.
        Returns:
            Dict[str, float]: Dictionary mapping asset symbols to target allocation percentage (0.0 to 1.0).
        """
        pass

class FixedWeightAssetAllocator(BaseAssetAllocator):
    """
    Allocates assets based on predefined fixed weights.
    """
    def __init__(self, target_weights: Dict[str, float], params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.target_weights = target_weights
        if not self.target_weights:
            logger.warning("FixedWeightAssetAllocator initialized with no target weights.")
        elif abs(sum(self.target_weights.values()) - 1.0) > 1e-6 and sum(self.target_weights.values()) != 0 : # Allow 0 for no allocation
            logger.warning(f"Sum of target weights ({sum(self.target_weights.values())}) is not 1.0. Allocations will be normalized or may behave unexpectedly.")

    def allocate(
        self,
        assets: List[str],
        portfolio_value: float,
        historical_data: Optional[Dict[str, pd.DataFrame]] = None,
        trade_history: Optional[pd.DataFrame] = None
    ) -> Dict[str, float]:
        allocation_targets_pct: Dict[str, float] = {}
        relevant_target_weights = {asset: self.target_weights.get(asset, 0.0) for asset in assets if asset in self.target_weights}
        total_weight_for_relevant_assets = sum(relevant_target_weights.values())

        if total_weight_for_relevant_assets == 0:
            logger.debug("No target weights specified for the given assets or total weight is zero. No allocation.")
            return {asset: 0.0 for asset in assets}

        for asset in assets:
            if asset in relevant_target_weights:
                normalized_weight = relevant_target_weights[asset] / total_weight_for_relevant_assets
                allocation_targets_pct[asset] = normalized_weight
            else:
                allocation_targets_pct[asset] = 0.0
        logger.debug(f"FixedWeightAssetAllocator target percentage allocation: {allocation_targets_pct}")
        return allocation_targets_pct

class OptimalFAllocator(BaseAssetAllocator):
    """
    Allocates capital based on Vince's Optimal f (Kelly Criterion variant).
    This is a placeholder and should be used with caution as estimating inputs is difficult.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.default_win_probability = float(self.params.get('optimalf_default_win_probability', 0.51))
        self.default_payoff_ratio = float(self.params.get('optimalf_default_payoff_ratio', 1.1))
        self.kelly_fraction = float(self.params.get('optimalf_kelly_fraction', 0.25))
        self.min_trades_for_stats = int(self.params.get('optimalf_min_trades_for_stats', 20))
        logger.info(f"OptimalFAllocator initialized. Default WinProb: {self.default_win_probability}, Default Payoff: {self.default_payoff_ratio}, Kelly Fraction: {self.kelly_fraction}, Min Trades: {self.min_trades_for_stats}")

    def allocate(
        self,
        assets: List[str],
        portfolio_value: float,
        historical_data: Optional[Dict[str, pd.DataFrame]] = None,
        trade_history: Optional[pd.DataFrame] = None
    ) -> Dict[str, float]:
        # This implementation is complex and context-specific.
        # For now, it will act as a simple fallback.
        logger.warning("OptimalFAllocator not fully implemented, returning equal weights.")
        num_assets = len(assets)
        if num_assets == 0:
            return {}
        equal_weight = 1.0 / num_assets
        return {asset: equal_weight for asset in assets}


class HRPAllocator(BaseAssetAllocator):
    """
    Allocates assets using Hierarchical Risk Parity (HRP) by De Prado.
    Requires historical returns data for assets.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.linkage_method = self.params.get('hrp_linkage_method', 'ward')
        logger.info(f"HRPAllocator initialized. Linkage method: {self.linkage_method}")

    def _get_cluster_var(self, cov_matrix: pd.DataFrame, cluster_items: List[int]) -> float:
        cluster_cov_matrix = cov_matrix.iloc[cluster_items, cluster_items]
        parity_w = 1.0 / np.diag(cluster_cov_matrix)
        parity_w = parity_w / parity_w.sum()
        cluster_var = np.dot(parity_w, np.dot(cluster_cov_matrix, parity_w))
        return cluster_var

    def _get_recursive_bisection(self, sort_ix: List[int], current_weights: np.ndarray, cov_matrix: pd.DataFrame) -> np.ndarray:
        if len(sort_ix) == 1:
            return current_weights

        mid_point = len(sort_ix) // 2
        cluster1_items = sort_ix[:mid_point]
        cluster2_items = sort_ix[mid_point:]

        cluster1_var = self._get_cluster_var(cov_matrix, cluster1_items)
        cluster2_var = self._get_cluster_var(cov_matrix, cluster2_items)

        # Handle potential division by zero if a cluster has zero variance
        total_cluster_var = cluster1_var + cluster2_var
        alpha = cluster2_var / total_cluster_var if total_cluster_var != 0 else 0.5

        for i in cluster1_items:
            current_weights[i] *= alpha
        for i in cluster2_items:
            current_weights[i] *= (1 - alpha)

        current_weights = self._get_recursive_bisection(cluster1_items, current_weights, cov_matrix)
        current_weights = self._get_recursive_bisection(cluster2_items, current_weights, cov_matrix)

        return current_weights

    def allocate(
        self,
        assets: List[str],
        portfolio_value: float,
        historical_data: Optional[Dict[str, pd.DataFrame]] = None,
        trade_history: Optional[pd.DataFrame] = None
    ) -> Dict[str, float]:

        num_assets = len(assets)
        if not historical_data or num_assets == 0:
            return {}

        # --- FIX for HRPAllocator Fallback ---
        if num_assets < 2:
            logger.warning("HRPAllocator requires at least 2 assets for allocation. Falling back to equal weight for the single asset.")
            return {assets[0]: 1.0} if num_assets == 1 else {}

        returns_data = {}
        for asset in assets:
            if asset in historical_data and not historical_data[asset].empty:
                returns_data[asset] = historical_data[asset]['close'].pct_change().dropna()
            else:
                logger.warning(f"No historical 'close' data for asset {asset} in HRPAllocator. Cannot perform allocation.")
                return {ast: 1.0 / num_assets for ast in assets}

        returns_df = pd.DataFrame(returns_data).dropna()
        if returns_df.shape[0] < 2 or returns_df.shape[1] < 2:
            logger.warning(f"Not enough processed return data for HRP. Shape: {returns_df.shape}. Falling back to equal weight.")
            return {asset: 1.0 / num_assets for asset in assets}

        try:
            cov_matrix = returns_df.cov()
            corr_matrix = returns_df.corr()

            dist_matrix = np.sqrt(0.5 * (1 - corr_matrix))
            condensed_dist_matrix = squareform(dist_matrix)
            link = linkage(condensed_dist_matrix, method=self.linkage_method)

            sort_ix = dendrogram(link, no_plot=True)['leaves']

            initial_weights = np.ones(num_assets)
            hrp_weights_array = self._get_recursive_bisection(sort_ix, initial_weights, cov_matrix)

            hrp_weights = pd.Series(hrp_weights_array, index=[assets[i] for i in sort_ix])
            hrp_weights = hrp_weights / hrp_weights.sum()
            hrp_weights = hrp_weights.reindex(assets).fillna(0.0)

            allocations = hrp_weights.to_dict()
            logger.info(f"HRP Allocator target percentage allocation: { {k: f'{v*100:.2f}%' for k, v in allocations.items()} }")
            return allocations
        except Exception as e:
            logger.error(f"Error during HRP calculation: {e}. Falling back to equal weight.", exc_info=True)
            return {asset: 1.0 / num_assets for asset in assets}
</code>

kamikaze_komodo/portfolio_constructor/__init__.py:
<code>
# kamikaze_komodo/portfolio_constructor/__init__.py
# This file makes the 'portfolio_constructor' directory a Python package.
</code>

kamikaze_komodo/portfolio_constructor/rebalancer.py:
<code>
# kamikaze_komodo/portfolio_constructor/rebalancer.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from kamikaze_komodo.core.models import PortfolioSnapshot
from kamikaze_komodo.core.enums import OrderSide, OrderType
from kamikaze_komodo.app_logger import get_logger
import pandas as pd

logger = get_logger(__name__)

class BaseRebalancer(ABC):
    """
    Abstract base class for portfolio rebalancing logic.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def generate_rebalancing_orders(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float],
        asset_prices: Dict[str, float]
    ) -> List[Dict[str, Any]]:
        """
        Generates orders needed to rebalance the portfolio to target allocations.
        """
        pass

class BasicRebalancer(BaseRebalancer):
    """
    Rebalances the portfolio based on a target allocation dictionary.
    If an asset with a current position is NOT in the target dictionary, it is held.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.min_order_value_usd = float(self.params.get('rebalancer_min_order_value_usd', 10.0))
        logger.info(f"BasicRebalancer initialized with Min Order Value: ${self.min_order_value_usd}")

    def generate_rebalancing_orders(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float],
        asset_prices: Dict[str, float]
    ) -> List[Dict[str, Any]]:
        orders: List[Dict[str, Any]] = []
        if current_portfolio.total_value_usd <= 0:
            logger.warning("Portfolio total value is zero or negative. Cannot generate rebalancing orders.")
            return orders

        # Determine the target value in USD for each asset based on the total portfolio equity
        target_asset_values: Dict[str, float] = {
            asset: current_portfolio.total_value_usd * target_pct
            for asset, target_pct in target_allocations_pct.items()
        }

        all_assets_in_scope = set(current_portfolio.positions.keys()).union(set(target_allocations_pct.keys()))

        for asset in all_assets_in_scope:
            price = asset_prices.get(asset)
            if price is None or price <= 0:
                logger.warning(f"Cannot generate order for {asset}: price is missing or invalid ({price}).")
                continue

            current_quantity = current_portfolio.positions.get(asset, 0.0)
            
            # If an asset is not in the target dictionary, its target quantity is its current quantity.
            # This correctly implements the "HOLD" signal.
            target_value = target_asset_values.get(asset, current_quantity * price)
            target_quantity = target_value / price

            quantity_diff = target_quantity - current_quantity
            value_of_trade = abs(quantity_diff * price)

            if value_of_trade < self.min_order_value_usd:
                continue

            if abs(quantity_diff) > 1e-9: # Use a small epsilon to avoid floating point noise
                order_side = OrderSide.BUY if quantity_diff > 0 else OrderSide.SELL
                orders.append({
                    'symbol': asset,
                    'type': OrderType.MARKET,
                    'side': order_side,
                    'amount': abs(quantity_diff)
                })
                logger.info(f"Generated rebalancing order for {asset}: {order_side.value} {abs(quantity_diff):.6f} units. Target Value: ${target_value:.2f}, Current Value: ${current_quantity * price:.2f}")

        return orders
</code>

kamikaze_komodo/backtesting_engine/performance_analyzer.py:
<code>
# FILE: kamikaze_komodo/backtesting_engine/performance_analyzer.py
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from kamikaze_komodo.core.models import Trade
from kamikaze_komodo.core.enums import OrderSide, TradeResult
from kamikaze_komodo.app_logger import get_logger
from collections import deque


logger = get_logger(__name__)

class PerformanceAnalyzer:
    def __init__(
        self,
        trades: List[Trade], # Expects a list of COMPLETED trades
        initial_capital: float,
        final_capital: float,
        equity_curve_df: Optional[pd.DataFrame] = None, # Timestamp-indexed 'total_value'
        risk_free_rate_annual: float = 0.02, # Annual risk-free rate (e.g., 2%)
        annualization_factor: int = 252 # Trading days in a year for Sharpe/Sortino
    ):
        if not trades:
            logger.warning("PerformanceAnalyzer initialized with no completed trades. Some metrics might be zero or NaN.")
        
        self.trades_df = pd.DataFrame([trade.model_dump() for trade in trades])
        if not self.trades_df.empty:
            self.trades_df['entry_timestamp'] = pd.to_datetime(self.trades_df['entry_timestamp'])
            self.trades_df['exit_timestamp'] = pd.to_datetime(self.trades_df['exit_timestamp'])
    
        self.initial_capital = initial_capital
        self.final_capital = final_capital
        self.equity_curve_df = equity_curve_df
        self.risk_free_rate_annual = risk_free_rate_annual
        self.annualization_factor = annualization_factor
        
        logger.info(f"PerformanceAnalyzer initialized. Completed Trades: {len(trades)}, Initial: ${initial_capital:,.2f}, Final: ${final_capital:,.2f}")
        logger.info(f"Using Annual Risk-Free Rate: {self.risk_free_rate_annual*100:.2f}%, Annualization Factor: {self.annualization_factor}")

    def _calculate_periodic_returns(self) -> Optional[pd.Series]:
        if self.equity_curve_df is None or self.equity_curve_df.empty or 'total_value' not in self.equity_curve_df.columns:
            logger.warning("Equity curve data is missing or invalid. Cannot calculate periodic returns for Sharpe/Sortino.")
            return None
        # Resample to daily returns for annualization, handling potential non-unique index if multiple records per day
        daily_equity = self.equity_curve_df['total_value'].resample('D').last().ffill()
        periodic_returns = daily_equity.pct_change().dropna()
        return periodic_returns

    def calculate_metrics(self) -> Dict[str, Any]:
        metrics: Dict[str, Any] = {
            "initial_capital": self.initial_capital,
            "final_capital": self.final_capital,
            "total_net_profit": 0.0,
            "total_return_pct": 0.0,
            "total_trades": 0,
            "winning_trades": 0,
            "losing_trades": 0,
            "breakeven_trades": 0,
            "win_rate_pct": 0.0,
            "loss_rate_pct": 0.0,
            "average_pnl_per_trade": 0.0,
            "average_win_pnl": 0.0,
            "average_loss_pnl": 0.0,
            "profit_factor": np.nan,
            "max_drawdown_pct": 0.0,
            "sharpe_ratio": np.nan,
            "sortino_ratio": np.nan,
            "calmar_ratio": np.nan,
            "total_fees_paid": 0.0,
            "average_holding_period_hours": 0.0,
            "longest_win_streak": 0,
            "longest_loss_streak": 0,
        }

        # Calculate metrics from the equity curve first, as they don't depend on trades
        metrics["total_net_profit"] = self.final_capital - self.initial_capital
        if self.initial_capital > 0:
            metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
        
        # Max Drawdown (from equity curve)
        if self.equity_curve_df is not None and not self.equity_curve_df.empty and 'total_value' in self.equity_curve_df.columns:
            # FIX: Clip equity at 0 to prevent drawdowns > 100%
            equity_values = self.equity_curve_df['total_value'].clip(lower=0)
            if len(equity_values) > 1:
                peak = equity_values.expanding(min_periods=1).max()
                # Ensure peak is not zero to avoid division by zero if equity starts at 0
                peak_safe = peak.replace(0, np.nan)
                drawdown = (equity_values - peak_safe) / peak_safe
                metrics["max_drawdown_pct"] = abs(drawdown.min()) * 100 if pd.notna(drawdown.min()) else 0.0

        if self.trades_df.empty:
            logger.warning("No completed trades to analyze. Returning portfolio-level metrics only.")
            return metrics

        pnl_series = self.trades_df['pnl'].dropna()
        if pnl_series.empty:
            logger.warning("No PnL data in trades to analyze. Returning portfolio-level metrics only.")
            return metrics
        
        # --- Trade-based metrics ---
        metrics["total_trades"] = len(pnl_series)
        wins = pnl_series[pnl_series > 0]
        losses = pnl_series[pnl_series < 0]
        breakevens = pnl_series[pnl_series == 0]
        metrics["winning_trades"] = len(wins)
        metrics["losing_trades"] = len(losses)
        metrics["breakeven_trades"] = len(breakevens)

        if metrics["total_trades"] > 0:
            metrics["win_rate_pct"] = (metrics["winning_trades"] / metrics["total_trades"]) * 100
            metrics["loss_rate_pct"] = (metrics["losing_trades"] / metrics["total_trades"]) * 100
            metrics["average_pnl_per_trade"] = pnl_series.mean()
        if not wins.empty: metrics["average_win_pnl"] = wins.mean()
        if not losses.empty: metrics["average_loss_pnl"] = losses.mean()

        gross_profit = wins.sum()
        gross_loss = abs(losses.sum())
        if gross_loss > 0: metrics["profit_factor"] = gross_profit / gross_loss
        elif gross_profit > 0: metrics["profit_factor"] = np.inf
    
        metrics["total_fees_paid"] = self.trades_df['commission'].sum()

        # --- Portfolio Ratios ---
        periodic_returns = self._calculate_periodic_returns()
        if periodic_returns is not None and len(periodic_returns) > 1:
            risk_free_rate_periodic = self.risk_free_rate_annual / self.annualization_factor
            excess_returns = periodic_returns - risk_free_rate_periodic
        
            # Sharpe Ratio
            sharpe_avg_excess_return = excess_returns.mean()
            sharpe_std_excess_return = excess_returns.std()
            if sharpe_std_excess_return is not None and sharpe_std_excess_return > 1e-9:
                metrics["sharpe_ratio"] = (sharpe_avg_excess_return / sharpe_std_excess_return) * np.sqrt(self.annualization_factor)
        
            # Sortino Ratio
            downside_returns = excess_returns[excess_returns < 0]
            if not downside_returns.empty:
                downside_deviation = downside_returns.std()
                if downside_deviation is not None and downside_deviation > 1e-9:
                    metrics["sortino_ratio"] = (sharpe_avg_excess_return / downside_deviation) * np.sqrt(self.annualization_factor)
    
        # Calmar Ratio
        if metrics["max_drawdown_pct"] is not None and metrics["max_drawdown_pct"] > 0:
            if self.equity_curve_df is not None and not self.equity_curve_df.empty:
                start_date = self.equity_curve_df.index.min()
                end_date = self.equity_curve_df.index.max()
                duration_years = (end_date - start_date).days / 365.25
                if duration_years > 0:
                    total_return = (self.final_capital / self.initial_capital) - 1 if self.initial_capital > 0 else 0
                    annualized_return = ((1 + total_return) ** (1 / duration_years)) - 1
                    metrics["calmar_ratio"] = (annualized_return * 100) / metrics["max_drawdown_pct"]

        # Average Holding Period
        if not self.trades_df.empty and 'exit_timestamp' in self.trades_df.columns and 'entry_timestamp' in self.trades_df.columns:
            valid_trades_for_duration = self.trades_df.dropna(subset=['entry_timestamp', 'exit_timestamp'])
            if not valid_trades_for_duration.empty:
                holding_periods = (valid_trades_for_duration['exit_timestamp'] - valid_trades_for_duration['entry_timestamp'])
                metrics["average_holding_period_hours"] = holding_periods.mean().total_seconds() / 3600 if not holding_periods.empty else 0.0

        # Win/Loss Streaks
        if not pnl_series.empty:
            win_streak, loss_streak = 0, 0
            current_win_streak, current_loss_streak = 0, 0
            for pnl_val in pnl_series:
                if pnl_val > 0:
                    current_win_streak += 1; current_loss_streak = 0
                elif pnl_val < 0:
                    current_loss_streak += 1; current_win_streak = 0
                else:
                    current_win_streak = 0; current_loss_streak = 0
                win_streak = max(win_streak, current_win_streak)
                loss_streak = max(loss_streak, current_loss_streak)
            metrics["longest_win_streak"] = win_streak
            metrics["longest_loss_streak"] = loss_streak

        return metrics

    def print_summary(self, metrics: Optional[Dict[str, Any]] = None):
        if metrics is None:
            metrics = self.calculate_metrics()

        summary = f"""
        --------------------------------------------------
        |              Backtest Performance Summary              |
        --------------------------------------------------
        | Metric                       | Value                 |
        --------------------------------------------------
        | Initial Capital              | ${metrics.get("initial_capital", 0):<15,.2f} |
        | Final Capital                | ${metrics.get("final_capital", 0):<15,.2f} |
        | Total Net Profit             | ${metrics.get("total_net_profit", 0):<15,.2f} |
        | Total Return                 | {metrics.get("total_return_pct", 0):<15.2f}% |
        | Total Trades                 | {metrics.get("total_trades", 0):<16} |
        | Winning Trades               | {metrics.get("winning_trades", 0):<16} |
        | Losing Trades                | {metrics.get("losing_trades", 0):<16} |
        | Breakeven Trades             | {metrics.get("breakeven_trades", 0):<16} |
        | Win Rate                     | {metrics.get("win_rate_pct", 0):<15.2f}% |
        | Loss Rate                    | {metrics.get("loss_rate_pct", 0):<15.2f}% |
        | Average PnL per Trade        | ${metrics.get("average_pnl_per_trade", 0):<15,.2f} |
        | Average Win PnL              | ${metrics.get("average_win_pnl", 0):<15,.2f} |
        | Average Loss PnL             | ${metrics.get("average_loss_pnl", 0):<15,.2f} |
        | Profit Factor                | {metrics.get("profit_factor", float('nan')):<16.2f} |
        | Max Drawdown                 | {metrics.get("max_drawdown_pct", 0):<15.2f}% |
        | Sharpe Ratio                 | {metrics.get("sharpe_ratio", float('nan')):<16.2f} |
        | Sortino Ratio                | {metrics.get("sortino_ratio", float('nan')):<16.2f} |
        | Calmar Ratio                 | {metrics.get("calmar_ratio", float('nan')):<16.2f} |
        | Avg Holding Period (hours)   | {metrics.get("average_holding_period_hours", 0):<16.2f} |
        | Longest Win Streak           | {metrics.get("longest_win_streak", 0):<16} |
        | Longest Loss Streak          | {metrics.get("longest_loss_streak", 0):<16} |
        | Total Fees Paid              | ${metrics.get("total_fees_paid", 0):<15,.2f} |
        --------------------------------------------------
        """
        print(summary)
        logger.info("Performance summary generated." + summary.replace("\n        |", "\n"))
</code>

kamikaze_komodo/backtesting_engine/__init__.py:
<code>
# kamikaze_komodo/backtesting_engine/__init__.py
# This file makes the 'backtesting_engine' directory a Python package.
</code>

kamikaze_komodo/backtesting_engine/optimizer.py:
<code>
# kamikaze_komodo/backtesting_engine/optimizer.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Callable, Tuple, Optional
import itertools
import optuna # For more advanced optimization like TPE
import asyncio

from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.orchestration.portfolio_manager import PortfolioManager
from kamikaze_komodo.backtesting_engine.engine import BacktestingEngine
from kamikaze_komodo.backtesting_engine.performance_analyzer import PerformanceAnalyzer
from kamikaze_komodo.config.settings import settings as app_settings
from kamikaze_komodo.portfolio_constructor.asset_allocator import FixedWeightAssetAllocator
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager, ATRStopManager, PercentageStopManager, TripleBarrierStopManager
from kamikaze_komodo.risk_control_module.volatility_band_stop_manager import VolatilityBandStopManager
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class StrategyOptimizer:
    """
    Optimizes strategy parameters using various methods like Grid Search or Optuna.
    Supports walk-forward optimization.
    """
    def __init__(
        self,
        strategy_class: type,
        data_feed_df: pd.DataFrame,
        param_grid: Dict[str, List[Any]],
        optimization_metric: str = 'total_net_profit',
        initial_capital: float = 10000.0,
        commission_bps: float = 0.0,
        slippage_bps: float = 0.0,
        position_sizer_class_name: Optional[str] = None, # Note: This is now legacy. Sizing is handled by AssetAllocator.
        position_sizer_params: Optional[Dict[str, Any]] = None,
        stop_manager_class_name: Optional[str] = None,
        stop_manager_params: Optional[Dict[str, Any]] = None,
        sentiment_data_df: Optional[pd.DataFrame] = None,
        symbol: Optional[str] = None,
        timeframe: Optional[str] = None
    ):
        self.strategy_class = strategy_class
        self.data_feed_df = data_feed_df
        self.param_grid = param_grid
        self.optimization_metric = optimization_metric
        self.initial_capital = initial_capital
        self.commission_bps = commission_bps
        self.slippage_bps = slippage_bps

        # Legacy parameter, will be ignored by the new engine structure.
        self.position_sizer_class_name = position_sizer_class_name
        if self.position_sizer_class_name:
            logger.warning(f"position_sizer_class_name ('{position_sizer_class_name}') is a legacy parameter and will not be used in the unified backtesting engine.")

        self.position_sizer_params = position_sizer_params if position_sizer_params else {}
        self.stop_manager_class_name = stop_manager_class_name
        self.stop_manager_params = stop_manager_params if stop_manager_params else {}

        self.sentiment_data_df = sentiment_data_df
        self.symbol = symbol if symbol else (app_settings.default_symbol if app_settings else "OPTIMIZE_SYMBOL")
        self.timeframe = timeframe if timeframe else (app_settings.default_timeframe if app_settings else "OPTIMIZE_TF")

        logger.info(f"StrategyOptimizer initialized for {strategy_class.__name__} on {self.symbol} ({self.timeframe}). Optimizing for: {self.optimization_metric}")


    async def _run_backtest_for_params(self, params_set: Dict[str, Any], current_data_feed: pd.DataFrame) -> float:
        """Runs a single backtest for a given set of parameters."""
        try:
            # 1. Create the single strategy instance with the current parameter set
            strategy_instance = self.strategy_class(
                symbol=self.symbol,
                timeframe=self.timeframe,
                params=params_set
            )

            # 2. For single-asset optimization, asset allocation is simply 100% to the asset if a signal exists.
            # We use a FixedWeightAssetAllocator to achieve this. The PortfolioManager will use it.
            asset_allocator = FixedWeightAssetAllocator(target_weights={self.symbol: 1.0})

            # 3. Create a PortfolioManager configured for this single strategy run.
            portfolio_manager = PortfolioManager(
                trading_universe=[self.symbol],
                strategy_instances=[strategy_instance],
                asset_allocator=asset_allocator,
            )

            # 4. Create the stop manager for this run, using parameters from the optimization set.
            stop_manager_instance = None
            if self.stop_manager_class_name:
                stop_module_map = {
                    "ATRStopManager": ATRStopManager,
                    "PercentageStopManager": PercentageStopManager,
                    "TripleBarrierStopManager": TripleBarrierStopManager,
                    "VolatilityBandStopManager": VolatilityBandStopManager,
                }
                StopManagerClass = stop_module_map.get(self.stop_manager_class_name)
                if StopManagerClass:
                    stop_manager_instance = StopManagerClass(params=params_set)
                else:
                    logger.error(f"Could not find stop manager class: {self.stop_manager_class_name}")


            # 5. The data feed for the engine needs to be a dictionary
            data_feeds = {self.symbol: current_data_feed}

            # 6. Instantiate the BacktestingEngine with the specific PM, data, and stop manager
            engine = BacktestingEngine(
                portfolio_manager=portfolio_manager,
                data_feeds=data_feeds,
                initial_capital=self.initial_capital,
                commission_bps=self.commission_bps,
                slippage_bps=self.slippage_bps,
                stop_manager=stop_manager_instance,
            )

            trades_log, final_portfolio, equity_curve = await engine.run()

            # 7. Calculate performance metrics
            risk_free_rate = float(app_settings.config.get('BacktestingPerformance', 'RiskFreeRateAnnual', fallback=0.02)) if app_settings else 0.02
            annual_factor = int(app_settings.config.get('BacktestingPerformance', 'AnnualizationFactor', fallback=252)) if app_settings else 252

            analyzer = PerformanceAnalyzer(
                trades=trades_log,
                initial_capital=self.initial_capital,
                final_capital=final_portfolio['final_portfolio_value'],
                equity_curve_df=equity_curve,
                risk_free_rate_annual=risk_free_rate,
                annualization_factor=annual_factor
            )
            metrics = analyzer.calculate_metrics()
            metric_value = metrics.get(self.optimization_metric, -float('inf'))

            if pd.isna(metric_value):
                metric_value = -float('inf') if self.optimization_metric != 'max_drawdown_pct' else float('inf')

            return float(metric_value)

        except Exception as e:
            logger.error(f"Error during backtest for params {params_set}: {e}", exc_info=True)
            return -float('inf') if self.optimization_metric != 'max_drawdown_pct' else float('inf')


    async def grid_search(self) -> Tuple[Optional[Dict[str, Any]], float, pd.DataFrame]:
        param_names = list(self.param_grid.keys())
        param_value_combinations = list(itertools.product(*self.param_grid.values()))

        results = []
        best_metric = -float('inf')
        if self.optimization_metric == 'max_drawdown_pct':
            best_metric = float('inf')

        best_params = None
        logger.info(f"Starting Grid Search with {len(param_value_combinations)} combinations.")

        for i, combo in enumerate(param_value_combinations):
            current_params = dict(zip(param_names, combo))
            logger.debug(f"Grid Search - Combo {i+1}/{len(param_value_combinations)}: {current_params}")
            metric_value = await self._run_backtest_for_params(current_params, self.data_feed_df)
            results.append({**current_params, 'metric_value': metric_value})

            if self.optimization_metric == 'max_drawdown_pct':
                if metric_value < best_metric:
                    best_metric = metric_value
                    best_params = current_params
            elif metric_value > best_metric:
                best_metric = metric_value
                best_params = current_params

        results_df = pd.DataFrame(results)
        if best_params:
            logger.info(f"Grid Search completed. Best params: {best_params}, Best {self.optimization_metric}: {best_metric:.4f}")
        else:
            logger.warning("Grid Search completed but no best parameters found (all trials might have failed or yielded non-comparable results).")
        return best_params, best_metric, results_df.sort_values(by='metric_value', ascending=(self.optimization_metric == 'max_drawdown_pct'))


    def optuna_optimize(self, n_trials: int = 100, study_name: Optional[str] = None, storage_url: Optional[str] = None) -> Tuple[Optional[Dict[str, Any]], float, optuna.study.Study]:
        if not study_name:
            study_name = f"{self.strategy_class.__name__}_{self.symbol.replace('/', '')}_{self.timeframe}_Optimization"

        direction = 'minimize' if self.optimization_metric == 'max_drawdown_pct' else 'maximize'
        study = optuna.create_study(study_name=study_name, storage=storage_url, load_if_exists=True, direction=direction)

        # Optuna's objective function is synchronous. We need to run our async backtest from within it.
        def objective(trial: optuna.trial.Trial) -> float:
            params_set = {}
            for param_name, values in self.param_grid.items():
                if not values:
                    logger.warning(f"Parameter '{param_name}' in grid has empty values. Skipping for Optuna.")
                    continue
                if isinstance(values[0], bool):
                    params_set[param_name] = trial.suggest_categorical(param_name, [True, False])
                elif isinstance(values[0], int) and len(values) >= 2:
                    step = values[2] if len(values) > 2 else 1
                    params_set[param_name] = trial.suggest_int(param_name, values[0], values[1], step=step)
                elif isinstance(values[0], float) and len(values) >= 2:
                    params_set[param_name] = trial.suggest_float(param_name, values[0], values[1])
                elif isinstance(values, list):
                    params_set[param_name] = trial.suggest_categorical(param_name, values)
                else:
                    logger.warning(f"Parameter '{param_name}' in grid has unsupported format for Optuna. Values: {values}. Skipping.")

            # Get the running asyncio event loop and run the async backtest function
            try:
                loop = asyncio.get_running_loop()
                metric_value = loop.run_until_complete(self._run_backtest_for_params(params_set, self.data_feed_df))
            except RuntimeError: # If no loop is running (e.g., script is purely sync)
                metric_value = asyncio.run(self._run_backtest_for_params(params_set, self.data_feed_df))

            return metric_value

        logger.info(f"Starting Optuna optimization with {n_trials} trials. Optimizing for {self.optimization_metric} ({direction}).")
        study.optimize(objective, n_trials=n_trials, timeout=None)

        best_params = None
        best_metric_value = study.best_value if study.best_trial else (-float('inf') if direction == 'maximize' else float('inf'))
        if study.best_trial:
            best_params = study.best_params
            logger.info(f"Optuna optimization completed. Best params: {best_params}, Best {self.optimization_metric}: {best_metric_value:.4f}")
        else:
            logger.warning("Optuna optimization completed but no best trial found (all trials might have failed or yielded non-comparable results).")
        return best_params, best_metric_value, study
</code>

kamikaze_komodo/backtesting_engine/engine.py:
<code>
# FILE: kamikaze_komodo/backtesting_engine/engine.py
import pandas as pd
import pandas_ta as ta
from typing import List, Dict, Any, Optional
from collections import deque
import numpy as np
import asyncio
import os

from kamikaze_komodo.core.models import BarData, Trade, Order, PortfolioSnapshot
from kamikaze_komodo.core.enums import SignalType, OrderSide, TradeResult, OrderType
from kamikaze_komodo.app_logger import get_logger, logger as root_logger
from datetime import datetime, timezone, timedelta
from kamikaze_komodo.orchestration.portfolio_manager import PortfolioManager
from kamikaze_komodo.config.settings import settings
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager, ATRStopManager, PercentageStopManager, TripleBarrierStopManager
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher

logger = get_logger(__name__)

STOP_MANAGER_CLASS_MAP = {
    "ATRStopManager": ATRStopManager,
    "PercentageStopManager": PercentageStopManager,
    "TripleBarrierStopManager": TripleBarrierStopManager,
    "TripleBarrier": TripleBarrierStopManager,
}

class SimulatedExchangeAPI:
    def __init__(self, engine: 'BacktestingEngine'):
        self._engine = engine

    async def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount: float, price: Optional[float] = None, params: Optional[Dict] = None) -> Order:
        return self._engine._execute_order(symbol, side, amount, type, price)

    async def close(self): pass

class BacktestingEngine:
    def __init__(
        self,
        portfolio_manager: PortfolioManager,
        data_feeds: Dict[str, pd.DataFrame],
        initial_capital: float = 10000.0,
        commission_bps: float = 0.0,
        slippage_bps: float = 0.0,
        stop_manager: Optional[BaseStopManager] = None,
    ):
        if not data_feeds: raise ValueError("data_feeds dictionary cannot be empty.")
        
        self.data_feeds = {symbol: df.sort_index() for symbol, df in data_feeds.items()}
        for asset, df in self.data_feeds.items():
            if all(col in df.columns for col in ['high', 'low', 'close']):
                df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=14)

        self.initial_capital = initial_capital
        self.commission_rate = commission_bps / 10000.0
        self.slippage_rate = slippage_bps / 10000.0
        
        self.simulated_exchange = SimulatedExchangeAPI(self)
        self.portfolio_manager = portfolio_manager
        self.portfolio_manager.exchange_api = self.simulated_exchange

        if stop_manager:
            self.stop_manager = stop_manager
        else:
            self.stop_manager = self._initialize_stop_manager()

        self.current_cash = initial_capital
        self.current_portfolio_value = initial_capital
        self.open_positions: Dict[str, Trade] = {}
        self.completed_trades_log: List[Trade] = []
        self.portfolio_history: List[Dict[str, Any]] = []
        self.order_id_counter = 0
        
        logger.info(f"BacktestingEngine initialized. Stop Manager: {self.stop_manager.__class__.__name__ if self.stop_manager else 'None'}")

    def _initialize_stop_manager(self) -> BaseStopManager:
        stop_manager_name = settings.config.get('RiskManagement', 'StopManager_Default', fallback='PercentageStopManager')
        stop_manager_class = STOP_MANAGER_CLASS_MAP.get(stop_manager_name)
        if not stop_manager_class:
            logger.error(f"StopManager '{stop_manager_name}' not found. Defaulting to PercentageStopManager.")
            stop_manager_class = PercentageStopManager
        
        params = settings.get_strategy_params(stop_manager_name) or settings.get_strategy_params('RiskManagement')
        return stop_manager_class(params=params)

    @classmethod
    async def create(cls, portfolio_manager: PortfolioManager, initial_capital: float, commission_bps: float, slippage_bps: float) -> 'BacktestingEngine':
        root_logger.info("--- BacktestingEngine: Loading Data Feeds via create() ---")
        if not settings: raise ValueError("Settings cannot be loaded.")
        data_feeds = await cls._load_data_feeds_async()
        return cls(portfolio_manager, data_feeds, initial_capital, commission_bps, slippage_bps)

    @staticmethod
    async def _load_data_feeds_async() -> Dict[str, pd.DataFrame]:
        data_feeds = {}
        portfolio_config = settings.get_strategy_params('Portfolio')
        trading_universe: List[str] = [s.strip() for s in portfolio_config.get('tradinguniverse', '').split(',')]
        timeframe = settings.default_timeframe
        
        if not trading_universe or trading_universe == ['']: return {}

        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        start_date = datetime.now(timezone.utc) - timedelta(days=settings.historical_data_days)
        
        for asset in trading_universe:
            logger.info(f"Loading data for {asset}...")
            bars = db_manager.retrieve_bar_data(asset, timeframe, start_date=start_date)
            if not bars or len(bars) < 200:
                logger.info(f"Fetching fresh data for {asset}...")
                bars = await data_fetcher.fetch_historical_data_for_period(asset, timeframe, start_date)
                if bars: db_manager.store_bar_data(bars)
            
            if bars:
                df = pd.DataFrame([bar.model_dump() for bar in bars])
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df.set_index('timestamp', inplace=True)
                data_feeds[asset] = df
                logger.info(f"Loaded {len(df)} bars for {asset}.")
            else:
                logger.error(f"Could not load data for asset {asset}. It will be excluded from the backtest.")
        
        await data_fetcher.close()
        db_manager.close()
        
        if not data_feeds:
            raise ValueError("No data could be loaded for any asset in the universe. Aborting backtest.")
        return data_feeds

    def _get_next_order_id(self) -> str:
        self.order_id_counter += 1
        return f"order_{self.order_id_counter:05d}"

    def _apply_slippage(self, price: float, side: OrderSide) -> float:
        if self.slippage_rate == 0.0: return price
        return price * (1 + self.slippage_rate) if side == OrderSide.BUY else price * (1 - self.slippage_rate)

    def _execute_order(self, symbol: str, side: OrderSide, amount: float, order_type: OrderType, price: Optional[float] = None, from_stop: bool = False) -> Order:
        if amount <= 1e-9: return
        
        current_bar = self.current_bars[symbol]
        execution_price = self._apply_slippage(price if price is not None else current_bar.close, side)
        order_id = self._get_next_order_id()
        commission = (amount * execution_price) * self.commission_rate

        position = self.open_positions.get(symbol)
        
        if position and position.side != side:
            # Closing or reversing position
            close_amount = min(amount, position.amount)
            pnl = (execution_price - position.entry_price) * close_amount if position.side == OrderSide.BUY else (position.entry_price - execution_price) * close_amount
            self.current_cash += pnl - commission

            position.amount -= close_amount
            if position.amount < 1e-9:
                closed_trade = self.open_positions.pop(symbol)
                closed_trade.exit_price = execution_price
                closed_trade.exit_timestamp = current_bar.timestamp
                closed_trade.exit_order_id = order_id
                closed_trade.pnl = pnl - closed_trade.commission
                self.completed_trades_log.append(closed_trade)

            if amount > close_amount: # Reversal
                new_amount = amount - close_amount
                trade = Trade(
                    id=order_id, entry_order_id=order_id, symbol=symbol, side=side, amount=new_amount,
                    entry_price=execution_price, entry_timestamp=current_bar.timestamp, commission=0,
                    custom_fields={"atr_at_entry": self.data_feeds[symbol].loc[current_bar.timestamp].get('atr')}
                )
                self.open_positions[symbol] = trade
        else:
            # Opening or increasing position
            new_amount = amount + (position.amount if position else 0)
            new_entry_price = ((position.entry_price * position.amount) + (execution_price * amount)) / new_amount if position else execution_price
            
            if position:
                position.entry_price = new_entry_price
                position.amount = new_amount
                position.commission += commission
            else:
                trade = Trade(
                    id=order_id, entry_order_id=order_id, symbol=symbol, side=side, amount=amount,
                    entry_price=execution_price, entry_timestamp=current_bar.timestamp, commission=commission,
                    custom_fields={"atr_at_entry": self.data_feeds[symbol].loc[current_bar.timestamp].get('atr')}
                )
                self.open_positions[symbol] = trade
        
        return Order(
            id=order_id, symbol=symbol, type=order_type, side=side, amount=amount, price=execution_price,
            timestamp=current_bar.timestamp, status='filled'
        )
        
    def _update_portfolio_history(self, timestamp: datetime):
        open_pnl = 0
        positions_value = 0
        for symbol, position in self.open_positions.items():
            current_price = self.current_bars.get(symbol, position).close
            positions_value += position.amount * current_price
            pnl = (current_price - position.entry_price) * position.amount if position.side == OrderSide.BUY else (position.entry_price - current_price) * position.amount
            open_pnl += pnl
        
        self.current_portfolio_value = self.current_cash + positions_value
        self.portfolio_history.append({"timestamp": timestamp, "cash": self.current_cash, "unrealized_pnl": open_pnl, "total_value": self.current_portfolio_value})
        positions_summary = {s: p.amount if p.side == OrderSide.BUY else -p.amount for s, p in self.open_positions.items()}
        self.portfolio_manager.portfolio_snapshot = PortfolioSnapshot(timestamp=timestamp, total_value_usd=self.current_portfolio_value, cash_balance_usd=self.current_cash, positions=positions_summary)
    
    def _check_stops(self, timestamp: datetime):
        if not self.stop_manager: return
        
        trades_to_close = []
        for symbol, trade in list(self.open_positions.items()):
            current_bar = self.current_bars.get(symbol)
            if not current_bar: continue
            
            stop_price = self.stop_manager.check_stop_loss(trade, current_bar, self.current_bar_index)
            if stop_price:
                trades_to_close.append({'trade': trade, 'price': stop_price})
                continue
            
            tp_price = self.stop_manager.check_take_profit(trade, current_bar)
            if tp_price:
                trades_to_close.append({'trade': trade, 'price': tp_price})

        for item in trades_to_close:
            trade = item['trade']
            price = item['price']
            logger.info(f"STOP TRIGGERED: Closing trade {trade.id} on {trade.symbol}.")
            closing_side = OrderSide.SELL if trade.side == OrderSide.BUY else OrderSide.BUY
            self._execute_order(symbol=trade.symbol, side=closing_side, amount=trade.amount, order_type=OrderType.MARKET, price=price, from_stop=True)

    async def run(self) -> tuple[List[Trade], Dict[str, Any], pd.DataFrame]:
        logger.info("Starting portfolio backtest run...")
        all_timestamps = sorted(list(set.union(*[set(df.index) for df in self.data_feeds.values()])))
        
        if not all_timestamps:
            return [], {"initial_capital": self.initial_capital, "final_portfolio_value": self.initial_capital}, pd.DataFrame()

        self._update_portfolio_history(all_timestamps[0] - pd.Timedelta(seconds=1))

        for i, timestamp in enumerate(all_timestamps):
            self.current_bar_index = i
            self.current_bars: Dict[str, BarData] = {}
            historical_data_slice: Dict[str, pd.DataFrame] = {}
            
            has_new_data_for_cycle = False
            for asset, df in self.data_feeds.items():
                if timestamp in df.index:
                    has_new_data_for_cycle = True
                    historical_data_slice[asset] = df.loc[:timestamp]
                    
                    bar_data_dict = df.loc[timestamp].to_dict()
                    bar_data_dict.pop('symbol', None)
                    bar_data_dict.pop('timeframe', None)
                    
                    for key, value in bar_data_dict.items():
                        if isinstance(value, float) and np.isnan(value):
                            bar_data_dict[key] = None
                            
                    self.current_bars[asset] = BarData(timestamp=timestamp, symbol=asset, timeframe=self.portfolio_manager.timeframe, **bar_data_dict)
            
            self._update_portfolio_history(timestamp)
            self._check_stops(timestamp)

            if has_new_data_for_cycle:
                await self.portfolio_manager.run_cycle(historical_data_for_cycle=historical_data_slice)

        logger.info("Portfolio backtest run completed.")
        final_portfolio_state = {"initial_capital": self.initial_capital, "final_portfolio_value": self.current_portfolio_value}
        equity_curve_df = pd.DataFrame(self.portfolio_history).set_index('timestamp')
        
        return self.completed_trades_log, final_portfolio_state, equity_curve_df
</code>

kamikaze_komodo/config/settings.py:
<code>
# FILE: kamikaze_komodo/config/settings.py
import configparser
import os
from kamikaze_komodo.app_logger import get_logger
from typing import Dict, List, Optional, Any

logger = get_logger(__name__)

# Define a single, reliable project root that other modules can import.
# This file is in .../kamikaze_komodo/config/, so THREE levels up is the project root.
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class Config:
    """
    Manages application configuration using config.ini and secrets.ini.
    """
    def __init__(self, config_file_rel_path='kamikaze_komodo/config/config.ini', secrets_file_rel_path='kamikaze_komodo/config/secrets.ini'):
        self.config = configparser.ConfigParser()
        self.secrets = configparser.ConfigParser()

        self.config_file_path = os.path.join(PROJECT_ROOT, config_file_rel_path)
        self.secrets_file_path = os.path.join(PROJECT_ROOT, secrets_file_rel_path)

        if not os.path.exists(self.config_file_path):
            logger.error(f"Config file not found: {self.config_file_path}")
            raise FileNotFoundError(f"Config file not found: {self.config_file_path}")
        if not os.path.exists(self.secrets_file_path):
            logger.warning(f"Secrets file not found: {self.secrets_file_path}. Some features might be unavailable.")

        self.config.read(self.config_file_path)
        self.secrets.read(self.secrets_file_path)

        # General Settings
        self.log_level: str = self.config.get('General', 'LogLevel', fallback='INFO')
        self.log_file_path: str = self.config.get('General', 'LogFilePath', fallback='logs/kamikaze_komodo.log')

        # API Settings
        self.exchange_id_to_use: str = self.config.get('API', 'ExchangeID', fallback='krakenfutures')
        self.kraken_api_key: Optional[str] = self.secrets.get('KRAKEN_API', 'API_KEY', fallback=None)
        self.kraken_secret_key: Optional[str] = self.secrets.get('KRAKEN_API', 'SECRET_KEY', fallback=None)
        self.kraken_testnet: bool = self.config.getboolean('API', 'KrakenTestnet', fallback=True)

        # Data Fetching Settings
        self.default_symbol: str = self.config.get('DataFetching', 'DefaultSymbol', fallback='PF_XBTUSD')
        self.default_timeframe: str = self.config.get('DataFetching', 'DefaultTimeframe', fallback='4h')
        self.historical_data_days: int = self.config.getint('DataFetching', 'HistoricalDataDays', fallback=365)
        self.data_fetch_limit_per_call: int = self.config.getint('DataFetching', 'DataFetchLimitPerCall', fallback=500)

        # Trading Settings
        self.max_portfolio_risk: float = self.config.getfloat('Trading', 'MaxPortfolioRisk', fallback=0.02)
        self.default_leverage: float = self.config.getfloat('Trading', 'DefaultLeverage', fallback=1.0)
        self.commission_bps: float = self.config.getfloat('Trading', 'CommissionBPS', fallback=10.0)

        # EWMAC Strategy Settings (Example, specific strategies below)
        self.ewmac_short_window: int = self.config.getint('EWMAC_Strategy', 'ShortWindow', fallback=12)
        self.ewmac_long_window: int = self.config.getint('EWMAC_Strategy', 'LongWindow', fallback=26)
        self.ewmac_signal_window: int = self.config.getint('EWMAC_Strategy', 'SignalWindow', fallback=9)
        self.ewmac_atr_period: int = self.config.getint('EWMAC_Strategy', 'atr_period', fallback=14)


        # --- Phase 3: Risk Management Settings ---
        self.position_sizer_type: str = self.config.get('RiskManagement', 'PositionSizer', fallback='FixedFractional')
        self.fixed_fractional_allocation_fraction: float = self.config.getfloat('RiskManagement', 'FixedFractional_AllocationFraction', fallback=0.10)
        self.atr_based_risk_per_trade_fraction: float = self.config.getfloat('RiskManagement', 'ATRBased_RiskPerTradeFraction', fallback=0.01)
        self.atr_based_atr_multiple_for_stop: float = self.config.getfloat('RiskManagement', 'ATRBased_ATRMultipleForStop', fallback=2.0)

        self.stop_manager_type: str = self.config.get('RiskManagement', 'StopManager_Default', fallback='PercentageBased')
        _sl_pct_str = self.config.get('RiskManagement', 'PercentageStop_LossPct', fallback='0.02')
        self.percentage_stop_loss_pct: Optional[float] = float(_sl_pct_str) if _sl_pct_str and _sl_pct_str.lower() not in ['none', '0', '0.0'] else None
        _tp_pct_str = self.config.get('RiskManagement', 'PercentageStop_TakeProfitPct', fallback='0.05')
        self.percentage_stop_take_profit_pct: Optional[float] = float(_tp_pct_str) if _tp_pct_str and _tp_pct_str.lower() not in ['none', '0', '0.0'] else None
        self.atr_stop_atr_multiple: float = self.config.getfloat('RiskManagement', 'ATRStop_ATRMultiple', fallback=2.0)

        # --- Phase 3: Portfolio Constructor Settings ---
        self.asset_allocator_type: str = self.config.get('PortfolioConstructor', 'AssetAllocator', fallback='FixedWeight')
        default_symbol_config_key = f'DefaultAllocation_{self.default_symbol.replace("/", "").replace(":", "")}'
        self.default_allocation_for_symbol: float = self.config.getfloat('PortfolioConstructor', default_symbol_config_key, fallback=1.0)
        self.rebalancer_deviation_threshold: float = self.config.getfloat('PortfolioConstructor', 'Rebalancer_DeviationThreshold', fallback=0.05)

        # --- Phase 4: AI News Analysis Settings ---
        self.enable_sentiment_analysis: bool = self.config.getboolean('AI_NewsAnalysis', 'EnableSentimentAnalysis', fallback=True)
        self.sentiment_llm_provider: str = self.config.get('AI_NewsAnalysis', 'SentimentLLMProvider', fallback='VertexAI')
        self.browser_agent_llm_provider: str = self.config.get('AI_NewsAnalysis', 'BrowserAgent_LLMProvider', fallback='VertexAI')
        self.browser_agent_max_steps: int = self.config.getint('AI_NewsAnalysis', 'BrowserAgent_Max_Steps', fallback=20)


        self.sentiment_filter_threshold_long: float = self.config.getfloat('AI_NewsAnalysis', 'SentimentFilter_Threshold_Long', fallback=0.1)
        self.sentiment_filter_threshold_short: float = self.config.getfloat('AI_NewsAnalysis', 'SentimentFilter_Threshold_Short', fallback=-0.1)
        
        self.simulated_sentiment_data_path: Optional[str] = self.config.get('AI_NewsAnalysis', 'SimulatedSentimentDataPath', fallback=None)
        if self.simulated_sentiment_data_path and self.simulated_sentiment_data_path.lower() in ['none', '']:
            self.simulated_sentiment_data_path = None
        
        if self.simulated_sentiment_data_path and not os.path.isabs(self.simulated_sentiment_data_path):
            self.simulated_sentiment_data_path = os.path.join(PROJECT_ROOT, self.simulated_sentiment_data_path)


        self.news_scraper_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'NewsScraper_Enable', fallback=True)
        self.notification_listener_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'NotificationListener_Enable', fallback=False)
        self.browser_agent_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'BrowserAgent_Enable', fallback=False)
        
        # VertexAI Settings
        self.vertex_ai_project_id: Optional[str] = self.config.get('VertexAI', 'ProjectID', fallback=None)
        self.vertex_ai_location: Optional[str] = self.config.get('VertexAI', 'Location', fallback=None)
        self.vertex_ai_sentiment_model_name: str = self.config.get('VertexAI', 'SentimentModelName', fallback='gemini-1.5-flash-preview-0514')
        self.vertex_ai_browser_agent_model_name: str = self.config.get('VertexAI', 'BrowserAgentModelName', fallback='gemini-1.5-flash-preview-0514')

        if self.vertex_ai_project_id and self.vertex_ai_project_id.lower() == 'your-gcp-project-id':
            logger.warning("Vertex AI ProjectID is set to 'your-gcp-project-id'. Please update it in config.ini.")
            self.vertex_ai_project_id = None

        self.rss_feeds: List[Dict[str, str]] = []
        if self.config.has_section('AI_NewsAnalysis'):
            for key, value in self.config.items('AI_NewsAnalysis'):
                clean_key = key.strip().lower()
                if clean_key.startswith("rssfeed_"):
                    feed_name_part = clean_key.replace("rssfeed_", "")
                    feed_name = feed_name_part.replace("_", " ").title()
                    self.rss_feeds.append({"name": feed_name, "url": value})
        if not self.rss_feeds:
            logger.warning("No RSS feeds configured in config.ini under [AI_NewsAnalysis] with 'RSSFeed_' prefix.")


    def get_strategy_params(self, strategy_or_component_name: str) -> dict:
        """
        Retrieves parameters for a given strategy or component section name.
        Example section names: EWMAC_Strategy, LightGBM_Forecaster, MLForecaster_Strategy
        """
        params = {}
        found_section = None
        for section in self.config.sections():
            if section.lower() == strategy_or_component_name.lower():
                found_section = section
                break
        
        if found_section and self.config.has_section(found_section):
            params = dict(self.config.items(found_section))
            for key, value in params.items():
                original_value = value
                try:
                    if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                        params[key] = int(value)
                    else:
                        try:
                            params[key] = float(value)
                        except ValueError:
                            if value.lower() == 'true': params[key] = True
                            elif value.lower() == 'false': params[key] = False
                            elif value.lower() in ['none', '']: params[key] = None
                            else:
                                params[key] = original_value
                except Exception as e:
                    logger.debug(f"Could not auto-convert param '{key}' with value '{original_value}' in section '{found_section}'. Kept as string. Error: {e}")
                    params[key] = original_value
        else:
            logger.warning(f"No specific configuration section found for: {strategy_or_component_name}. Using defaults or globally passed params.")
        
        # FIX: Correct attribute names to match what's defined in __init__
        if 'sentimentfilter_long_threshold' not in params:
            params['sentimentfilter_long_threshold'] = self.sentiment_filter_threshold_long
        if 'sentimentfilter_short_threshold' not in params:
            params['sentimentfilter_short_threshold'] = self.sentiment_filter_threshold_short
            
        return params

    def get_news_scraper_config(self) -> Dict[str, Any]:
        cfg = {"rss_feeds": self.rss_feeds, "websites": []}
        return cfg


try:
    settings = Config()
except FileNotFoundError as e:
    logger.critical(f"Could not initialize settings due to missing configuration file: {e}")
    settings = None # type: ignore
except Exception as e_global:
    logger.critical(f"Failed to initialize Config object: {e_global}", exc_info=True)
    settings = None # type: ignore

if settings and (not settings.kraken_api_key or "YOUR_API_KEY" in str(settings.kraken_api_key).upper() or "D27PYGI95TLS" in str(settings.kraken_api_key).upper()):
    logger.warning(f"API Key for '{settings.exchange_id_to_use}' appears to be a placeholder or is not configured in secrets.ini. Authenticated interaction will be limited/simulated.")
</code>

kamikaze_komodo/config/__init__.py:
<code>
# kamikaze_komodo/config/__init__.py
# This file makes the 'config' directory a Python package.
</code>

kamikaze_komodo/config/secrets.ini:
<code>
; kamikaze_komodo/config/secrets.ini
; This file should be in .gitignore and contain sensitive information.
[KRAKEN_API]
API_KEY = 'd27PYGi95tlsV4gVotVNXinHOTAxXY2usUta7kw3IogO9/9kpLHCHgcv'
SECRET_KEY = 'kB+i8be+l7J6Lr+RyjodrqNyQXrIn6reFeNfDsmMs01zsQg3KPGSSshd9l4KwvY92LQyYamDc1lMrHsnZ6+LaWQP'

[DATABASE]
User = db_user
Password = db_password
</code>

kamikaze_komodo/config/config.ini:
<code>
# kamikaze_komodo/config/config.ini

[General]
LogLevel = INFO
LogFilePath = logs/kamikaze_komodo.log
RunMode = backtest

[API]
ExchangeID = krakenfutures
KrakenTestnet = True

[DataFetching]
DefaultSymbol = PF_XBTUSD
DefaultTimeframe = 4h
HistoricalDataDays = 730 
DataFetchLimitPerCall = 500

[Trading]
CommissionBPS = 10
SlippageBPS = 2

# --- PORTFOLIO CONFIGURATION ---
# This section defines the overall portfolio for the main backtest run.
[Portfolio]
# We are focusing the universe on the single asset we optimized.
TradingUniverse = PF_XBTUSD
# We are activating only the single best strategy found by the optimizer.
ActiveStrategies = EhlersInstantaneousTrendline_Strategy
# For a single asset, HRP falls back to 100% allocation, which is what we want.
AssetAllocator = HRPAllocator
Rebalancer = BasicRebalancer
EnableRegimeFilter = False

[Backtesting]
InitialCapital = 10000.0


# === STRATEGY-SPECIFIC PARAMETERS ===

# --- This is the winning strategy with its optimized parameter ---
[EhlersInstantaneousTrendline_Strategy]
it_lag_trigger = 2
atr_period = 14
EnableShorting = True


# === RISK MANAGEMENT PARAMETERS ===

[RiskManagement]
# This defines the DEFAULT Stop Manager to use.
StopManager_Default = TripleBarrier

# --- This is the dedicated section for the TripleBarrier Stop Manager ---
# --- The program will now correctly load these optimized values. ---
[TripleBarrier]
TripleBarrier_TP_Multiple = 2.0
TripleBarrier_SL_Multiple = 2.0
TripleBarrier_Time_Limit_Bars = 20


# --- Other sections are kept for reference ---

[Rebalancer]
Rebalancer_Min_Order_Value_USD = 10.0

[BacktestingPerformance]
RiskFreeRateAnnual = 0.02
AnnualizationFactor = 252

[EWMAC_Strategy]
ShortWindow = 12
LongWindow = 26
EnableShorting = True

[FundingRateArb_Strategy]
entry_threshold = 0.0001
exit_threshold = 0.00005
EnableShorting = True

[MLForecaster_Strategy]
longconfidencethreshold = 0.55
shortconfidencethreshold = 0.55
atr_period = 14
EnableShorting = True

[KMeans_Regime_Model]
Num_Clusters = 3
FeaturesForClustering = volatility_20d, atr_14d_percentage
TrainingDaysHistory = 1095
ModelSavePath = ml_models/trained_models
ModelFileName = kmeans_regime_PF_XBTUSD_4h.joblib

[AI_NewsAnalysis]
EnableSentimentAnalysis = False
SimulatedSentimentDataPath = kamikaze_komodo/data/simulated_sentiment_data.csv

[VertexAI]
ProjectID = kamikazekomodo
Location = us-central1
</code>

kamikaze_komodo/core/enums.py:
<code>
# kamikaze_komodo/core/enums.py

from enum import Enum


class OrderType(Enum):

    """

    Represents the type of an order.

    """

    MARKET = "market"

    LIMIT = "limit"

    STOP = "stop"

    STOP_LIMIT = "stop_limit"

    TAKE_PROFIT = "take_profit"

    TAKE_PROFIT_LIMIT = "take_profit_limit"


class OrderSide(Enum):

    """

    Represents the side of an order.

    """

    BUY = "buy"

    SELL = "sell"


class SignalType(Enum):

    """

    Represents the type of trading signal generated by a strategy.

    """

    LONG = "LONG"

    SHORT = "SHORT"

    HOLD = "HOLD"

    CLOSE_LONG = "CLOSE_LONG"

    CLOSE_SHORT = "CLOSE_SHORT"


class CandleInterval(Enum):

    """

    Represents common candle intervals for market data.

    Follows CCXT conventions where possible.

    """

    ONE_MINUTE = "1m"

    THREE_MINUTES = "3m"

    FIVE_MINUTES = "5m"

    FIFTEEN_MINUTES = "15m"

    THIRTY_MINUTES = "30m"

    ONE_HOUR = "1h"

    TWO_HOURS = "2h"

    FOUR_HOURS = "4h"

    SIX_HOURS = "6h"

    EIGHT_HOURS = "8h"

    TWELVE_HOURS = "12h"

    ONE_DAY = "1d"

    THREE_DAYS = "3d"

    ONE_WEEK = "1w"

    ONE_MONTH = "1M"


class TradeResult(Enum):

    """

    Represents the outcome of a trade.

    """

    WIN = "WIN"

    LOSS = "LOSS"

    BREAKEVEN = "BREAKEVEN"

</code>

kamikaze_komodo/core/__init__.py:
<code>
# kamikaze_komodo/core/__init__.py
# This file makes the 'core' directory a Python package.
</code>

kamikaze_komodo/core/utils.py:
<code>
from datetime import datetime, timezone
from kamikaze_komodo.core.models import BarData
def format_timestamp(ts: datetime, fmt: str = "%Y-%m-%d %H:%M:%S %Z") -> str:
    """
    Formats a datetime object into a string.
    Ensures timezone awareness, defaulting to UTC if naive.
    """
    if ts.tzinfo is None:
        ts = ts.replace(tzinfo=timezone.utc)
    return ts.strftime(fmt)
def current_timestamp_ms() -> int:
    """
    Returns the current UTC timestamp in milliseconds.
    """
    return int(datetime.now(timezone.utc).timestamp() * 1000)
def ohlcv_to_bardata(ohlcv: list, symbol: str, timeframe: str) -> BarData:
    """
    Converts a CCXT OHLCV list [timestamp_ms, open, high, low, close, volume]
    to a BarData object.
    """
    from kamikaze_komodo.core.models import BarData # Local import to avoid circular dependency
    
    if len(ohlcv) != 6:
        raise ValueError("OHLCV list must contain 6 elements: timestamp, open, high, low, close, volume")
    dt_object = datetime.fromtimestamp(ohlcv[0] / 1000, tz=timezone.utc)
    return BarData(
        timestamp=dt_object,
        open=float(ohlcv[1]),
        high=float(ohlcv[2]),
        low=float(ohlcv[3]),
        close=float(ohlcv[4]),
        volume=float(ohlcv[5]),
        symbol=symbol,
        timeframe=timeframe
    )
# Add other utility functions as needed, e.g.,
# - Mathematical helpers not in TA-Lib
# - Data validation functions
# - etc.
</code>

kamikaze_komodo/core/models.py:
<code>
# kamikaze_komodo/core/models.py
# Added fields to NewsArticle and Trade as potentially needed by new modules.
# Added prediction_value and prediction_confidence to BarData for Phase 5 ML strategy.
# Phase 6: Added PairTrade model.
from typing import Optional, List, Dict, Any # Added Any
from pydantic import BaseModel, Field
from datetime import datetime, timezone
import pydantic # Added timezone
from kamikaze_komodo.core.enums import OrderType, OrderSide, SignalType, TradeResult

class BarData(BaseModel):
    """
    Represents OHLCV market data for a specific time interval.
    """
    timestamp: datetime = Field(..., description="The start time of the candle, expected to be timezone-aware (UTC)")
    open: float = Field(..., gt=0, description="Opening price")
    high: float = Field(..., gt=0, description="Highest price")
    low: float = Field(..., gt=0, description="Lowest price")
    close: float = Field(..., gt=0, description="Closing price")
    volume: float = Field(..., ge=0, description="Trading volume")
    symbol: Optional[str] = Field(None, description="Trading symbol, e.g., BTC/USD")
    timeframe: Optional[str] = Field(None, description="Candle timeframe, e.g., 1h")
    
    # Optional fields for indicators or sentiment
    atr: Optional[float] = Field(None, description="Average True Range at this bar")
    sentiment_score: Optional[float] = Field(None, description="Sentiment score associated with this bar's timestamp")
    
    # Fields for ML predictions (Phase 5)
    prediction_value: Optional[float] = Field(None, description="Predicted value by an ML model (e.g., future price, return)")
    prediction_confidence: Optional[float] = Field(None, description="Confidence of the ML prediction (0.0 to 1.0)")

    # Phase 6: Market Regime
    market_regime: Optional[int] = Field(None, description="Market regime identified by a model (e.g., 0, 1, 2)")

    # Phase 7, P2: Funding Rate
    funding_rate: Optional[float] = Field(None, description="The funding rate for the perpetual future at this timestamp.")


    class Config:
        frozen = False # Allow modification by strategies/engine

class FundingRate(BaseModel):
    """
    Represents a single funding rate data point for a perpetual future.
    """
    symbol: str
    timestamp: datetime
    funding_rate: float
    mark_price: Optional[float] = None


class Order(BaseModel):
    # ... (no changes from existing)
    id: str = Field(..., description="Unique order identifier (from exchange or internal)")
    symbol: str = Field(..., description="Trading symbol, e.g., BTC/USD")
    type: OrderType = Field(..., description="Type of order (market, limit, etc.)")
    side: OrderSide = Field(..., description="Order side (buy or sell)")
    amount: float = Field(..., gt=0, description="Quantity of the asset to trade")
    price: Optional[float] = Field(None, gt=0, description="Price for limit or stop orders")
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description="Time the order was created")
    status: str = Field("open", description="Current status of the order (e.g., open, filled, canceled)")
    filled_amount: float = Field(0.0, ge=0, description="Amount of the order that has been filled")
    average_fill_price: Optional[float] = Field(None, description="Average price at which the order was filled")
    exchange_id: Optional[str] = Field(None, description="Order ID from the exchange")

class Trade(BaseModel):
    """
    Represents an executed trade.
    """
    id: str = Field(..., description="Unique trade identifier")
    symbol: str = Field(..., description="Trading symbol, e.g., BTC/USD")
    entry_order_id: str = Field(..., description="ID of the order that opened the trade")
    exit_order_id: Optional[str] = Field(None, description="ID of the order that closed the trade")
    side: OrderSide = Field(..., description="Trade side (buy/long or sell/short)")
    entry_price: float = Field(..., gt=0, description="Price at which the trade was entered")
    exit_price: Optional[float] = Field(None, description="Price at which the trade was exited (must be >0 if set)")
    amount: float = Field(..., gt=0, description="Quantity of the asset traded")
    entry_timestamp: datetime = Field(..., description="Time the trade was entered")
    exit_timestamp: Optional[datetime] = Field(None, description="Time the trade was exited")
    pnl: Optional[float] = Field(None, description="Profit or Loss for the trade")
    pnl_percentage: Optional[float] = Field(None, description="Profit or Loss percentage for the trade")
    commission: float = Field(0.0, ge=0, description="Trading commission paid")
    result: Optional[TradeResult] = Field(None, description="Outcome of the trade (Win/Loss/Breakeven)")
    notes: Optional[str] = Field(None, description="Any notes related to the trade")
    # Added for ATR based stops or other context
    custom_fields: Dict[str, Any] = Field(default_factory=dict, description="Custom fields for additional trade data, e.g., atr_at_entry")

    @pydantic.field_validator('exit_price')
    def exit_price_must_be_positive(cls, v):
        if v is not None and v <= 0:
            raise ValueError('exit_price must be positive if set')
        return v

class NewsArticle(BaseModel):
    """
    Represents a news article relevant to market analysis.
    """
    id: str = Field(..., description="Unique identifier for the news article (e.g., URL hash or URL itself)")
    url: str = Field(..., description="Source URL of the article")
    title: str = Field(..., description="Headline or title of the article")
    publication_date: Optional[datetime] = Field(None, description="Date the article was published (UTC)")
    retrieval_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description="Date the article was retrieved (UTC)")
    source: str = Field(..., description="Source of the news (e.g., CoinDesk, CoinTelegraph, RSS feed name)")
    content: Optional[str] = Field(None, description="Full text content of the article")
    summary: Optional[str] = Field(None, description="AI-generated or scraped summary")
    
    # Sentiment related fields
    sentiment_score: Optional[float] = Field(None, description="Overall sentiment score (-1.0 to 1.0)")
    sentiment_label: Optional[str] = Field(None, description="Sentiment label (e.g., positive, negative, neutral, bullish, bearish)")
    sentiment_confidence: Optional[float] = Field(None, description="Confidence of the sentiment analysis (0.0 to 1.0)")
    key_themes: Optional[List[str]] = Field(default_factory=list, description="Key themes identified by sentiment analysis")
    
    related_symbols: Optional[List[str]] = Field(default_factory=list, description="Cryptocurrencies mentioned or related")
    raw_llm_response: Optional[Dict[str, Any]] = Field(None, description="Raw response from LLM for sentiment if available")


class PortfolioSnapshot(BaseModel):
    # ... (no changes from existing)
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    total_value_usd: float = Field(..., description="Total portfolio value in USD")
    cash_balance_usd: float = Field(..., description="Available cash in USD")
    positions: Dict[str, float] = Field(default_factory=dict, description="Asset quantities, e.g., {'BTC': 0.5, 'ETH': 10}") # symbol: quantity
    open_pnl_usd: float = Field(0.0, description="Total open Profit/Loss in USD for current positions")


class PairTrade(BaseModel):
    """
    Represents a pair trade involving two assets.
    Phase 6 Model.
    """
    id: str = Field(..., description="Unique identifier for the pair trade")
    asset1_symbol: str = Field(..., description="Symbol of the first asset in the pair")
    asset2_symbol: str = Field(..., description="Symbol of the second asset in the pair")
    
    asset1_trade_id: Optional[str] = Field(None, description="Trade ID for the leg involving asset1")
    asset2_trade_id: Optional[str] = Field(None, description="Trade ID for the leg involving asset2")

    entry_timestamp: datetime = Field(..., description="Time the pair trade was initiated")
    exit_timestamp: Optional[datetime] = Field(None, description="Time the pair trade was closed")
    
    # Spread details at entry
    entry_spread: float = Field(..., description="Spread value at the time of entry")
    entry_zscore: Optional[float] = Field(None, description="Z-score of the spread at entry")
    
    # Spread details at exit
    exit_spread: Optional[float] = Field(None, description="Spread value at the time of exit")
    exit_zscore: Optional[float] = Field(None, description="Z-score of the spread at exit")

    # P&L for the combined pair trade
    pnl: Optional[float] = Field(None, description="Overall Profit or Loss for the pair trade")
    pnl_percentage: Optional[float] = Field(None, description="Overall Profit or Loss percentage for the pair trade")
    
    total_commission: float = Field(0.0, ge=0, description="Total commission for both legs of the pair trade")
    status: str = Field("open", description="Status of the pair trade (e.g., open, closed)")
    exit_reason: Optional[str] = Field(None, description="Reason for closing the pair trade (e.g., spread reversion, stop loss)")
    notes: Optional[str] = Field(None, description="Any notes related to the pair trade")
    custom_fields: Dict[str, Any] = Field(default_factory=dict, description="Custom fields for additional pair trade data")
</code>

kamikaze_komodo/risk_control_module/stop_manager.py:
<code>
# FILE: kamikaze_komodo/risk_control_module/stop_manager.py
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from kamikaze_komodo.core.models import BarData, Trade
from kamikaze_komodo.core.enums import OrderSide
from kamikaze_komodo.app_logger import get_logger
from datetime import timedelta

logger = get_logger(__name__)

class BaseStopManager(ABC):
    """
    Abstract base class for stop-loss and take-profit management.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def check_stop_loss(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        bar_index: int # New param needed for time-based stops
    ) -> Optional[float]: # Returns stop price if triggered, else None
        """
        Checks if the stop-loss condition is met for the current trade.
        """
        pass

    @abstractmethod
    def check_take_profit(
        self,
        current_trade: Trade,
        latest_bar: BarData
    ) -> Optional[float]: # Returns take profit price if triggered, else None
        """
        Checks if the take-profit condition is met for the current trade.
        """
        pass

class PercentageStopManager(BaseStopManager):
    """
    Manages stops based on a fixed percentage from the entry price.
    """
    def __init__(self, stop_loss_pct: Optional[float] = None, take_profit_pct: Optional[float] = None, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.stop_loss_pct = float(self.params.get('percentagestop_losspct', stop_loss_pct if stop_loss_pct is not None else 0))
        self.take_profit_pct = float(self.params.get('percentagestop_takeprofitpct', take_profit_pct if take_profit_pct is not None else 0))

        if self.stop_loss_pct < 0 or self.stop_loss_pct >= 1.0 :
              if self.stop_loss_pct != 0:
                  raise ValueError("stop_loss_pct must be between 0 (inclusive, to disable) and 1 (exclusive).")
        if self.take_profit_pct < 0:
              if self.take_profit_pct != 0:
                  raise ValueError("take_profit_pct must be non-negative (0 to disable).")
        
        self.stop_loss_pct = None if self.stop_loss_pct == 0 else self.stop_loss_pct
        self.take_profit_pct = None if self.take_profit_pct == 0 else self.take_profit_pct
            
        logger.info(f"PercentageStopManager initialized. SL: {self.stop_loss_pct*100 if self.stop_loss_pct else 'N/A'}%, TP: {self.take_profit_pct*100 if self.take_profit_pct else 'N/A'}%")

    def check_stop_loss(self, current_trade: Trade, latest_bar: BarData, bar_index: int) -> Optional[float]:
        if not self.stop_loss_pct or not current_trade or current_trade.entry_price <= 0:
            return None

        if current_trade.side == OrderSide.BUY:
            stop_price = current_trade.entry_price * (1 - self.stop_loss_pct)
            if latest_bar.low <= stop_price:
                logger.info(f"STOP LOSS (BUY) triggered for trade {current_trade.id} ({current_trade.symbol}) at SL price {stop_price:.4f} (Bar Low: {latest_bar.low:.4f}, Entry: {current_trade.entry_price:.4f})")
                return stop_price
        elif current_trade.side == OrderSide.SELL:
            stop_price = current_trade.entry_price * (1 + self.stop_loss_pct)
            if latest_bar.high >= stop_price:
                logger.info(f"STOP LOSS (SELL) triggered for trade {current_trade.id} ({current_trade.symbol}) at SL price {stop_price:.4f} (Bar High: {latest_bar.high:.4f}, Entry: {current_trade.entry_price:.4f})")
                return stop_price
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData) -> Optional[float]:
        if not self.take_profit_pct or not current_trade or current_trade.entry_price <= 0:
            return None

        if current_trade.side == OrderSide.BUY:
            profit_price = current_trade.entry_price * (1 + self.take_profit_pct)
            if latest_bar.high >= profit_price:
                logger.info(f"TAKE PROFIT (BUY) triggered for trade {current_trade.id} ({current_trade.symbol}) at TP price {profit_price:.4f} (Bar High: {latest_bar.high:.4f}, Entry: {current_trade.entry_price:.4f})")
                return profit_price
        elif current_trade.side == OrderSide.SELL:
            profit_price = current_trade.entry_price * (1 - self.take_profit_pct)
            if latest_bar.low <= profit_price:
                logger.info(f"TAKE PROFIT (SELL) triggered for trade {current_trade.id} ({current_trade.symbol}) at TP price {profit_price:.4f} (Bar Low: {latest_bar.low:.4f}, Entry: {current_trade.entry_price:.4f})")
                return profit_price
        return None

class ATRStopManager(BaseStopManager):
    """
    Manages stops based on ATR.
    """
    def __init__(self, atr_multiple: float = 2.0, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.atr_multiple = float(self.params.get('atrstop_atrmultiple', atr_multiple))
        logger.info(f"ATRStopManager initialized with ATR multiple: {self.atr_multiple}")

    def check_stop_loss(self, current_trade: Trade, latest_bar: BarData, bar_index: int) -> Optional[float]:
        atr_at_entry = current_trade.custom_fields.get("atr_at_entry") if hasattr(current_trade, 'custom_fields') and current_trade.custom_fields else None
        
        if atr_at_entry is None or atr_at_entry <= 1e-8:
            if latest_bar.atr is not None and latest_bar.atr > 1e-8:
                logger.debug(f"ATR at entry not available for trade {current_trade.id}. Using latest_bar.atr ({latest_bar.atr:.6f}) for ATR stop check.")
                atr_at_entry = latest_bar.atr
            else:
                logger.debug(f"ATR value not available or invalid for trade {current_trade.id}. Cannot apply ATR stop. ATR at entry: {atr_at_entry}, Latest bar ATR: {latest_bar.atr}")
                return None
        
        stop_distance = self.atr_multiple * atr_at_entry
        if current_trade.side == OrderSide.BUY:
            stop_price = current_trade.entry_price - stop_distance
            if latest_bar.low <= stop_price:
                logger.info(f"ATR STOP LOSS (BUY) for {current_trade.symbol} at {stop_price:.4f} (Entry: {current_trade.entry_price:.4f}, ATR Used: {atr_at_entry:.6f}, BarLow: {latest_bar.low:.4f})")
                return stop_price
        elif current_trade.side == OrderSide.SELL:
            stop_price = current_trade.entry_price + stop_distance
            if latest_bar.high >= stop_price:
                logger.info(f"ATR STOP LOSS (SELL) for {current_trade.symbol} at {stop_price:.4f} (Entry: {current_trade.entry_price:.4f}, ATR Used: {atr_at_entry:.6f}, BarHigh: {latest_bar.high:.4f})")
                return stop_price
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData) -> Optional[float]:
        return None

class TripleBarrierStopManager(BaseStopManager):
    """
    Implements De Prado's Triple-Barrier Method.
    Sets a stop-loss, a take-profit, and a time-based exit.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.tp_multiple = float(self.params.get('triplebarrier_tp_multiple', 1.5))  # Take profit as a multiple of risk (ATR)
        self.sl_multiple = float(self.params.get('triplebarrier_sl_multiple', 1.0))  # Stop loss as a multiple of risk (ATR)
        self.time_limit_bars = int(self.params.get('triplebarrier_time_limit_bars', 10)) # Time limit in number of bars
        logger.info(f"TripleBarrierStopManager initialized. TP Multiple: {self.tp_multiple}, SL Multiple: {self.sl_multiple}, Time Limit: {self.time_limit_bars} bars.")

    def _get_risk_and_barriers(self, trade: Trade, bar: BarData) -> Optional[Dict[str, float]]:
        """Calculates the risk (e.g., ATR at entry) and derives the barriers."""
        risk_per_unit = trade.custom_fields.get("atr_at_entry")
        if risk_per_unit is None or risk_per_unit <= 1e-8:
            logger.warning(f"ATR at entry not found for trade {trade.id}. Cannot apply Triple Barrier.")
            return None

        entry_price = trade.entry_price
        if trade.side == OrderSide.BUY:
            sl_price = entry_price - (self.sl_multiple * risk_per_unit)
            tp_price = entry_price + (self.tp_multiple * risk_per_unit)
        else: # OrderSide.SELL
            sl_price = entry_price + (self.sl_multiple * risk_per_unit)
            tp_price = entry_price - (self.tp_multiple * risk_per_unit)

        return {"sl_price": sl_price, "tp_price": tp_price}

    def check_stop_loss(self, current_trade: Trade, latest_bar: BarData, bar_index: int) -> Optional[float]:
        barriers = self._get_risk_and_barriers(current_trade, latest_bar)
        if barriers is None:
            return None

        sl_price = barriers['sl_price']
        
        # 1. Price-based stop loss
        if current_trade.side == OrderSide.BUY and latest_bar.low <= sl_price:
            logger.info(f"Triple-Barrier SL (BUY) triggered for trade {current_trade.id} at {sl_price:.4f}")
            return sl_price
        if current_trade.side == OrderSide.SELL and latest_bar.high >= sl_price:
            logger.info(f"Triple-Barrier SL (SELL) triggered for trade {current_trade.id} at {sl_price:.4f}")
            return sl_price
        
        # 2. Time-based stop (vertical barrier)
        entry_bar_index = current_trade.custom_fields.get("entry_bar_index")
        if entry_bar_index is not None:
            bars_held = bar_index - entry_bar_index
            if bars_held >= self.time_limit_bars:
                logger.info(f"Triple-Barrier TIME LIMIT reached for trade {current_trade.id} after {bars_held} bars.")
                return latest_bar.close # Exit at the current closing price
        
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData) -> Optional[float]:
        barriers = self._get_risk_and_barriers(current_trade, latest_bar)
        if barriers is None:
            return None

        tp_price = barriers['tp_price']
        
        if current_trade.side == OrderSide.BUY and latest_bar.high >= tp_price:
            logger.info(f"Triple-Barrier TP (BUY) triggered for trade {current_trade.id} at {tp_price:.4f}")
            return tp_price
        if current_trade.side == OrderSide.SELL and latest_bar.low <= tp_price:
            logger.info(f"Triple-Barrier TP (SELL) triggered for trade {current_trade.id} at {tp_price:.4f}")
            return tp_price
        
        return None
</code>

kamikaze_komodo/risk_control_module/__init__.py:
<code>
# kamikaze_komodo/risk_control_module/__init__.py
# This file makes the 'risk_control_module' directory a Python package.
logger_name = "KamikazeKomodo.risk_control_module" # Satisfy linter
</code>

kamikaze_komodo/risk_control_module/volatility_band_stop_manager.py:
<code>
# kamikaze_komodo/risk_control_module/volatility_band_stop_manager.py
from typing import Optional, Dict, Any
import pandas as pd
import pandas_ta as ta
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager
from kamikaze_komodo.core.models import BarData, Trade
from kamikaze_komodo.core.enums import OrderSide
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class VolatilityBandStopManager(BaseStopManager):
    """
    Manages stops based on volatility bands like Bollinger Bands or Keltner Channels.
    Can be used for trailing stops along the bands.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.band_type = self.params.get('volatilitybandstop_band_type', 'bollinger').lower() # 'bollinger' or 'keltner'
        
        # Bollinger Band params
        self.bb_period = int(self.params.get('volatilitybandstop_bb_period', 20))
        self.bb_std_dev = float(self.params.get('volatilitybandstop_bb_stddev', 2.0))
        
        # Keltner Channel params (if used)
        self.kc_period = int(self.params.get('volatilitybandstop_kc_period', 20)) # EMA period
        self.kc_atr_period = int(self.params.get('volatilitybandstop_kc_atr_period', 10))
        self.kc_atr_multiplier = float(self.params.get('volatilitybandstop_kc_atr_multiplier', 1.5))

        self.trail_type = self.params.get('volatilitybandstop_trailtype', 'none').lower() # e.g., 'trailing_bb_upper', 'trailing_bb_lower', 'none'
        
        # Store current stop levels if trailing
        self.current_trailing_stop_price: Optional[float] = None

        logger.info(f"VolatilityBandStopManager initialized. Band: {self.band_type}, Trail: {self.trail_type}")

    def _calculate_bands(self, data_history: pd.DataFrame) -> pd.DataFrame:
        df = data_history.copy()
        if df.empty or len(df) < max(self.bb_period, self.kc_period, self.kc_atr_period):
            return df # Not enough data

        if self.band_type == 'bollinger':
            if 'close' in df.columns and len(df) >= self.bb_period:
                try:
                    bbands = ta.bbands(df['close'], length=self.bb_period, std=self.bb_std_dev)
                    if bbands is not None and not bbands.empty:
                        # pandas_ta typically names columns like BBL_20_2.0, BBM_20_2.0, BBU_20_2.0
                        df['band_lower'] = bbands[f'BBL_{self.bb_period}_{self.bb_std_dev:.1f}']
                        df['band_middle'] = bbands[f'BBM_{self.bb_period}_{self.bb_std_dev:.1f}']
                        df['band_upper'] = bbands[f'BBU_{self.bb_period}_{self.bb_std_dev:.1f}']
                except Exception as e:
                    logger.error(f"Error calculating Bollinger Bands for VolStopManager: {e}")
                    df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
            else:
                 df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA

        elif self.band_type == 'keltner':
            if all(c in df.columns for c in ['high', 'low', 'close']) and len(df) >= max(self.kc_period, self.kc_atr_period):
                try:
                    kc = ta.kc(df['high'], df['low'], df['close'], length=self.kc_period, atr_length=self.kc_atr_period, mamode="EMA", multiplier=self.kc_atr_multiplier)
                    if kc is not None and not kc.empty:
                        # Column names from pandas_ta for Keltner might be like KCLer_20_10_1.5, KCMer_20_10_1.5, KCUer_20_10_1.5
                        # Need to verify exact names or use generic ones if possible. Let's assume standard:
                        df['band_lower'] = kc.iloc[:,0] # Lower band often first column
                        df['band_middle'] = kc.iloc[:,1] # Middle band
                        df['band_upper'] = kc.iloc[:,2] # Upper band
                except Exception as e:
                    logger.error(f"Error calculating Keltner Channels for VolStopManager: {e}")
                    df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
            else:
                 df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
        else:
            logger.warning(f"Unsupported band_type: {self.band_type} in VolatilityBandStopManager.")
            df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
        return df

    def check_stop_loss(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        data_history_for_bands: Optional[pd.DataFrame] = None # Pass full history for band calculation
    ) -> Optional[float]:
        if not data_history_for_bands or data_history_for_bands.empty:
            logger.warning("Data history for bands not provided to VolatilityBandStopManager.")
            return None

        df_with_bands = self._calculate_bands(data_history_for_bands)
        if df_with_bands.empty or 'band_lower' not in df_with_bands.columns or 'band_upper' not in df_with_bands.columns:
            logger.debug("Bands not available for stop loss check.")
            return None
        
        latest_band_lower = df_with_bands['band_lower'].iloc[-1]
        latest_band_upper = df_with_bands['band_upper'].iloc[-1]

        if pd.isna(latest_band_lower) or pd.isna(latest_band_upper):
            logger.debug("Latest band values are NaN.")
            return None

        stop_price = None

        if self.trail_type == 'none': # Fixed stop based on band at entry (requires band_at_entry)
            # This simple version will use current bands as stop.
            # For entry-based band stop, band value at entry should be stored in Trade.custom_fields
            if current_trade.side == OrderSide.BUY:
                stop_price = latest_band_lower # Simplistic: stop at current lower band
                if latest_bar.low <= stop_price:
                    logger.info(f"VOL_BAND STOP (BUY, fixed on current) for {current_trade.symbol} at {stop_price:.4f}")
                    return stop_price
            elif current_trade.side == OrderSide.SELL:
                stop_price = latest_band_upper # Simplistic: stop at current upper band
                if latest_bar.high >= stop_price:
                    logger.info(f"VOL_BAND STOP (SELL, fixed on current) for {current_trade.symbol} at {stop_price:.4f}")
                    return stop_price
        else: # Trailing stop logic
            if current_trade.side == OrderSide.BUY:
                # Trail stop along the lower band (or middle band)
                potential_stop = latest_band_lower # Default to lower band for long
                if self.trail_type == 'trailing_bb_middle' or self.trail_type == 'trailing_kc_middle':
                     if 'band_middle' in df_with_bands.columns and pd.notna(df_with_bands['band_middle'].iloc[-1]):
                        potential_stop = df_with_bands['band_middle'].iloc[-1]
                
                if self.current_trailing_stop_price is None or potential_stop > self.current_trailing_stop_price:
                    self.current_trailing_stop_price = potential_stop
                
                if self.current_trailing_stop_price and latest_bar.low <= self.current_trailing_stop_price:
                    logger.info(f"VOL_BAND TRAILING STOP (BUY) for {current_trade.symbol} at {self.current_trailing_stop_price:.4f}")
                    stop_price_to_return = self.current_trailing_stop_price
                    self.current_trailing_stop_price = None # Reset after hit
                    return stop_price_to_return

            elif current_trade.side == OrderSide.SELL:
                # Trail stop along the upper band (or middle band)
                potential_stop = latest_band_upper # Default to upper band for short
                if self.trail_type == 'trailing_bb_middle' or self.trail_type == 'trailing_kc_middle':
                     if 'band_middle' in df_with_bands.columns and pd.notna(df_with_bands['band_middle'].iloc[-1]):
                        potential_stop = df_with_bands['band_middle'].iloc[-1]

                if self.current_trailing_stop_price is None or potential_stop < self.current_trailing_stop_price:
                    self.current_trailing_stop_price = potential_stop

                if self.current_trailing_stop_price and latest_bar.high >= self.current_trailing_stop_price:
                    logger.info(f"VOL_BAND TRAILING STOP (SELL) for {current_trade.symbol} at {self.current_trailing_stop_price:.4f}")
                    stop_price_to_return = self.current_trailing_stop_price
                    self.current_trailing_stop_price = None # Reset after hit
                    return stop_price_to_return
        return None

    def check_take_profit(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        # data_history_for_bands: Optional[pd.DataFrame] = None # If TP uses bands
    ) -> Optional[float]:
        # Volatility bands are typically used for stops or dynamic exits, not fixed TP.
        # Could implement TP if price touches opposite band, e.g.
        # For now, this manager focuses on stop-loss.
        # Reset trailing stop if trade is closed by other means (e.g. strategy signal)
        if self.current_trailing_stop_price is not None and current_trade.exit_timestamp is not None:
             self.current_trailing_stop_price = None
        return None

    def reset_trailing_stop(self):
        """Called when a new trade is initiated or an old one is closed by other means."""
        self.current_trailing_stop_price = None
</code>

kamikaze_komodo/risk_control_module/position_sizer.py:
<code>
# kamikaze_komodo/risk_control_module/position_sizer.py
from abc import ABC, abstractmethod
from typing import Dict, Optional, Any, Tuple
import numpy as np
from kamikaze_komodo.core.models import BarData # For ATR based sizers potentially
from kamikaze_komodo.app_logger import get_logger
logger = get_logger(__name__)

class BasePositionSizer(ABC):
    """
    Abstract base class for position sizing strategies.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float, # Total equity
        strategy_signal_strength: Optional[float] = None, # e.g. ML confidence
        latest_bar: Optional[BarData] = None, # For ATR or volatility based
        atr_value: Optional[float] = None # Explicit ATR if available
    ) -> Optional[float]: # Returns position size in asset units, or None if no trade
        """
        Calculates the size of the position to take.
        Args:
            symbol (str): The asset symbol.
            current_price (float): The current price of the asset.
            available_capital (float): The cash available for trading. (May not be used by all sizers)
            current_portfolio_value (float): The total current value of the portfolio (equity).
            strategy_signal_strength (Optional[float]): Confidence or strength of the signal.
            latest_bar (Optional[BarData]): Latest bar data for volatility calculation.
            atr_value (Optional[float]): Pre-calculated ATR value.
        Returns:
            Optional[float]: The quantity of the asset to trade. None if cannot size or no trade.
        """
        pass

class FixedFractionalPositionSizer(BasePositionSizer):
    """
    Sizes positions based on a fixed fraction of the total portfolio equity.
    """
    def __init__(self, fraction: float = 0.01, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        # fraction is often sourced from config: FixedFractional_AllocationFraction
        self.fraction_to_allocate = float(self.params.get('fixedfractional_allocationfraction', fraction))
        if not 0 < self.fraction_to_allocate <= 1.0:
            logger.error(f"Fraction must be between 0 (exclusive) and 1 (inclusive). Got {self.fraction_to_allocate}")
            raise ValueError("Fraction must be > 0 and <= 1.")
        logger.info(f"FixedFractionalPositionSizer initialized with fraction: {self.fraction_to_allocate}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float, # Cash
        current_portfolio_value: float, # Equity
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        if current_price <= 0:
            logger.warning(f"Current price for {symbol} is non-positive ({current_price}). Cannot calculate position size.")
            return None
        if current_portfolio_value <= 0:
            logger.warning(f"Current portfolio value is non-positive ({current_portfolio_value}). Cannot calculate position size.")
            return None
        
        capital_to_allocate = current_portfolio_value * self.fraction_to_allocate
        
        if capital_to_allocate > available_capital :
            logger.warning(f"Calculated capital to allocate ({capital_to_allocate:.2f}) for {symbol} exceeds available cash ({available_capital:.2f}). Using available cash.")
            capital_to_allocate = available_capital
        
        if capital_to_allocate <= 1.0: # Minimum capital to allocate (e.g. $1)
            logger.info(f"Not enough capital to allocate for {symbol} based on fixed fraction ({capital_to_allocate:.2f}). Min trade value not met.")
            return None

        position_size = capital_to_allocate / current_price
        logger.info(f"FixedFractional Sizing for {symbol}: Allocating ${capital_to_allocate:.2f} (Equity: ${current_portfolio_value:.2f}, Fraction: {self.fraction_to_allocate}). Position Size: {position_size:.8f} units at ${current_price:.4f}.")
        return position_size

class ATRBasedPositionSizer(BasePositionSizer):
    """
    Sizes positions based on Average True Range (ATR) to normalize risk per trade.
    This implementation assumes you risk a fixed percentage of portfolio equity,
    and the stop loss is N * ATR away.
    """
    def __init__(self, risk_per_trade_fraction: float = 0.01, atr_multiple_for_stop: float = 2.0, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        # Params often from config: ATRBased_RiskPerTradeFraction, ATRBased_ATRMultipleForStop
        self.risk_per_trade_fraction = float(self.params.get('atrbased_riskpertradefraction', risk_per_trade_fraction))
        self.atr_multiple_for_stop = float(self.params.get('atrbased_atrmultipleforstop', atr_multiple_for_stop))
        logger.info(f"ATRBasedPositionSizer initialized with risk_fraction: {self.risk_per_trade_fraction}, atr_multiple: {self.atr_multiple_for_stop}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None # Allow passing pre-calculated ATR
    ) -> Optional[float]:
        
        effective_atr = atr_value
        if effective_atr is None and latest_bar and latest_bar.atr is not None:
            effective_atr = latest_bar.atr
        
        if effective_atr is None or effective_atr <= 1e-8: # Check for very small or zero ATR
            logger.warning(f"ATR value for {symbol} is missing, zero or invalid ({effective_atr}). Cannot size using ATRBasedPositionSizer.")
            return None
        
        if current_price <= 0:
            logger.warning(f"Current price for {symbol} is non-positive ({current_price}). Cannot size position.")
            return None
        if current_portfolio_value <= 0:
            logger.warning(f"Current portfolio value is non-positive ({current_portfolio_value}). Cannot size position.")
            return None

        capital_to_risk = current_portfolio_value * self.risk_per_trade_fraction
        
        stop_distance_per_unit = self.atr_multiple_for_stop * effective_atr
        if stop_distance_per_unit <= 1e-8: # Avoid division by zero or tiny stop
            logger.warning(f"Stop distance per unit is zero or too small for {symbol} (ATR: {effective_atr}, Multiple: {self.atr_multiple_for_stop}). Cannot size position.")
            return None
            
        position_size = capital_to_risk / stop_distance_per_unit
        
        position_cost = position_size * current_price
        if position_cost > available_capital:
            logger.warning(f"Calculated position cost (${position_cost:.2f}) for {symbol} exceeds available cash (${available_capital:.2f}). Reducing size to available cash.")
            position_size = available_capital / current_price 
            if position_size <= 1e-8 : return None # Ensure size is not effectively zero

        logger.info(f"ATRBased Sizing for {symbol}: Risking ${capital_to_risk:.2f} (Equity: ${current_portfolio_value:.2f}). "
                    f"ATR: {effective_atr:.6f}, StopDist: ${stop_distance_per_unit:.4f}. "
                    f"Calculated Size: {position_size:.8f} units at ${current_price:.4f}.")
        return position_size

class PairTradingPositionSizer(BasePositionSizer):
    """
    Sizes positions for a pair trade, aiming for dollar neutrality if configured.
    This sizer would return a tuple or dict with sizes for both legs.
    For now, let's make it return the size for ONE leg, assuming the strategy will call it twice
    or it's used in a context where dollar neutrality is applied to a total pair capital.
    A more advanced version would calculate sizes for both legs simultaneously.
    Simplified: calculate size for one leg based on total capital allocated to the pair.
    """
    def __init__(self, dollar_neutral: bool = True, fraction_of_equity_for_pair: float = 0.1, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.dollar_neutral = str(self.params.get('pairtradingpositionsizer_dollarneutral', dollar_neutral)).lower() == 'true'
        self.fraction_of_equity_for_pair = float(self.params.get('fraction_of_equity_for_pair', fraction_of_equity_for_pair))
        logger.info(f"PairTradingPositionSizer initialized. Dollar Neutral: {self.dollar_neutral}, Fraction for Pair: {self.fraction_of_equity_for_pair}")

    def calculate_size(
        self,
        symbol: str, # Symbol of the leg being sized
        current_price: float,
        available_capital: float, # Overall available cash
        current_portfolio_value: float, # Total equity
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None,
        # Additional params for pair trading
        other_leg_price: Optional[float] = None, # Price of the other asset in the pair
        hedge_ratio: Optional[float] = None # If sizing based on hedge ratio rather than pure dollar neutrality
    ) -> Optional[float]: # Returns size for the given 'symbol' leg
        
        if current_price <= 0: return None
        if current_portfolio_value <=0: return None

        # Capital allocated to the entire pair trade
        total_capital_for_pair_trade = current_portfolio_value * self.fraction_of_equity_for_pair

        if self.dollar_neutral:
            # Each leg gets half of the capital allocated to the pair trade.
            capital_for_this_leg = total_capital_for_pair_trade / 2.0
            
            # Ensure we don't allocate more than available cash for the whole pair (simplistic check)
            if total_capital_for_pair_trade > available_capital:
                logger.warning(f"Total capital for pair trade (${total_capital_for_pair_trade:.2f}) exceeds available cash (${available_capital:.2f}). Reducing allocation.")
                # This reduction needs to be applied carefully; for now, we'll cap this leg's capital based on a proportional reduction.
                # This is a rough adjustment. Proper handling involves checking margin and total cost of both legs.
                reduction_factor = available_capital / total_capital_for_pair_trade if total_capital_for_pair_trade > 0 else 0
                capital_for_this_leg *= reduction_factor

            if capital_for_this_leg <= 1.0: # Min capital for a leg
                logger.info(f"Not enough capital for leg {symbol} in pair trade ({capital_for_this_leg:.2f})")
                return None
            
            position_size = capital_for_this_leg / current_price
            logger.info(f"PairTrading Sizing (Dollar Neutral) for leg {symbol}: Capital for leg ${capital_for_this_leg:.2f}. Size: {position_size:.8f} units.")
            return position_size
        else:
            # Non-dollar neutral (e.g., based on hedge ratio or other logic) - complex, placeholder
            # This might involve the hedge_ratio to determine relative number of units.
            # For now, just implement a simple allocation for the one leg based on total pair capital.
            if total_capital_for_pair_trade > available_capital:
                 total_capital_for_pair_trade = available_capital # Cap at available cash

            if total_capital_for_pair_trade <= 1.0: return None # Min capital for the pair
            
            # This is naive if not dollar neutral and not using hedge ratio correctly.
            # Assume this leg takes its proportional share based on some other factor or fixed fraction.
            # For Phase 6, if not dollar neutral, it's underspecified here.
            # Let's assume it falls back to a simple fraction for this leg for now.
            capital_for_this_leg = total_capital_for_pair_trade # If only one leg is sized by this call.
            position_size = capital_for_this_leg / current_price
            logger.warning(f"PairTrading Sizing (Non-Dollar Neutral) for leg {symbol} is simplified. Allocating full pair capital portion ${capital_for_this_leg:.2f}. Size: {position_size:.8f} units.")
            return position_size


class OptimalFPositionSizer(BasePositionSizer):
    """
    Sizes positions based on Vince's Optimal f (Kelly Criterion variant).
    Requires win probability and payoff ratio, which are hard to estimate robustly.
    This is a placeholder for a more sophisticated implementation.
    """
    def __init__(self, win_probability: float = 0.55, payoff_ratio: float = 1.5, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.win_probability = float(self.params.get('optimalf_win_probability', win_probability))
        self.payoff_ratio = float(self.params.get('optimalf_payoff_ratio', payoff_ratio)) # AvgWin / AvgLoss
        if not (0 <= self.win_probability <= 1): raise ValueError("Win probability must be between 0 and 1.")
        if self.payoff_ratio <= 0: raise ValueError("Payoff ratio must be positive.")
        logger.info(f"OptimalFPositionSizer initialized. Win Prob: {self.win_probability}, Payoff Ratio: {self.payoff_ratio}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        if current_price <= 0 or current_portfolio_value <= 0: return None

        # Kelly Formula: f* = (W * P - L) / (W * L_ratio) where W=win prob, P=payoff, L=loss prob, L_ratio=payoff ratio
        # Simplified Kelly: f = W - ( (1-W) / R ) where W is win probability, R is payoff_ratio (AvgWin/AvgLoss)
        kelly_f = self.win_probability - ((1 - self.win_probability) / self.payoff_ratio)

        if kelly_f <= 0:
            logger.info(f"Optimal f ({kelly_f:.4f}) is zero or negative for {symbol}. No position taken.")
            return None
        
        # Use a fraction of Kelly (e.g., half Kelly) for risk reduction
        fractional_kelly = self.params.get('optimalf_kelly_fraction', 0.5) * kelly_f
        
        capital_to_allocate = current_portfolio_value * fractional_kelly
        
        if capital_to_allocate > available_capital:
            capital_to_allocate = available_capital
        if capital_to_allocate <= 1.0:
            logger.info(f"Not enough capital for {symbol} after Optimal F ({capital_to_allocate:.2f}).")
            return None

        position_size = capital_to_allocate / current_price
        logger.info(f"OptimalF Sizing for {symbol}: Kelly f*={kelly_f:.4f}, Using {fractional_kelly*100:.2f}% of equity. Allocating ${capital_to_allocate:.2f}. Size: {position_size:.8f} units.")
        return position_size


class MLConfidencePositionSizer(BasePositionSizer):
    """
    Sizes positions based on a base fraction of equity, modulated by ML model confidence.
    """
    def __init__(self, base_fraction: float = 0.05, min_alloc_fraction: float = 0.01, max_alloc_fraction: float = 0.2, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.base_fraction = float(self.params.get('mlconfidence_base_fraction', base_fraction))
        self.min_alloc_fraction = float(self.params.get('mlconfidence_min_alloc_fraction', min_alloc_fraction))
        self.max_alloc_fraction = float(self.params.get('mlconfidence_max_alloc_fraction', max_alloc_fraction))
        logger.info(f"MLConfidencePositionSizer: BaseFraction={self.base_fraction}, MinAlloc={self.min_alloc_fraction}, MaxAlloc={self.max_alloc_fraction}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        strategy_signal_strength: Optional[float] = None, # This is the ML confidence (0.0 to 1.0)
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        if current_price <= 0 or current_portfolio_value <= 0: return None
        
        if strategy_signal_strength is None:
            logger.warning("ML model confidence (strategy_signal_strength) not provided. Cannot use MLConfidencePositionSizer. Defaulting to no trade.")
            return None
        
        # Ensure confidence is within [0,1]
        confidence = max(0.0, min(1.0, strategy_signal_strength))
        
        # Modulate base fraction by confidence. Example: If confidence is high, use higher fraction.
        # Linear modulation: alloc_fraction = min_alloc + (max_alloc - min_alloc) * confidence
        # Or, scale base_fraction: alloc_fraction = base_fraction * (0.5 + confidence) # (scales from 0.5*base to 1.5*base)
        # Let's use a simpler direct scaling of base_fraction, capped by min/max.
        # Scaler: 0.5 (low conf) to 1.5 (high conf), centered at 1.0 for conf=0.5
        # confidence_scaler = 0.5 + confidence 
        # effective_fraction = self.base_fraction * confidence_scaler
        
        # More direct: if confidence is low (e.g. <0.5), use min_alloc. If high (e.g. >0.8), use max_alloc. Interpolate.
        # For simplicity, let's make it proportional to confidence, bounded by min/max overall allocation.
        # If confidence is 0.5, use base_fraction. If 1.0, use max_alloc. If 0.0 use min_alloc (or even zero).
        
        if confidence < 0.5: # Lower confidence scales down from base_fraction towards min_alloc_fraction
             # Interpolate between min_alloc_fraction and base_fraction
            # when confidence is 0, use min_alloc_fraction. when confidence is 0.5, use base_fraction.
            # slope = (base_fraction - min_alloc_fraction) / 0.5
            # effective_fraction = min_alloc_fraction + slope * confidence
            # Simpler: if confidence = 0 -> min_alloc, if confidence=0.5 -> base_fraction
            # scale_factor = confidence / 0.5 # confidence from 0 to 0.5 -> scale_factor from 0 to 1
            # effective_fraction = self.min_alloc_fraction + (self.base_fraction - self.min_alloc_fraction) * scale_factor
            # This is still a bit complex. Alternative:
            effective_fraction = self.base_fraction * (confidence * 1.5) # Scales from 0 to 0.75 * base_fraction for conf 0 to 0.5
        else: # Higher confidence scales up from base_fraction towards max_alloc_fraction
            # Interpolate between base_fraction and max_alloc_fraction
            # when confidence is 0.5, use base_fraction. when confidence is 1.0, use max_alloc_fraction
            # slope = (max_alloc_fraction - base_fraction) / 0.5
            # effective_fraction = base_fraction + slope * (confidence - 0.5)
             effective_fraction = self.base_fraction * (0.75 + (confidence-0.5)*1.5) # Scales from 0.75*base to 1.5*base for conf 0.5 to 1.0


        effective_fraction = np.clip(effective_fraction, self.min_alloc_fraction, self.max_alloc_fraction)

        capital_to_allocate = current_portfolio_value * effective_fraction
        
        if capital_to_allocate > available_capital:
            capital_to_allocate = available_capital
        if capital_to_allocate <= 1.0:
            logger.info(f"Not enough capital for {symbol} using ML confidence ({capital_to_allocate:.2f}). Confidence: {confidence:.2f}, Eff.Frac: {effective_fraction:.4f}")
            return None
            
        position_size = capital_to_allocate / current_price
        logger.info(f"MLConfidence Sizing for {symbol}: Confidence={confidence:.2f}, Eff.Alloc.Frac={effective_fraction:.4f}. Allocating ${capital_to_allocate:.2f}. Size: {position_size:.8f} units.")
        return position_size
</code>

kamikaze_komodo/strategy_framework/base_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/base_strategy.py
# Updated to include optional sentiment_score in on_bar_data
# Updated update_data_history for new BarData fields
# Phase 6: Added market_regime to BarData and data_history.
# Phase 6: Added enable_shorting parameter.
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Union # Added List, Union
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
import pandas as pd
from pydantic import BaseModel # Ensure pydantic.BaseModel is imported



logger = get_logger(__name__)

class SignalCommand(BaseModel):
    signal_type: SignalType
    symbol: str
    price: Optional[float] = None
    # Add amount if strategy determines it, otherwise position sizer will.
    # amount: Optional[float] = None 
    related_bar_data: Optional[BarData] = None
    # For pair trades, might include specific instructions for each leg
    custom_params: Optional[Dict[str, Any]] = None

class BaseStrategy(ABC):
    """
    Abstract base class for all trading strategies.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        self.symbol = symbol # Primary symbol for single-asset strategies or one leg of a pair
        self.timeframe = timeframe
        self.params = params if params is not None else {}
        self.current_position_status: Optional[SignalType] = None # Tracks if currently LONG, SHORT or None (no position)
    
        # Phase 6: Enable shorting based on strategy parameters
        self.enable_shorting: bool = self.params.get('enableshorting', False) # Default to False if not specified
        if isinstance(self.enable_shorting, str): # Handle string 'True'/'False' from config
            self.enable_shorting = self.enable_shorting.lower() == 'true'

        # Initialize data_history with potential columns including new ones from BarData
        self.data_history = pd.DataFrame(columns=[
            'open', 'high', 'low', 'close', 'volume',
            'atr', 'sentiment_score',
            'prediction_value', 'prediction_confidence', # New Phase 5 fields
            'market_regime' # New Phase 6 field
        ])
        logger.info(f"Initialized BaseStrategy '{self.name}' for {symbol} ({timeframe}) with params: {self.params}. Shorting enabled: {self.enable_shorting}")

    @abstractmethod
    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        """
        Generates trading signals based on the provided historical data.
        This method is typically called once during backtesting setup or for historical analysis.
        Args:
            data (pd.DataFrame): DataFrame with historical OHLCV data, indexed by timestamp.
                                Expected columns: 'open', 'high', 'low', 'close', 'volume'.
                                May also contain 'atr', 'sentiment_score', 'prediction_value',
                                'prediction_confidence', 'market_regime'.
            sentiment_series (Optional[pd.Series]): Series with historical sentiment scores, indexed by timestamp.
        Returns:
            pd.Series: A Pandas Series indexed by timestamp, containing SignalType values.
        """
        pass

    @abstractmethod
    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        """
        Processes a new bar of data and decides on a trading action.
        This method is typically called for each new data point in a live or simulated environment.
        Can return a single SignalType or a list of SignalCommands for multi-leg strategies.
        Args:
            bar_data (BarData): The new BarData object, potentially with .atr or .sentiment_score,
                                .prediction_value, .prediction_confidence, .market_regime populated.
            sentiment_score (Optional[float]): External sentiment score for the current bar.
                                            (Note: bar_data.sentiment_score might also be used if populated by engine)
            market_regime_data (Optional[Any]): External market regime data for current bar.
                                                (Note: bar_data.market_regime might also be used if populated by engine)
        Returns:
            Union[Optional[SignalType], List[SignalCommand]]:
                - A single signal (LONG, SHORT, HOLD, CLOSE_LONG, CLOSE_SHORT) or None if no action.
                - A list of SignalCommand objects for multi-leg trades (e.g., pair trading).
        """
        pass
    
    def update_data_history(self, new_bar_data: BarData):
        """Appends new bar data to the internal history including ATR, sentiment, prediction, and regime fields if available."""
        new_timestamp = new_bar_data.timestamp
        new_row_data = {
            'open': new_bar_data.open, 'high': new_bar_data.high,
            'low': new_bar_data.low, 'close': new_bar_data.close,
            'volume': new_bar_data.volume,
            'atr': new_bar_data.atr,
            'sentiment_score': new_bar_data.sentiment_score,
            'prediction_value': new_bar_data.prediction_value,
            'prediction_confidence': new_bar_data.prediction_confidence,
            'market_regime': new_bar_data.market_regime,
        }
        # Use .loc to append the new row, which handles dtypes better and avoids FutureWarnings
        for col, value in new_row_data.items():
            if col in self.data_history.columns:
                self.data_history.loc[new_timestamp, col] = value


    def get_parameters(self) -> Dict[str, Any]:
        return self.params

    def set_parameters(self, params: Dict[str, Any]):
        self.params.update(params)
        # Update shorting capability if specified in new params
        if 'enableshorting' in self.params:
            self.enable_shorting = str(self.params['enableshorting']).lower() == 'true'
        logger.info(f"Strategy {self.__class__.__name__} parameters updated: {self.params}. Shorting enabled: {self.enable_shorting}")

    @property
    def name(self) -> str:
        return self.__class__.__name__

# Add Pydantic BaseModel for SignalCommand if not already defined elsewhere (e.g., in core.models if broadly used)
# For now, defining it here for clarity in BaseStrategy context.
</code>

kamikaze_komodo/strategy_framework/__init__.py:
<code>
# kamikaze_komodo/strategy_framework/__init__.py
# This file makes the 'strategy_framework' directory a Python package.
</code>

kamikaze_komodo/strategy_framework/strategy_manager.py:
<code>
# kamikaze_komodo/strategy_framework/strategy_manager.py
from typing import List, Dict, Any
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.app_logger import get_logger
logger = get_logger(__name__)
class StrategyManager:
    """
    Manages the loading, initialization, and execution of trading strategies.
    """
    def __init__(self):
        self.strategies: List[BaseStrategy] = []
        logger.info("StrategyManager initialized.")
    def add_strategy(self, strategy: BaseStrategy):
        """Adds a strategy instance to the manager."""
        if not isinstance(strategy, BaseStrategy):
            logger.error("Attempted to add an invalid strategy object.")
            raise ValueError("Strategy must be an instance of BaseStrategy.")
        
        self.strategies.append(strategy)
        logger.info(f"Strategy '{strategy.name}' for {strategy.symbol} ({strategy.timeframe}) added to StrategyManager.")
    def remove_strategy(self, strategy_name: str, symbol: str, timeframe: str):
        """Removes a strategy by its name, symbol, and timeframe."""
        initial_count = len(self.strategies)
        self.strategies = [
            s for s in self.strategies 
            if not (s.name == strategy_name and s.symbol == symbol and s.timeframe == timeframe)
        ]
        if len(self.strategies) < initial_count:
            logger.info(f"Strategy '{strategy_name}' for {symbol} ({timeframe}) removed.")
        else:
            logger.warning(f"Strategy '{strategy_name}' for {symbol} ({timeframe}) not found for removal.")
    def load_strategies_from_config(self, config: Dict[str, Any]):
        """
        Loads strategies based on a configuration dictionary.
        This is a placeholder for a more dynamic loading mechanism.
        For now, strategies are added manually or via specific calls.
        """
        # Example:
        # for strategy_config in config.get('strategies', []):
        #     strategy_class = resolve_strategy_class(strategy_config['name']) # Utility to get class from name
        #     params = strategy_config.get('params', {})
        #     symbol = strategy_config.get('symbol')
        #     timeframe = strategy_config.get('timeframe')
        #     if strategy_class and symbol and timeframe:
        #         self.add_strategy(strategy_class(symbol, timeframe, params))
        logger.warning("load_strategies_from_config is a placeholder and not fully implemented.")
        pass
    def on_bar_data_all(self, bar_data: BarData) -> Dict[str, SignalType]:
        """
        Distributes new bar data to all relevant strategies and collects signals.
        A strategy is relevant if the bar_data.symbol and bar_data.timeframe match.
        Returns:
            Dict[str, SignalType]: A dictionary where keys are strategy identifiers
                                   (e.g., "EWMACStrategy_BTC/USD_1h") and values are signals.
        """
        signals_from_strategies: Dict[str, SignalType] = {}
        for strategy in self.strategies:
            if strategy.symbol == bar_data.symbol and strategy.timeframe == bar_data.timeframe:
                signal = strategy.on_bar_data(bar_data)
                if signal: # Only record actual signals, not None or HOLD if not meaningful here
                    strategy_id = f"{strategy.name}_{strategy.symbol.replace('/', '')}_{strategy.timeframe}"
                    signals_from_strategies[strategy_id] = signal
                    logger.debug(f"Signal from {strategy_id}: {signal.name}")
        return signals_from_strategies
    def get_all_strategies(self) -> List[BaseStrategy]:
        return self.strategies
# Example Usage (Conceptual)
if __name__ == '__main__':
    from kamikaze_komodo.strategy_framework.strategies.ewmac import EWMACStrategy
    from kamikaze_komodo.config.settings import settings # Assuming settings are loaded
    if settings:
        manager = StrategyManager()
        
        # Create and add a strategy instance
        ewmac_params = {
            'short_window': settings.ewmac_short_window,
            'long_window': settings.ewmac_long_window
        }
        ewmac_btc_1h = EWMACStrategy(symbol="BTC/USD", timeframe="1h", params=ewmac_params)
        manager.add_strategy(ewmac_btc_1h)
        # Simulate receiving bar data
        # In a real system, this BarData would come from DataFetcher
        from datetime import datetime, timezone
        example_bar = BarData(
            timestamp=datetime.now(timezone.utc),
            open=40000, high=40500, low=39800, close=40200, volume=100,
            symbol="BTC/USD", timeframe="1h"
        )
        # To actually get a signal, the strategy needs historical data first.
        # This is a simplified call. `ewmac_btc_1h.update_data_history(bar)` would need to be called many times first.
        # For a single bar without history, it will likely return HOLD or an error if not enough data.
        # signals = manager.on_bar_data_all(example_bar)
        # logger.info(f"Signals received: {signals}")
        logger.info("StrategyManager example completed. For meaningful signals, strategies need historical data.")
    else:
        logger.error("Settings not loaded, cannot run StrategyManager example.")
</code>

kamikaze_komodo/strategy_framework/strategies/funding_rate_arb_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/funding_rate_arb_strategy.py
import pandas as pd
from typing import Dict, Any, Optional, Union, List

from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class FundingRateArbStrategy(BaseStrategy):
    """
    A strategy that aims to collect funding payments from perpetual futures.
    This is a simplified version that only shorts the perpetual future when the
    funding rate is positive and high, implying that longs are paying shorts.
    It does not execute the spot leg, as the backtester currently does not
    support multi-exchange or multi-asset-type (spot vs. future) positions
    within a single strategy. The PnL will come from funding payments (simulated
    in the backtester) and the price movement of the future itself.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.entry_threshold = float(self.params.get('entry_threshold', 0.0001))  # e.g., enter short if funding is > 0.01%
        self.exit_threshold = float(self.params.get('exit_threshold', 0.00005)) # e.g., exit if funding drops below 0.005%

        logger.info(
            f"Initialized FundingRateArbStrategy for {symbol} ({timeframe}) "
            f"with Entry Threshold: {self.entry_threshold:.5f} and Exit Threshold: {self.exit_threshold:.5f}. "
            f"This strategy requires shorting to be enabled."
        )
        if not self.enable_shorting:
            logger.error(f"FundingRateArbStrategy requires 'enableshorting = True' but it is False.")

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        # This strategy's logic is primarily bar-by-bar and depends on the funding_rate field.
        logger.warning("generate_signals is not the primary method for FundingRateArbStrategy; logic is in on_bar_data.")
        return pd.Series(index=data.index, dtype='object').fillna(SignalType.HOLD)

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(bar_data)

        funding_rate = bar_data.funding_rate
        if funding_rate is None:
            # If funding rate is not available on this bar, we cannot make a decision.
            # In a real system, you might hold or use the last known rate. Here we'll hold.
            return SignalType.HOLD

        signal_to_return = SignalType.HOLD

        # Entry logic: If not in a position and funding is high, enter a short to collect payments.
        if self.current_position_status is None:
            if funding_rate > self.entry_threshold and self.enable_shorting:
                signal_to_return = SignalType.SHORT
                self.current_position_status = SignalType.SHORT
                logger.info(f"{bar_data.timestamp} - FundingRateArb SHORT for {self.symbol}. Rate: {funding_rate:.5f} > Entry: {self.entry_threshold:.5f}")

        # Exit logic: If in a short position and funding rate drops, close the position.
        elif self.current_position_status == SignalType.SHORT:
            if funding_rate < self.exit_threshold:
                signal_to_return = SignalType.CLOSE_SHORT
                self.current_position_status = None
                logger.info(f"{bar_data.timestamp} - FundingRateArb CLOSE_SHORT for {self.symbol}. Rate: {funding_rate:.5f} < Exit: {self.exit_threshold:.5f}")
        
        return signal_to_return
</code>

kamikaze_komodo/strategy_framework/strategies/ml_forecaster_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/ml_forecaster_strategy.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
import os
import joblib
import numpy as np
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class MLForecasterStrategy(BaseStrategy):
    """
    A strategy that uses a primary model to predict side and a secondary
    "meta-model" to predict the probability of success (confidence).
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.primary_model = None
        self.meta_model = None
        
        # Load the pre-trained primary and meta models
        base_path = os.path.join(PROJECT_ROOT, "ml_models/trained_models")
        primary_model_path = os.path.join(base_path, f"primary_{symbol.replace('/', '_')}.joblib")
        meta_model_path = os.path.join(base_path, f"meta_{symbol.replace('/', '_')}.joblib")

        try:
            if os.path.exists(primary_model_path):
                self.primary_model = joblib.load(primary_model_path)
                logger.info(f"Primary model loaded for {symbol} from {primary_model_path}")
            else:
                logger.error(f"Primary model not found at {primary_model_path}")

            if os.path.exists(meta_model_path):
                self.meta_model = joblib.load(meta_model_path)
                logger.info(f"Meta model loaded for {symbol} from {meta_model_path}")
            else:
                logger.error(f"Meta model not found at {meta_model_path}")
        except Exception as e:
            logger.error(f"Error loading ML models for {symbol}: {e}", exc_info=True)
            self.primary_model = self.meta_model = None
            
        self.long_confidence_threshold = float(self.params.get('longconfidencethreshold', 0.55))
        self.short_confidence_threshold = float(self.params.get('shortconfidencethreshold', 0.55))


    def _create_features(self, data_df: pd.DataFrame) -> pd.DataFrame:
        df = data_df.copy()
        for lag in [1, 3, 5, 10]:
            df[f'log_return_lag_{lag}'] = np.log(df['close'] / df['close'].shift(lag))
        df['volatility_20'] = df['log_return_lag_1'].rolling(window=20).std()
        
        # Add ATR for risk management modules
        if all(col in df.columns for col in ['high', 'low', 'close']):
            atr_period = int(self.params.get('atr_period', 14))
            if len(df) >= atr_period:
                df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=atr_period)
        
        return df.dropna()

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        logger.warning("generate_signals is not implemented for this real-time focused ML strategy.")
        return pd.Series(index=data.index, dtype='object').fillna(SignalType.HOLD)

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(bar_data)
        
        if self.primary_model is None or self.meta_model is None or len(self.data_history) < 50:
            return SignalType.HOLD

        features_df = self._create_features(self.data_history)
        if features_df.empty: return SignalType.HOLD
            
        latest_features = features_df.iloc[[-1]] # Keep as DataFrame

        # 1. Get side prediction from primary model
        side_prediction = self.primary_model.predict(latest_features)[0]
        primary_pred_proba = self.primary_model.predict_proba(latest_features)[0, 1]

        # 2. Get confidence from meta model
        meta_features = pd.DataFrame({'primary_pred_proba': [primary_pred_proba]})
        confidence = self.meta_model.predict_proba(meta_features)[0, 1] # Probability of class 1 (win)
        
        bar_data.prediction_confidence = confidence

        # 3. Generate Signal based on side and confidence
        signal_to_return = SignalType.HOLD
        
        if side_prediction == 1 and confidence > self.long_confidence_threshold:
            if self.current_position_status != SignalType.LONG:
                signal_to_return = SignalType.LONG
                self.current_position_status = SignalType.LONG
        elif side_prediction == -1 and confidence > self.short_confidence_threshold:
            if self.current_position_status != SignalType.SHORT and self.enable_shorting:
                signal_to_return = SignalType.SHORT
                self.current_position_status = SignalType.SHORT
        else: # Close position if confidence drops or side flips
            if self.current_position_status == SignalType.LONG:
                signal_to_return = SignalType.CLOSE_LONG
                self.current_position_status = None
            elif self.current_position_status == SignalType.SHORT:
                signal_to_return = SignalType.CLOSE_SHORT
                self.current_position_status = None

        logger.info(f"{bar_data.timestamp} - MLMeta Signal: {signal_to_return.value}, Side Pred: {side_prediction}, Confidence: {confidence:.2f}")

        return signal_to_return
</code>

kamikaze_komodo/strategy_framework/strategies/ehlers_instantaneous_trendline.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/ehlers_instantaneous_trendline.py
import pandas as pd
import pandas_ta as ta
import numpy as np
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class EhlersInstantaneousTrendlineStrategy(BaseStrategy):
    """
    Implements a stateless Ehlers' Instantaneous Trendline strategy.
    It generates entry signals only. Exits are handled by the StopManager.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        self.it_lag_trigger = int(self.params.get('it_lag_trigger', 1))
        self.atr_period = int(self.params.get('atr_period', 14))

        logger.info(
            f"Initialized EhlersInstantaneousTrendlineStrategy for {symbol} ({timeframe}) "
            f"with IT Lag Trigger: {self.it_lag_trigger}. Shorting Enabled: {self.enable_shorting}"
        )

    def _calculate_indicators(self, data_df: pd.DataFrame) -> pd.DataFrame:
        if data_df.empty or 'close' not in data_df.columns or len(data_df) < 3:
            return pd.DataFrame()

        df = data_df.copy()
        close_prices = df['close']
        # The core IT is a 3-bar filter, but we apply it over the series
        it_values = (close_prices + 2 * close_prices.shift(1) + close_prices.shift(2)) / 4
        df['it'] = it_values
        df['it_trigger'] = df['it'].shift(self.it_lag_trigger)
        
        if all(col in df.columns for col in ['high', 'low', 'close']):
            if len(df) >= self.atr_period:
                df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)
        return df

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        logger.warning("generate_signals is not the primary method for this strategy; logic is in on_bar_data.")
        return pd.Series(index=data.index, dtype='object').fillna(SignalType.HOLD)

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(bar_data)
        if len(self.data_history) < 3 + self.it_lag_trigger:
            return SignalType.HOLD

        df = self._calculate_indicators(self.data_history)
        if df.empty or len(df) < 2 or 'it' not in df.columns or 'it_trigger' not in df.columns:
            return SignalType.HOLD
            
        if 'atr' in df.columns and pd.notna(df['atr'].iloc[-1]):
            bar_data.atr = df['atr'].iloc[-1]

        latest_it = df['it'].iloc[-1]
        prev_it = df['it'].iloc[-2]
        latest_it_trigger = df['it_trigger'].iloc[-1]
        prev_it_trigger = df['it_trigger'].iloc[-2]

        if any(pd.isna(v) for v in [latest_it, prev_it, latest_it_trigger, prev_it_trigger]):
            return SignalType.HOLD

        is_bullish_cross = latest_it > latest_it_trigger and prev_it <= prev_it_trigger
        is_bearish_cross = latest_it < latest_it_trigger and prev_it >= prev_it_trigger

        if is_bullish_cross:
            return SignalType.LONG
        elif is_bearish_cross and self.enable_shorting:
            return SignalType.SHORT
        
        return SignalType.HOLD
</code>

kamikaze_komodo/strategy_framework/strategies/__init__.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/__init__.py
# This file makes the 'strategies' directory a Python package.
</code>

kamikaze_komodo/strategy_framework/strategies/bollinger_band_breakout_strategy.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/bollinger_band_breakout_strategy.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BollingerBandBreakoutStrategy(BaseStrategy):
    """
    Implements a stateless Bollinger Band Breakout strategy.
    Generates entry signals only. Exits are handled by the StopManager.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.bb_period = int(self.params.get('bb_period', 20))
        self.bb_std_dev = float(self.params.get('bb_std_dev', 2.0))
        self.atr_period = int(self.params.get('atr_period', 14))

        logger.info(
            f"Initialized BollingerBandBreakoutStrategy for {symbol} ({timeframe}) "
            f"with BB Period: {self.bb_period}, StdDev: {self.bb_std_dev}. "
            f"Shorting Enabled: {self.enable_shorting}"
        )

    def _calculate_indicators(self, data_df: pd.DataFrame) -> pd.DataFrame:
        if data_df.empty or len(data_df) < self.bb_period:
            return pd.DataFrame()

        df = data_df.copy()
        try:
            bbands = ta.bbands(df['close'], length=self.bb_period, std=self.bb_std_dev)
            if bbands is not None and not bbands.empty:
                df['bb_lower'] = bbands[f'BBL_{self.bb_period}_{self.bb_std_dev:.1f}']
                df['bb_upper'] = bbands[f'BBU_{self.bb_period}_{self.bb_std_dev:.1f}']

            if all(col in df.columns for col in ['high', 'low', 'close']):
                if len(df) >= self.atr_period:
                    df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)
            return df
        except Exception as e:
            logger.error(f"Error calculating BBands indicators: {e}")
            return pd.DataFrame()

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        logger.warning("generate_signals is not the primary method for this strategy; logic is in on_bar_data.")
        return pd.Series(index=data.index, dtype='object').fillna(SignalType.HOLD)

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(bar_data)
        if len(self.data_history) < self.bb_period + 1:
            return SignalType.HOLD

        df = self._calculate_indicators(self.data_history)
        if df.empty or len(df) < 2 or 'bb_upper' not in df.columns or 'bb_lower' not in df.columns:
            return SignalType.HOLD

        if 'atr' in df.columns and pd.notna(df['atr'].iloc[-1]):
            bar_data.atr = df['atr'].iloc[-1]

        latest_close = df['close'].iloc[-1]
        prev_close = df['close'].iloc[-2]
        latest_bb_upper = df['bb_upper'].iloc[-1]
        prev_bb_upper = df['bb_upper'].iloc[-2]
        latest_bb_lower = df['bb_lower'].iloc[-1]
        prev_bb_lower = df['bb_lower'].iloc[-2]

        if any(pd.isna(v) for v in [latest_close, prev_close, latest_bb_upper, prev_bb_upper, latest_bb_lower, prev_bb_lower]):
            return SignalType.HOLD

        long_breakout = latest_close > latest_bb_upper and prev_close <= prev_bb_upper
        short_breakout = latest_close < latest_bb_lower and prev_close >= prev_bb_lower

        if long_breakout:
            return SignalType.LONG
        elif short_breakout and self.enable_shorting:
            return SignalType.SHORT

        return SignalType.HOLD
</code>

kamikaze_komodo/strategy_framework/strategies/pair_trading_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/pair_trading_strategy.py
import pandas as pd
import pandas_ta as ta
import statsmodels.api as sm # For cointegration test
from statsmodels.tsa.stattools import adfuller # For stationarity test on spread
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType, OrderSide
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher # For fetching secondary asset data
from kamikaze_komodo.config.settings import settings as app_settings
from datetime import datetime, timedelta, timezone

logger = get_logger(__name__)

class PairTradingStrategy(BaseStrategy):
    """
    Implements a Pair Trading strategy based on cointegration.
    The 'symbol' parameter in __init__ will be considered asset1.
    Asset2 symbol must be provided in params.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params) # symbol is asset1
        
        self.asset1_symbol = symbol
        self.asset2_symbol = self.params.get('asset2_symbol')
        if not self.asset2_symbol:
            raise ValueError("PairTradingStrategy requires 'asset2_symbol' in params.")

        self.cointegration_lookback_days = int(self.params.get('cointegration_lookback_days', 90))
        self.cointegration_test_pvalue_threshold = float(self.params.get('cointegration_test_pvalue_threshold', 0.05))
        self.spread_zscore_entry_threshold = float(self.params.get('spread_zscore_entry_threshold', 2.0))
        self.spread_zscore_exit_threshold = float(self.params.get('spread_zscore_exit_threshold', 0.5))
        self.spread_calculation_window = int(self.params.get('spread_calculation_window', 20)) # For MA and StdDev of spread

        self.data_history_asset2 = pd.DataFrame(columns=['open', 'high', 'low', 'close', 'volume', 'atr'])
        self.is_cointegrated = False
        self.hedge_ratio: Optional[float] = None # From cointegration regression

        # Internal state for an active pair trade
        self.active_pair_trade_leg1_symbol: Optional[str] = None
        self.active_pair_trade_leg2_symbol: Optional[str] = None
        self.active_pair_trade_direction: Optional[str] = None # "long_spread" or "short_spread"

        logger.info(
            f"Initialized PairTradingStrategy for {self.asset1_symbol} / {self.asset2_symbol} ({timeframe}) "
            f"Cointegration Lookback: {self.cointegration_lookback_days} days, p-value: {self.cointegration_test_pvalue_threshold}. "
            f"Z-Score Entry: {self.spread_zscore_entry_threshold}, Exit: {self.spread_zscore_exit_threshold}. "
            f"Spread Window: {self.spread_calculation_window}. Shorting Enabled: {self.enable_shorting}"
        )
        # Note: self.enable_shorting must be true for pair trading to function correctly.
        if not self.enable_shorting:
            logger.warning(f"PairTradingStrategy for {self.asset1_symbol}/{self.asset2_symbol} has enable_shorting=False. This strategy requires shorting for one leg.")


    async def initialize_strategy_data(self, historical_data_asset1: pd.DataFrame, historical_data_asset2: pd.DataFrame):
        """
        Checks for cointegration using pre-fetched historical data.
        This method is called once at the start by the runner.
        """
        if historical_data_asset1.empty or historical_data_asset2.empty:
            logger.error("PairTradingStrategy received empty historical data for one or both assets.")
            self.is_cointegrated = False
            return

        # Store full history for later indicator calculation on individual assets
        self.data_history = historical_data_asset1.copy()
        self.data_history_asset2 = historical_data_asset2.copy()
        
        # FIX: Merge based on the index since 'timestamp' was set as the index.
        merged_df = pd.merge(
            self.data_history[['close']], 
            self.data_history_asset2[['close']], 
            left_index=True, 
            right_index=True, 
            how='inner', 
            suffixes=('_asset1', '_asset2')
        )
        merged_df.dropna(inplace=True)

        if len(merged_df) < self.spread_calculation_window * 2: # Need enough data
            logger.warning(f"Not enough synchronized historical data for {self.asset1_symbol} and {self.asset2_symbol} for cointegration analysis (found {len(merged_df)} bars).")
            self.is_cointegrated = False
            return
        
        # Check for cointegration using Engle-Granger
        close_asset1 = merged_df['close_asset1']
        close_asset2 = merged_df['close_asset2']

        # OLS regression: asset1 = hedge_ratio * asset2 + const
        model = sm.OLS(close_asset1, sm.add_constant(close_asset2, prepend=True))
        results = model.fit()
        # FIX: Use .iloc for explicit positional access to fix FutureWarning
        self.hedge_ratio = results.params.iloc[1]

        spread = close_asset1 - self.hedge_ratio * close_asset2
        
        # ADF test on the spread to check for stationarity
        adf_result = adfuller(spread.dropna())
        p_value = adf_result[1]

        if p_value < self.cointegration_test_pvalue_threshold:
            self.is_cointegrated = True
            logger.info(f"Pair {self.asset1_symbol}/{self.asset2_symbol} IS cointegrated. ADF p-value: {p_value:.4f}, Hedge Ratio: {self.hedge_ratio:.4f}")
        else:
            self.is_cointegrated = False
            self.hedge_ratio = None # Reset if not cointegrated
            logger.info(f"Pair {self.asset1_symbol}/{self.asset2_symbol} is NOT cointegrated. ADF p-value: {p_value:.4f}")
        
        return


    def _calculate_spread_zscore(self) -> Optional[float]:
        """Calculates the Z-score of the current spread."""
        if self.hedge_ratio is None or len(self.data_history) < self.spread_calculation_window or len(self.data_history_asset2) < self.spread_calculation_window:
            return None
            
        # Ensure both histories have the latest timestamp available
        last_ts_asset1 = self.data_history.index[-1]
        if last_ts_asset1 not in self.data_history_asset2.index:
            logger.debug(f"Latest timestamp {last_ts_asset1} for {self.asset1_symbol} not in {self.asset2_symbol} history. Cannot calculate current spread.")
            return None
            
        close1 = self.data_history['close'].loc[last_ts_asset1]
        close2 = self.data_history_asset2['close'].loc[last_ts_asset1]

        if pd.isna(close1) or pd.isna(close2): return None

        # Calculate historical spread for Z-score
        hist_close1 = self.data_history['close']
        hist_close2 = self.data_history_asset2['close']
        
        merged_closes = pd.merge(hist_close1.rename('c1'), hist_close2.rename('c2'), left_index=True, right_index=True, how='inner')
        if len(merged_closes) < self.spread_calculation_window: return None

        historical_spread = merged_closes['c1'] - self.hedge_ratio * merged_closes['c2']
        
        if len(historical_spread) < self.spread_calculation_window:
            return None
            
        spread_mean = historical_spread.rolling(window=self.spread_calculation_window).mean().iloc[-1]
        spread_std = historical_spread.rolling(window=self.spread_calculation_window).std().iloc[-1]

        if pd.isna(spread_mean) or pd.isna(spread_std) or spread_std == 0:
            return None
            
        current_spread = close1 - self.hedge_ratio * close2
        z_score = (current_spread - spread_mean) / spread_std
        logger.debug(f"Current Spread for {self.asset1_symbol}/{self.asset2_symbol}: {current_spread:.4f}, Mean: {spread_mean:.4f}, Std: {spread_std:.4f}, Z-Score: {z_score:.2f}")
        return z_score

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        # Pair trading decisions are made bar-by-bar based on spread Z-score.
        # This batch method is less suitable. Primary logic will be in on_bar_data.
        logger.warning("generate_signals is not the primary method for PairTradingStrategy; logic is in on_bar_data.")
        return pd.Series(index=data.index, dtype='object').fillna(SignalType.HOLD)


    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        # This strategy relies on the backtest engine to manage and align data for both assets.
        # It updates its internal history for asset1 (self.symbol) when its bar data arrives.
        # The backtest engine must provide the history for asset2 via its `data_feed_df_pair_asset2` parameter.
        if bar_data.symbol == self.asset1_symbol:
            self.update_data_history(bar_data)
        else:
            # This strategy instance should only process data for its primary symbol.
            # The engine is responsible for passing the right data.
            return SignalType.HOLD

        if not self.is_cointegrated or self.hedge_ratio is None:
            return SignalType.HOLD

        current_z_score = self._calculate_spread_zscore()
        if current_z_score is None:
            return SignalType.HOLD

        signals_to_execute: List[SignalCommand] = []

        # Exit logic first
        if self.current_position_status == SignalType.LONG: # Means we are long the spread (Long Asset1, Short Asset2)
            if current_z_score >= self.spread_zscore_exit_threshold:
                logger.info(f"Exiting LONG SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                # Backtest engine needs to get price for asset2 to close
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_LONG, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_SHORT, symbol=self.asset2_symbol))
                self.current_position_status = None
                return signals_to_execute

        elif self.current_position_status == SignalType.SHORT: # Means we are short the spread (Short Asset1, Long Asset2)
            if current_z_score <= -self.spread_zscore_exit_threshold:
                logger.info(f"Exiting SHORT SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_SHORT, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_LONG, symbol=self.asset2_symbol))
                self.current_position_status = None
                return signals_to_execute
        
        # Entry logic if no active pair trade
        if self.current_position_status is None:
            # Entry condition: Long the spread (Asset1 Long, Asset2 Short) if Z-score is very low
            if current_z_score < -self.spread_zscore_entry_threshold:
                logger.info(f"Entering LONG SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                signals_to_execute.append(SignalCommand(signal_type=SignalType.LONG, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.SHORT, symbol=self.asset2_symbol))
                self.current_position_status = SignalType.LONG # Representing "long the spread"
                return signals_to_execute

            # Entry condition: Short the spread (Asset1 Short, Asset2 Long) if Z-score is very high
            elif current_z_score > self.spread_zscore_entry_threshold:
                logger.info(f"Entering SHORT SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                signals_to_execute.append(SignalCommand(signal_type=SignalType.SHORT, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.LONG, symbol=self.asset2_symbol))
                self.current_position_status = SignalType.SHORT # Representing "short the spread"
                return signals_to_execute

        return SignalType.HOLD
</code>

kamikaze_komodo/strategy_framework/strategies/ewmac.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/ewmac.py
import pandas as pd
import pandas_ta as ta
import numpy as np
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class EWMACStrategy(BaseStrategy):
    """
    Implements a stateless Exponential Weighted Moving Average Crossover (EWMAC) strategy.
    Generates entry signals only. Exits are handled by the StopManager.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        self.short_window = int(self.params.get('shortwindow', 12))
        self.long_window = int(self.params.get('longwindow', 26))
        self.atr_period = int(self.params.get('atr_period', 14))
        
        logger.info(
            f"Initialized EWMACStrategy for {symbol} ({timeframe}) "
            f"with Short EMA: {self.short_window}, Long EMA: {self.long_window}. "
            f"Shorting Enabled: {self.enable_shorting}."
        )

    def _calculate_indicators(self, data_df: pd.DataFrame) -> pd.DataFrame:
        if data_df.empty or len(data_df) < self.long_window :
            return pd.DataFrame()
        df = data_df.copy()
        df[f'ema_short'] = ta.ema(df['close'], length=self.short_window)
        df[f'ema_long'] = ta.ema(df['close'], length=self.long_window)
        if all(col in df.columns for col in ['high', 'low', 'close']):
            if len(df) >= self.atr_period:
                df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)
        return df

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        logger.warning("generate_signals is not the primary method for this strategy; logic is in on_bar_data.")
        return pd.Series(index=data.index, dtype='object').fillna(SignalType.HOLD)

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(bar_data)
        if len(self.data_history) < self.long_window + 1:
            return SignalType.HOLD

        df = self._calculate_indicators(self.data_history)
        if df.empty or len(df) < 2 or 'ema_short' not in df.columns or 'ema_long' not in df.columns:
            return SignalType.HOLD

        if 'atr' in df.columns and pd.notna(df['atr'].iloc[-1]):
            bar_data.atr = df['atr'].iloc[-1]

        latest_ema_short = df['ema_short'].iloc[-1]
        prev_ema_short = df['ema_short'].iloc[-2]
        latest_ema_long = df['ema_long'].iloc[-1]
        prev_ema_long = df['ema_long'].iloc[-2]

        if any(pd.isna(v) for v in [latest_ema_short, prev_ema_short, latest_ema_long, prev_ema_long]):
            return SignalType.HOLD

        is_golden_cross = latest_ema_short > latest_ema_long and prev_ema_short <= prev_ema_long
        is_death_cross = latest_ema_short < latest_ema_long and prev_ema_short >= prev_ema_long

        if is_golden_cross:
            return SignalType.LONG
        elif is_death_cross and self.enable_shorting:
            return SignalType.SHORT
            
        return SignalType.HOLD
</code>

kamikaze_komodo/ai_news_analysis_agent_module/news_scraper.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/news_scraper.py
import asyncio
import feedparser
import newspaper # type: ignore
import httpx
from typing import List, Optional, Dict, Any
# from bs4 import BeautifulSoup # Keep for potential future direct HTML parsing needs
from kamikaze_komodo.core.models import NewsArticle
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone, timedelta
from kamikaze_komodo.config.settings import settings # Import global settings

logger = get_logger(__name__)

class NewsScraper:
    """
    Scrapes news from specified sources (RSS feeds, websites).
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. NewsScraper cannot be initialized.")
            raise ValueError("Settings not loaded.")

        scraper_config = settings.get_news_scraper_config()
        self.rss_feeds: List[Dict[str, str]] = scraper_config.get("rss_feeds", [])
        self.websites_to_scrape: List[Dict[str, str]] = scraper_config.get("websites", []) # For Newspaper3k or custom BS4

        if not self.rss_feeds and not self.websites_to_scrape:
            logger.warning("NewsScraper initialized, but no RSS feeds or websites are configured in settings.")
        else:
            logger.info(f"NewsScraper initialized. RSS feeds: {len(self.rss_feeds)}, Websites: {len(self.websites_to_scrape)}")
            if self.rss_feeds:
                logger.debug(f"Configured RSS Feeds: {[feed['name'] for feed in self.rss_feeds]}")

    async def _fetch_url_content(self, url: str) -> Optional[str]:
        try:
            async with httpx.AsyncClient(timeout=20.0, follow_redirects=True) as client:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 KamikazeKomodoBot/1.0'
                }
                response = await client.get(url, headers=headers)
                response.raise_for_status()
                return response.text
        except httpx.RequestError as e:
            logger.error(f"HTTP request error fetching URL {url}: {e}")
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP status error fetching URL {url}: {e.response.status_code} - {e.response.text[:200]}")
        except Exception as e_gen:
            logger.error(f"Generic error fetching URL {url}: {e_gen}", exc_info=True)
        return None

    async def scrape_rss_feed(self, feed_name: str, feed_url: str, limit: int = 15) -> List[NewsArticle]:
        articles: List[NewsArticle] = []
        logger.info(f"Scraping RSS feed: {feed_name} from {feed_url}")

        feed_content = await self._fetch_url_content(feed_url)
        if not feed_content:
            logger.warning(f"Could not fetch content for RSS feed {feed_name} ({feed_url}). Skipping.")
            return articles

        try:
            loop = asyncio.get_event_loop()
            # feedparser is synchronous
            parsed_feed = await loop.run_in_executor(None, feedparser.parse, feed_content)

            if parsed_feed.bozo:
                logger.warning(f"Error parsing RSS feed {feed_name} ({feed_url}): {parsed_feed.bozo_exception}")

            if not parsed_feed.entries:
                logger.info(f"No entries found in RSS feed: {feed_name} ({feed_url}).")
                return articles

            for entry in parsed_feed.entries[:limit]:
                title = entry.get("title")
                link = entry.get("link")
                if not title or not link:
                    logger.debug(f"Skipping entry with missing title or link in {feed_name}: {entry.get('id', 'N/A')}")
                    continue

                published_time_struct = entry.get("published_parsed")
                updated_time_struct = entry.get("updated_parsed")

                pub_date: Optional[datetime] = None
                time_struct_to_use = published_time_struct or updated_time_struct

                if time_struct_to_use:
                    try:
                        pub_date = datetime(*time_struct_to_use[:6], tzinfo=timezone.utc)
                    except Exception as e_date:
                        logger.warning(f"Could not parse date for article '{title}' from {feed_name}: {time_struct_to_use}, error: {e_date}")

                # Fallback if date parsing fails or not present
                if pub_date is None:
                    pub_date = datetime.now(timezone.utc) # Use retrieval time as a last resort
                    logger.debug(f"Using current time as publication date for '{title}' from {feed_name} due to missing/unparseable date.")


                article_id = link # Use URL as a unique ID

                content_summary = entry.get("summary") or entry.get("description")

                # Attempt to extract related symbols from title or summary (basic)
                related_symbols = []
                text_for_symbols = (title + " " + (content_summary if content_summary else "")).lower()
                # This is very basic; a proper NER would be better
                common_crypto_symbols = {"btc", "bitcoin", "eth", "ethereum", "sol", "solana", "xrp", "ada", "cardano", "doge", "shib"}
                for sym in common_crypto_symbols:
                    if sym in text_for_symbols:
                        related_symbols.append(sym.upper())

                articles.append(NewsArticle(
                    id=article_id,
                    url=link,
                    title=title,
                    publication_date=pub_date,
                    retrieval_date=datetime.now(timezone.utc),
                    source=feed_name,
                    content=None, # Full content fetch can be added here or later by newspaper3k
                    summary=content_summary,
                    related_symbols=list(set(related_symbols)) # Unique symbols
                ))
            logger.info(f"Found {len(articles)} articles from RSS feed: {feed_name}")
        except Exception as e:
            logger.error(f"Failed to process RSS feed {feed_name} ({feed_url}): {e}", exc_info=True)
        return articles

    async def scrape_website_with_newspaper(self, site_name: str, site_url: str, limit_articles: int = 5) -> List[NewsArticle]:
        """Scrapes a website using Newspaper3k. Be mindful of terms of service."""
        articles_data: List[NewsArticle] = []
        logger.info(f"Scraping website: {site_name} ({site_url}) with Newspaper3k (limit: {limit_articles})")

        # Newspaper3k config
        config_np = newspaper.Config()
        config_np.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 KamikazeKomodoBot/1.0'
        config_np.request_timeout = 15
        config_np.memoize_articles = False # Disable caching for fresh data
        config_np.fetch_images = False # Don't need images
        config_np.verbose = False # newspaper's own verbosity

        try:
            loop = asyncio.get_event_loop()
            paper = await loop.run_in_executor(None, newspaper.build, site_url, config_np)

            count = 0
            for article_raw in paper.articles:
                if count >= limit_articles:
                    break
                try:
                    # Download and parse article content
                    await loop.run_in_executor(None, article_raw.download)
                    if not article_raw.is_downloaded:
                        logger.warning(f"Failed to download article: {article_raw.url} from {site_name}")
                        continue
                    await loop.run_in_executor(None, article_raw.parse)

                    title = article_raw.title
                    url = article_raw.url
                    if not title or not url:
                        logger.debug(f"Skipping article with no title/url from {site_name}")
                        continue

                    content = article_raw.text
                    summary_np = article_raw.summary # newspaper3k summary

                    pub_date_dt = article_raw.publish_date
                    if pub_date_dt and pub_date_dt.tzinfo is None:
                        pub_date_dt = pub_date_dt.replace(tzinfo=timezone.utc) # Assume UTC if naive, or local if known
                    elif pub_date_dt is None:
                        pub_date_dt = datetime.now(timezone.utc) # Fallback

                    related_symbols_np = []
                    text_for_symbols_np = (title + " " + (summary_np if summary_np else "") + " " + (content if content else "")).lower()
                    common_crypto_symbols_np = {"btc", "bitcoin", "eth", "ethereum", "sol", "solana", "xrp", "ada", "cardano", "doge", "shib"}
                    for sym_np in common_crypto_symbols_np:
                        if sym_np in text_for_symbols_np:
                            related_symbols_np.append(sym_np.upper())

                    articles_data.append(NewsArticle(
                        id=url, url=url, title=title,
                        publication_date=pub_date_dt,
                        retrieval_date=datetime.now(timezone.utc),
                        source=site_name,
                        content=content if content else None,
                        summary=summary_np if summary_np else None,
                        related_symbols=list(set(related_symbols_np))
                    ))
                    count += 1
                    logger.debug(f"Successfully scraped: {url} from {site_name}")
                except Exception as e_article:
                    logger.warning(f"Error processing article {article_raw.url} from {site_name} with Newspaper3k: {e_article}", exc_info=True)

            logger.info(f"Scraped {len(articles_data)} articles from {site_name} using Newspaper3k.")
        except Exception as e:
            logger.error(f"Failed to scrape website {site_name} ({site_url}) with Newspaper3k: {e}", exc_info=True)
        return articles_data

    async def scrape_all(self, limit_per_source: int = 10, since_hours_rss: Optional[int] = 24) -> List[NewsArticle]:
        """
        Scrapes all configured RSS feeds and websites.
        For RSS, optionally filters articles published within `since_hours_rss`.
        """
        all_articles: List[NewsArticle] = []

        # Scrape RSS Feeds
        rss_tasks = []
        if self.rss_feeds:
            for feed_info in self.rss_feeds:
                rss_tasks.append(self.scrape_rss_feed(feed_info['name'], feed_info['url'], limit=limit_per_source))

            rss_results_list = await asyncio.gather(*rss_tasks, return_exceptions=True)
            for result in rss_results_list:
                if isinstance(result, list):
                    all_articles.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"RSS scraping task failed: {result}", exc_info=True) # Log exception details
        else:
            logger.info("No RSS feeds configured to scrape.")

        # Filter RSS articles by publication date if since_hours_rss is provided
        if since_hours_rss is not None:
            cutoff_date = datetime.now(timezone.utc) - timedelta(hours=since_hours_rss)
            filtered_articles = []
            for article in all_articles:
                if article.publication_date and article.publication_date >= cutoff_date:
                    filtered_articles.append(article)
                elif not article.publication_date: # If no pub date, include it (conservative)
                    filtered_articles.append(article)
            count_removed = len(all_articles) - len(filtered_articles)
            if count_removed > 0:
                logger.info(f"Filtered out {count_removed} RSS articles older than {since_hours_rss} hours.")
            all_articles = filtered_articles

        # Scrape Websites (e.g., using Newspaper3k) - typically gets latest, less date control
        website_tasks = []
        if self.websites_to_scrape:
            for site_info in self.websites_to_scrape:
                website_tasks.append(self.scrape_website_with_newspaper(site_info['name'], site_info['url'], limit_articles=limit_per_source))

            website_results_list = await asyncio.gather(*website_tasks, return_exceptions=True)
            for result in website_results_list:
                if isinstance(result, list):
                    all_articles.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"Website scraping task failed: {result}", exc_info=True)
        else:
            logger.info("No direct websites configured to scrape with Newspaper3k.")

        # Deduplicate articles by URL (ID)
        unique_articles_dict: Dict[str, NewsArticle] = {}
        for article in all_articles:
            if article.id not in unique_articles_dict:
                unique_articles_dict[article.id] = article
            else: # If duplicate, prefer the one with more content or later retrieval
                existing_article = unique_articles_dict[article.id]
                if (article.content and not existing_article.content) or \
                   (article.retrieval_date > existing_article.retrieval_date):
                    unique_articles_dict[article.id] = article

        unique_articles_list = sorted(list(unique_articles_dict.values()), key=lambda x: x.publication_date or x.retrieval_date, reverse=True)

        logger.info(f"Total unique articles scraped from all sources: {len(unique_articles_list)}")
        return unique_articles_list

async def main_scraper_example():
    if not settings or not settings.news_scraper_enable:
        logger.info("NewsScraper is not enabled in settings or settings not loaded.")
        return

    scraper = NewsScraper()

    # Scrape all configured sources, limiting to 5 articles per source,
    # and only RSS articles from the last 48 hours
    all_scraped_articles = await scraper.scrape_all(limit_per_source=5, since_hours_rss=48)

    logger.info(f"--- All Scraped Articles ({len(all_scraped_articles)}) ---")
    if not all_scraped_articles:
        logger.info("No articles were scraped.")
        return

    for i, article in enumerate(all_scraped_articles[:10]): # Log details for first 10
        logger.info(f"{i+1}. Source: {article.source}, Title: {article.title}")
        logger.info(f"    URL: {article.url}")
        logger.info(f"    Date: {article.publication_date}, Retrieved: {article.retrieval_date}")
        logger.info(f"    Symbols: {article.related_symbols}")
        if article.summary:
            logger.info(f"    Summary: {article.summary[:150]}...")
        # if article.content: # Content can be very long
            # logger.info(f" Content Preview: {article.content[:100]}...")

    # Example: Store articles in DB
    if all_scraped_articles:
        from kamikaze_komodo.data_handling.database_manager import DatabaseManager
        db_manager = DatabaseManager()
        db_manager.store_news_articles(all_scraped_articles)
        logger.info(f"Stored {len(all_scraped_articles)} articles in the database.")

        # Retrieve and show some from DB
        retrieved = db_manager.retrieve_news_articles(limit=5)
        logger.info(f"--- Retrieved {len(retrieved)} articles from DB ---")
        for art_db in retrieved:
            logger.info(f"DB: {art_db.title} (Source: {art_db.source}, Date: {art_db.publication_date})")
        db_manager.close()

if __name__ == "__main__":
    if settings and settings.news_scraper_enable:
        asyncio.run(main_scraper_example())
    else:
        print("NewsScraper is not enabled in settings, or settings failed to load. Skipping example.")
</code>

kamikaze_komodo/ai_news_analysis_agent_module/sentiment_analyzer.py:
<code>
# FILE: kamikaze_komodo/ai_news_analysis_agent_module/sentiment_analyzer.py
from typing import List, Dict, Optional, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.core.models import NewsArticle
from kamikaze_komodo.config.settings import settings # Import global settings

logger = get_logger(__name__)

# Pydantic model for structured sentiment output
class SentimentAnalysisOutput(BaseModel):
    sentiment_label: str = Field(description="The overall sentiment (e.g., 'very bullish', 'bullish', 'neutral', 'bearish', 'very bearish', 'mixed').")
    sentiment_score: float = Field(description="A numerical score from -1.0 (very negative) to 1.0 (very positive). Neutral is 0.0.")
    key_themes: Optional[List[str]] = Field(default_factory=list, description="List of key themes or topics identified in the text related to sentiment.")
    confidence: Optional[float] = Field(description="Confidence score of the sentiment analysis (0.0 to 1.0).")

class SentimentAnalyzer:
    """
    Analyzes text for sentiment using a configured LLM via Langchain.
    Supports Google Vertex AI.
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. SentimentAnalyzer cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.llm_provider = settings.sentiment_llm_provider
        self.llm: Any = None # Will be initialized in _initialize_llm

        self._initialize_llm()

        # Define a structured prompt for sentiment analysis
        self.prompt_template = ChatPromptTemplate.from_messages(
            [
                ("system",
                 "You are an expert financial sentiment analyst specializing in cryptocurrency markets. "
                 "Analyze the provided text for its sentiment towards the cryptocurrency or market mentioned. "
                 "Consider factors like news events, market reactions, technological developments, and regulatory news. "
                 "Your output MUST be in JSON format, adhering to the following Pydantic model structure (SentimentAnalysisOutput): "
                 "```json\n"
                 "{\n"
                 "  \"sentiment_label\": \"<label: 'very bullish'|'bullish'|'neutral'|'bearish'|'very bearish'|'mixed'>\",\n"
                 "  \"sentiment_score\": <score_float: -1.0 to 1.0>,\n"
                 "  \"key_themes\": [\"<theme1>\", \"<theme2>\"],\n" # Optional
                 "  \"confidence\": <confidence_float: 0.0 to 1.0>\n" # Optional
                 "}\n"
                 "```"
                 "sentiment_score should range from -1.0 (very bearish/negative) to 1.0 (very bullish/positive). Neutral is 0.0. "
                 "key_themes should highlight important topics influencing the sentiment. confidence is your perceived accuracy of this analysis (0.0 to 1.0)."
                 ),
                ("human", "Please analyze the sentiment of the following text regarding {asset_context}:\n\n---\n{text_to_analyze}\n---"),
            ]
        )
        self.output_parser = JsonOutputParser(pydantic_object=SentimentAnalysisOutput)
        self.chain = self.prompt_template | self.llm | self.output_parser

    def _initialize_llm(self):
        logger.info(f"Initializing LLM for SentimentAnalyzer with provider: {self.llm_provider}")
        if self.llm_provider == "VertexAI":
            if not settings.vertex_ai_project_id or not settings.vertex_ai_location:
                logger.error("Vertex AI project ID or location is not configured in settings.py. Sentiment analysis will not work.")
                raise ValueError("Vertex AI project ID or location missing.")
            try:
                from langchain_google_vertexai import ChatVertexAI # Correct import
                self.llm = ChatVertexAI(
                    project=settings.vertex_ai_project_id,
                    location=settings.vertex_ai_location,
                    model_name=settings.vertex_ai_sentiment_model_name,
                    temperature=0.1, # Low temperature for more factual/consistent sentiment
                    # max_output_tokens=1024, # Optional: if needed for longer summaries/themes
                )
                logger.info(f"SentimentAnalyzer initialized with Vertex AI model: {settings.vertex_ai_sentiment_model_name}")
                logger.info("Ensure Google Cloud credentials (GOOGLE_APPLICATION_CREDENTIALS) are set in your environment.")
            except ImportError:
                logger.error("langchain-google-vertexai is not installed. Please install it: pip install langchain-google-vertexai")
                raise
            except Exception as e:
                logger.error(f"Failed to initialize Vertex AI LLM ({settings.vertex_ai_sentiment_model_name}): {e}", exc_info=True)
                raise
        else:
            logger.error(f"Unsupported LLM provider: {self.llm_provider}")
            raise ValueError(f"Unsupported LLM provider: {self.llm_provider}")

        if self.llm is None:
            logger.error("LLM could not be initialized.")
            raise ValueError("LLM initialization failed.")

    async def analyze_sentiment_structured(self, text: str, asset_context: str = "the market") -> Optional[SentimentAnalysisOutput]:
        """
        Analyzes text and returns a structured sentiment analysis including score and label.
        """
        if not text or not text.strip():
            logger.warning("No text provided for sentiment analysis.")
            return None

        if self.llm is None:
            logger.error("LLM not initialized. Cannot analyze sentiment.")
            return None

        logger.debug(f"Analyzing sentiment for text (context: {asset_context}): '{text[:200]}...'")
        try:
            # Max input tokens for gemini-2.5-flash-preview is high, but let's be reasonable.
            # Prompt itself consumes tokens. Max 30k chars ~ 7.5k tokens for text.
            max_chars = 30000
            if len(text) > max_chars:
                logger.warning(f"Text too long ({len(text)} chars), truncating to {max_chars} for sentiment analysis.")
                text = text[:max_chars]

            response_dict = await self.chain.ainvoke({"text_to_analyze": text, "asset_context": asset_context})

            # The output_parser should already return a SentimentAnalysisOutput object if successful
            # If it's a dict, it means JsonOutputParser might not have directly instantiated the Pydantic model
            # or the LLM didn't return perfect JSON matching the Pydantic model structure.
            if isinstance(response_dict, dict):
                try:
                    # Attempt to create Pydantic model from dict for validation and type safety
                    validated_output = SentimentAnalysisOutput(**response_dict)
                    logger.info(f"Structured sentiment for '{asset_context}': Label: {validated_output.sentiment_label}, Score: {validated_output.sentiment_score:.2f}, Confidence: {validated_output.confidence}")
                    return validated_output
                except Exception as p_exc:
                    logger.error(f"Pydantic validation failed for LLM JSON output: {response_dict}. Error: {p_exc}", exc_info=True)
                    return None
            elif isinstance(response_dict, SentimentAnalysisOutput): # Already a Pydantic object
                   logger.info(f"Structured sentiment for '{asset_context}': Label: {response_dict.sentiment_label}, Score: {response_dict.sentiment_score:.2f}, Confidence: {response_dict.confidence}")
                   return response_dict
            else:
                logger.error(f"Unexpected structured sentiment analysis output type: {type(response_dict)}. Content: {str(response_dict)[:500]}")
                return None
        except Exception as e:
            logger.error(f"Error during structured sentiment analysis with {self.llm_provider} model: {e}", exc_info=True)
            return None

    async def get_sentiment_for_article(self, article: NewsArticle, asset_context: Optional[str] = None) -> NewsArticle:
        """
        Analyzes sentiment for a NewsArticle object and updates its sentiment fields.
        Uses article title and summary/content.
        """
        if not asset_context and article.related_symbols:
            asset_context = ", ".join(article.related_symbols)
        elif not asset_context:
            # Try to infer from title if no symbols
            if "bitcoin" in article.title.lower() or "btc" in article.title.lower():
                asset_context = "Bitcoin"
            elif "ethereum" in article.title.lower() or "eth" in article.title.lower():
                asset_context = "Ethereum"
            else:
                asset_context = "the cryptocurrency market"

        text_to_analyze = article.title
        if article.summary:
            text_to_analyze += "\n\n" + article.summary
        elif article.content: # Fallback to content if no summary
            text_to_analyze += "\n\n" + article.content

        if not text_to_analyze.strip():
            logger.warning(f"No text content found in article {article.id} to analyze.")
            return article # Return original article if no text

        sentiment_result = await self.analyze_sentiment_structured(text_to_analyze, asset_context=asset_context)

        if sentiment_result:
            article.sentiment_label = sentiment_result.sentiment_label
            article.sentiment_score = sentiment_result.sentiment_score
            article.key_themes = sentiment_result.key_themes
            article.sentiment_confidence = sentiment_result.confidence
            # article.raw_llm_response can store the full dict if needed for debugging
            # article.raw_llm_response = sentiment_result.model_dump()
        return article

# Example Usage
async def main_sentiment_example():
    """ Example of using the SentimentAnalyzer """
    if not settings or not settings.vertex_ai_project_id:
        logger.error("Vertex AI settings (Project ID) not loaded for sentiment example. Set GOOGLE_APPLICATION_CREDENTIALS env var.")
        return

    try:
        analyzer = SentimentAnalyzer()
    except Exception as e:
        logger.error(f"Could not start SentimentAnalyzer: {e}")
        return

    example_texts = [
        ("Bitcoin surges past $70,000, analysts predict further upside due to ETF inflows and positive market structure.", "Bitcoin"),
        ("Regulatory crackdown imminent? SEC chair issues stark warning on crypto staking, leading to market jitters.", "Cryptocurrency Regulation"),
        ("Ethereum's Dencun upgrade successfully goes live on mainnet, promising significantly lower fees for Layer 2 solutions and boosting scalability.", "Ethereum"),
        ("The crypto market remains flat this week with low volatility and trading volume, investors seem hesitant.", "the crypto market"),
        ("Solana's network outage causes temporary panic, but recovery was swift. Developers are addressing the root cause.", "Solana")
    ]

    for text, context in example_texts:
        logger.info(f"\n--- Analyzing text for '{context}' ---")
        logger.info(f"Text: {text}")
        result = await analyzer.analyze_sentiment_structured(text, asset_context=context)
        if result:
            logger.info(f"  Sentiment Label: {result.sentiment_label}")
            logger.info(f"  Sentiment Score: {result.sentiment_score:.3f}")
            logger.info(f"  Key Themes: {result.key_themes}")
            logger.info(f"  Confidence: {result.confidence}")
        else:
            logger.warning("  Failed to get structured sentiment analysis.")

    sample_article = NewsArticle(
        id="test_article_sol_123",
        url="http://example.com/news_sol_1",
        title="Solana Ecosystem Sees Major Investment for DeFi Growth",
        summary="The Solana Foundation has announced a new $100 million fund dedicated to fostering DeFi projects on its blockchain. This move is expected to attract more developers and users, with SOL token price reacting positively.",
        source="Crypto News Daily",
        related_symbols=["SOL", "Solana"]
    )
    logger.info(f"\n--- Analyzing NewsArticle: {sample_article.title} ---")
    updated_article = await analyzer.get_sentiment_for_article(sample_article)
    logger.info(f"  Analyzed Article Sentiment: Label='{updated_article.sentiment_label}', Score={updated_article.sentiment_score}, Themes: {updated_article.key_themes}")

if __name__ == "__main__":
    import asyncio
    # Ensure GOOGLE_APPLICATION_CREDENTIALS is set in your environment
    # e.g., export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-file.json"
    asyncio.run(main_sentiment_example())
</code>

kamikaze_komodo/ai_news_analysis_agent_module/browser_agent.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/browser_agent.py
import asyncio
from typing import Optional, Any, Dict
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Import global settings

logger = get_logger(__name__)

class BrowserAgent:
    """
    Uses browser-use with a configured LLM (Vertex AI's Gemini)
    to perform targeted market research.
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. BrowserAgent cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.llm_provider = settings.browser_agent_llm_provider
        self.llm: Optional[Any] = None # To be initialized
        self.agent_is_ready = False
        self.browser_use_agent_class: Optional[type] = None # For storing the imported class

        try:
            self._initialize_llm()
            # Dynamically import browser_use only if LLM init is successful
            # global BrowserUseAgent # Make it global for the method if loaded
            from browser_use import Agent as BrowserUseAgentLib
            self.browser_use_agent_class = BrowserUseAgentLib # Store the class
            self.agent_is_ready = True
            logger.info("browser-use Agent component dynamically imported.")
        except ImportError:
            logger.error("browser-use library not found. Please install it: pip install browser-use")
            logger.error("Also run: playwright install --with-deps chromium")
        except Exception as e:
            logger.error(f"BrowserAgent initialization failed: {e}", exc_info=True)


    def _initialize_llm(self):
        logger.info(f"Initializing LLM for BrowserAgent with provider: {self.llm_provider}")
        if self.llm_provider == "VertexAI":
            if not settings.vertex_ai_project_id or not settings.vertex_ai_location:
                logger.error("Vertex AI project ID or location is not configured. BrowserAgent LLM will not work.")
                raise ValueError("Vertex AI project ID or location missing for BrowserAgent.")
            try:
                from langchain_google_vertexai import ChatVertexAI
                self.llm = ChatVertexAI(
                    project=settings.vertex_ai_project_id,
                    location=settings.vertex_ai_location,
                    model_name=settings.vertex_ai_browser_agent_model_name, # Use specific model for browser agent
                    temperature=0.2, # Slightly higher temp for research/summarization
                    # max_output_tokens=2048, # Optional
                )
                logger.info(f"BrowserAgent initialized with Vertex AI model: {settings.vertex_ai_browser_agent_model_name}")
                logger.info("Ensure Google Cloud credentials (GOOGLE_APPLICATION_CREDENTIALS) are set for Vertex AI.")
            except ImportError:
                logger.error("langchain-google-vertexai not found. pip install langchain-google-vertexai")
                raise
            except Exception as e:
                logger.error(f"Failed to initialize Vertex AI LLM for BrowserAgent: {e}", exc_info=True)
                raise
        else:
            logger.error(f"Unsupported LLM provider for BrowserAgent: {self.llm_provider}")
            raise ValueError(f"Unsupported LLM provider for BrowserAgent: {self.llm_provider}")

        if self.llm is None:
            raise ValueError("BrowserAgent LLM initialization failed.")


    async def conduct_research(self, research_task: str, max_steps: int = 20, use_vision: bool = False) -> Optional[Dict[str, Any]]:
        """
        Conducts research based on the given task using browser-use.
        """
        if not self.agent_is_ready or self.llm is None or self.browser_use_agent_class is None:
            logger.error("BrowserAgent or its LLM not initialized, or browser_use class not loaded. Cannot conduct research.")
            return None

        logger.info(f"Starting browser-use research task: '{research_task[:100]}...' (Max steps: {max_steps})")
        try:
            agent = self.browser_use_agent_class( # Use the stored class
                llm=self.llm,
                task=research_task,
                use_vision=use_vision,
                # verbose=True, # Can be noisy
            )
            result = await agent.run(max_steps=max_steps)
            logger.info("Browser-use research task completed.")

            final_output_text = ""
            if isinstance(result, dict):
                logger.debug(f"Browser agent raw result dictionary keys: {result.keys()}")
                final_output_text = result.get('output', result.get('answer', str(result)))
            elif isinstance(result, str):
                final_output_text = result
            else:
                final_output_text = str(result)
                logger.warning(f"Unexpected result type from browser-use agent: {type(result)}. Content: {final_output_text[:500]}")

            logger.info(f"Browser Agent Output: {final_output_text[:500]}...")
            return {"output": final_output_text, "full_result": result}

        except Exception as e:
            logger.error(f"An error occurred while running the browser-use agent: {e}", exc_info=True)
            logger.error("Possible issues: LLM server, network, task complexity, or website automation challenges.")
            return None

async def main_browser_agent_example():
    if not settings or not settings.browser_agent_enable:
        logger.info("BrowserAgent is not enabled in settings or settings not loaded.")
        return
    if not settings.vertex_ai_project_id and settings.browser_agent_llm_provider == "VertexAI":
        logger.error("Vertex AI Project ID not set for BrowserAgent. Ensure it's in config.ini and GOOGLE_APPLICATION_CREDENTIALS env var is set.")
        return

    try:
        browser_agent = BrowserAgent()
        if not browser_agent.agent_is_ready:
            logger.error("Browser agent could not be initialized. Exiting example.")
            return
    except Exception as e:
        logger.error(f"Failed to create BrowserAgent: {e}")
        return

    task = (
        "What is the latest news regarding Solana's network performance and any major ecosystem developments in June 2025? "
        "Visit 2 reputable crypto news websites (e.g., Decrypt, Cointelegraph). "
        "Summarize findings and list article URLs. Limit Browse to 4 steps per site."
    )
    # Get max_steps from settings
    max_steps_for_research = settings.browser_agent_max_steps if settings.browser_agent_max_steps > 0 else 25

    result_data = await browser_agent.conduct_research(task, max_steps=max_steps_for_research)

    if result_data and "output" in result_data:
        logger.info("\n--- Browser Agent Research Result ---")
        print(result_data["output"])
    elif result_data:
        logger.info("\n--- Browser Agent Raw Research Result ---")
        print(result_data)
    else:
        logger.warning("Browser agent research did not produce a result or failed.")

if __name__ == "__main__":
    # Ensure GOOGLE_APPLICATION_CREDENTIALS is set for Vertex AI
    # Ensure Playwright browsers are installed: playwright install --with-deps chromium
    # This example is best run if BrowserAgent_Enable is true in config.
    if settings and settings.browser_agent_enable:
        asyncio.run(main_browser_agent_example())
    else:
        print("BrowserAgent is not enabled in settings, or settings failed to load. Skipping example.")
        print("To run, set BrowserAgent_Enable = True in config.ini and ensure Vertex AI is configured.")
</code>

kamikaze_komodo/ai_news_analysis_agent_module/notification_listener.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/notification_listener.py
from kamikaze_komodo.app_logger import get_logger
# from jeepney import DBusAddress, new_method_call # If fully implementing
# from jeepney.io.asyncio import open_dbus_connection # If fully implementing
import asyncio
from typing import Callable, Awaitable, Dict, Any, Optional

logger = get_logger(__name__)

class NotificationListener:
    """
    Listens for desktop notifications using D-Bus (via Jeepney).
    Basic implementation for Phase 4. Full D-Bus interaction is complex and OS-dependent.
    """
    def __init__(self, callback_on_notification: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None):
        """
        Args:
            callback_on_notification: An async function to call when a notification is received.
                                      It should accept a dictionary with notification details.
        """
        self.callback_on_notification = callback_on_notification
        self._running = False
        logger.info("NotificationListener initialized.")
        if self.callback_on_notification is None:
            logger.warning("No callback provided for NotificationListener. It will not perform any actions on notifications.")
        logger.warning("Full D-Bus notification listening with Jeepney is a complex, OS-dependent feature and is currently a placeholder.")
        logger.warning("To enable, ensure Jeepney is installed and D-Bus is correctly configured on your Linux desktop.")


    async def start_listening(self):
        """
        Starts listening for D-Bus notifications. Placeholder for Jeepney implementation.
        """
        self._running = True
        logger.info("Notification listener started (simulated/placeholder).")
        if self.callback_on_notification is None:
            logger.error("NotificationListener started but no callback is set. It will be idle.")
            # We can simply return or let the placeholder loop run idly.

        # --- Conceptual Jeepney Implementation (Highly OS/Setup Dependent) ---
        # try:
        #     from jeepney import DBusAddress, new_method_call
        #     from jeepney.io.asyncio import open_dbus_connection
        #     logger.info("Attempting to connect to D-Bus for notifications...")
        #     conn = await open_dbus_connection()
        #     logger.info("Connected to D-Bus. Setting up match rule for org.freedesktop.Notifications.Notify.")
        #
        #     # Interface to call AddMatch on (DBUS daemon itself)
        #     dbus_daemon_addr = DBusAddress('org.freedesktop.DBus', '/org/freedesktop/DBus', 'org.freedesktop.DBus')
        #     match_rule = "type='signal',interface='org.freedesktop.Notifications',member='Notify'"
        #     add_match_msg = new_method_call(dbus_daemon_addr, 'AddMatch', 's', (match_rule,))
        #     await conn.send_and_get_reply(add_match_msg) # No reply expected or needed for AddMatch usually
        #     logger.info(f"Match rule added: {match_rule}")
        #
        #     while self._running:
        #         try:
        #             msg_received = await asyncio.wait_for(conn.receive(), timeout=1.0) # Add timeout
        #             if msg_received and msg_received.member == 'Notify' and msg_received.interface == 'org.freedesktop.Notifications':
        #                 parsed_notification = self.parse_notification_data(msg_received.body)
        #                 if self.callback_on_notification and parsed_notification:
        #                     logger.info(f"Received D-Bus Notification: {parsed_notification.get('summary')}")
        #                     await self.callback_on_notification(parsed_notification)
        #             elif msg_received:
        #                 logger.debug(f"Received other D-Bus message: Member={msg_received.member}, Interface={msg_received.interface}")
        #         except asyncio.TimeoutError:
        #             continue # Just to allow checking self._running
        #         except Exception as e_recv:
        #             logger.error(f"Error receiving/processing D-Bus message: {e_recv}", exc_info=True)
        #             await asyncio.sleep(5) # Avoid rapid error loops
        # except ImportError:
        #     logger.error("Jeepney library not found. D-Bus notification listener cannot run. Please install it.")
        # except ConnectionRefusedError:
        #     logger.error("Could not connect to D-Bus. Ensure D-Bus daemon is running and accessible.")
        # except Exception as e:
        #     logger.error(f"An error occurred in D-Bus notification listener setup: {e}", exc_info=True)
        # finally:
        #     if 'conn' in locals() and conn:
        #         # Optionally remove match rule before closing
        #         # remove_match_msg = new_method_call(dbus_daemon_addr, 'RemoveMatch', 's', (match_rule,))
        #         # await conn.send_and_get_reply(remove_match_msg)
        #         await conn.close()
        #         logger.info("D-Bus connection closed.")
        #     self._running = False
        # --- End Conceptual Jeepney ---

        # Current Placeholder Loop
        while self._running:
            await asyncio.sleep(30)
            if self.callback_on_notification is not None: # Only log if it's supposed to be doing something
                logger.debug("Notification listener placeholder task running (no actual D-Bus listening)...")

    def stop_listening(self):
        self._running = False
        logger.info("Notification listener stopped (simulated/placeholder).")

    def parse_notification_data(self, notification_body: tuple) -> Optional[Dict[str, Any]]:
        """
        Parses the D-Bus notification data (signal body for Notify) into a structured dictionary.
        Standard Notify signature: (app_name, replaces_id, app_icon, summary, body, actions, hints, expire_timeout)
                                    (s, u, s, s, s, as, a{sv}, i)
        """
        try:
            if isinstance(notification_body, tuple) and len(notification_body) == 8:
                return {
                    "app_name": notification_body[0],
                    "replaces_id": notification_body[1], # uint32
                    "app_icon": notification_body[2],
                    "summary": notification_body[3],  # Title
                    "body": notification_body[4],     # Message
                    "actions": notification_body[5], # List of strings (action identifiers)
                    "hints": notification_body[6],   # Dict of variant hints
                    "expire_timeout": notification_body[7] # int32
                }
            else:
                logger.warning(f"Received notification body with unexpected format or length: {notification_body}")
        except Exception as e:
            logger.error(f"Error parsing notification data: {e}. Body was: {notification_body}", exc_info=True)
        return None

async def dummy_notification_callback(notification_details: Dict[str, Any]):
    logger.info(f"Dummy Callback: Received Notification - Summary: '{notification_details.get('summary')}', Body: '{notification_details.get('body')}'")
    # Here, you might trigger news analysis, sentiment analysis, or other actions.

# Example:
# if __name__ == "__main__":
#     listener = NotificationListener(callback_on_notification=dummy_notification_callback)
#     # To test this, you would need a D-Bus environment (Linux desktop) and send a notification:
#     # e.g., using `notify-send "Test Summary" "This is a test notification body."` in terminal.
#     try:
#         asyncio.run(listener.start_listening())
#     except KeyboardInterrupt:
#         listener.stop_listening()
#         logger.info("Notification listener example stopped by user.")
</code>

kamikaze_komodo/ai_news_analysis_agent_module/__init__.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/__init__.py
# This file makes the 'ai_news_analysis_agent_module' directory a Python package.
</code>

