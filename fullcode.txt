kamikaze_komodo/app_logger.py:
<code>
# kamikaze_komodo/app_logger.py
import logging
import os
from logging.handlers import RotatingFileHandler

LOG_DIR = "logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

log_file_path = os.path.join(LOG_DIR, "kamikaze_komodo.log")

# Configure logging
logger = logging.getLogger("KamikazeKomodo")
logger.setLevel(logging.DEBUG)  # Set to DEBUG to capture all levels of messages

# Create handlers
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)  # Console logs info and above

file_handler = RotatingFileHandler(
    log_file_path, maxBytes=10*1024*1024, backupCount=5  # 10MB per file, 5 backups
)
file_handler.setLevel(logging.DEBUG)  # File logs debug and above

# Create formatters and add it to handlers
log_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s.%(funcName)s - %(message)s')
console_handler.setFormatter(log_format)
file_handler.setFormatter(log_format)

# Add handlers to the logger
if not logger.handlers:
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

def get_logger(module_name: str) -> logging.Logger:
    """
    Returns a logger instance for a specific module.
    """
    return logging.getLogger(f"KamikazeKomodo.{module_name}")
</code>

kamikaze_komodo/main.py:
<code>
# kamikaze_komodo/main.py
import asyncio
import os
import pandas as pd
from datetime import datetime, timedelta, timezone
from typing import List, Optional, Dict, Any

from kamikaze_komodo.app_logger import get_logger, logger as root_logger
from kamikaze_komodo.config.settings import settings

# Phase 1 & 2 imports
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.exchange_interaction.exchange_api import ExchangeAPI
# from kamikaze_komodo.strategy_framework.strategy_manager import StrategyManager # Not directly used in demos
from kamikaze_komodo.strategy_framework.strategies.ewmac import EWMACStrategy
from kamikaze_komodo.backtesting_engine.engine import BacktestingEngine
from kamikaze_komodo.backtesting_engine.performance_analyzer import PerformanceAnalyzer
from kamikaze_komodo.core.models import BarData, NewsArticle
# from kamikaze_komodo.core.enums import SignalType # For strategy checking, not directly used here

# Phase 3 imports
from kamikaze_komodo.risk_control_module.position_sizer import FixedFractionalPositionSizer, ATRBasedPositionSizer
from kamikaze_komodo.risk_control_module.stop_manager import PercentageStopManager, ATRStopManager

# Portfolio constructor components are more for multi-asset; for single-asset backtest, they are less directly demonstrated here.

# Phase 4 imports
from kamikaze_komodo.ai_news_analysis_agent_module.news_scraper import NewsScraper
from kamikaze_komodo.ai_news_analysis_agent_module.sentiment_analyzer import SentimentAnalyzer
# from kamikaze_komodo.ai_news_analysis_agent_module.browser_agent import BrowserAgent # Optional advanced feature
# from kamikaze_komodo.ai_news_analysis_agent_module.notification_listener import NotificationListener, dummy_notification_callback # Placeholder

logger = get_logger(__name__)

async def run_phase1_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 1: Core Infrastructure & Data")
    if not settings: root_logger.critical("Settings failed to load."); return

    db_manager = DatabaseManager()
    data_fetcher = DataFetcher() # Uses exchange_id from settings
    exchange_api = ExchangeAPI() # Uses exchange_id from settings

    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    start_period = datetime.now(timezone.utc) - timedelta(days=settings.historical_data_days if settings.historical_data_days > 0 else 30)
    end_period = datetime.now(timezone.utc)

    historical_data = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_period, end_period)
    if historical_data:
        logger.info(f"Fetched {len(historical_data)} bars for {symbol} ({timeframe}).")
        db_manager.store_bar_data(historical_data)
        retrieved_data = db_manager.retrieve_bar_data(symbol, timeframe, start_date=start_period, end_date=end_period)
        logger.info(f"Retrieved {len(retrieved_data)} bars from DB for {symbol} ({timeframe}). First bar: {retrieved_data[0].timestamp if retrieved_data else 'N/A'}")
    else:
        logger.warning(f"No historical data fetched for {symbol}.")

    balance = await exchange_api.fetch_balance()
    if balance:
        # Updated currency extraction logic
        base_currency = symbol.split('/')[0].split(':')[0] if '/' in symbol or ':' in symbol else "N/A"
        quote_currency = symbol.split('/')[1] if '/' in symbol else (symbol.split(':')[1] if ':' in symbol else "USD")
        
        logger.info(f"Fetched balance. Free {base_currency}: {balance.get(base_currency, {}).get('free', 'N/A')}, Free {quote_currency}: {balance.get(quote_currency, {}).get('free', 'N/A')}")
    else:
        logger.warning("Could not fetch account balance.")

    await data_fetcher.close()
    await exchange_api.close()
    db_manager.close()
    root_logger.info("Phase 1 Demonstration completed.")

async def run_phase2_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 2: Basic Strategy & Backtesting")
    if not settings: root_logger.critical("Settings failed to load."); return

    db_manager = DatabaseManager()
    data_fetcher = DataFetcher()
    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    hist_days = settings.historical_data_days if settings.historical_data_days > 0 else 365
    start_date = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_date = datetime.now(timezone.utc)

    ewmac_params_for_min_bars = settings.get_strategy_params("EWMAC")
    min_bars_needed = int(ewmac_params_for_min_bars.get('longwindow', settings.ewmac_long_window)) + 5 # Buffer

    historical_bars: List[BarData] = db_manager.retrieve_bar_data(symbol, timeframe, start_date, end_date)
    
    if not historical_bars or len(historical_bars) < min_bars_needed:
        logger.info(f"Insufficient data in DB for {symbol} ({len(historical_bars)}/{min_bars_needed}). Fetching fresh for {hist_days} days...")
        historical_bars = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_date, end_date)
        if historical_bars: db_manager.store_bar_data(historical_bars)
        else: logger.error(f"Failed to fetch sufficient data for {symbol}. Backtest cannot proceed."); await data_fetcher.close(); db_manager.close(); return
    
    await data_fetcher.close() # Close after fetching
    db_manager.close() # Close after retrieving/storing

    if not historical_bars or len(historical_bars) < min_bars_needed:
        logger.error(f"Still not enough historical data for {symbol} ({len(historical_bars)} bars vs {min_bars_needed} needed) after fetch attempt. Backtest aborted."); return
    
    logger.info(f"Using {len(historical_bars)} bars for backtesting {symbol}.")
    data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
    data_df.set_index('timestamp', inplace=True)

    if data_df.empty or len(data_df) < min_bars_needed:
        logger.error(f"DataFrame conversion error or insufficient points for EWMAC ({len(data_df)})."); return

    ewmac_params = settings.get_strategy_params("EWMAC") # Gets params from [EWMAC_Strategy] in config
    ewmac_strategy = EWMACStrategy(symbol=symbol, timeframe=timeframe, params=ewmac_params)
    
    initial_capital = 10000.00
    commission_bps = settings.commission_bps
    backtest_engine = BacktestingEngine(data_feed_df=data_df, strategy=ewmac_strategy, initial_capital=initial_capital, commission_bps=commission_bps)
    
    logger.info(f"Running basic backtest for EWMAC on {symbol}...")
    trades_log, final_portfolio = backtest_engine.run()

    if trades_log:
        logger.info(f"Backtest (Phase 2) completed. Generated {len(trades_log)} trades.")
        performance_analyzer = PerformanceAnalyzer(trades=trades_log, initial_capital=initial_capital, final_capital=final_portfolio['final_portfolio_value'])
        metrics = performance_analyzer.calculate_metrics()
        performance_analyzer.print_summary(metrics)
    else:
        logger.info(f"Backtest (Phase 2) completed. No trades executed. Final portfolio value: ${final_portfolio['final_portfolio_value']:.2f}")
    root_logger.info("Phase 2 Demonstration completed.")


async def run_phase3_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 3: Risk Management Integration")
    if not settings: root_logger.critical("Settings failed to load."); return

    db_manager = DatabaseManager()
    data_fetcher = DataFetcher()
    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    hist_days = settings.historical_data_days if settings.historical_data_days > 0 else 365
    start_date = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_date = datetime.now(timezone.utc)

    ewmac_params_for_min_bars_phase3 = settings.get_strategy_params("EWMAC")
    # For ATR-based risk, strategy needs enough data to calculate ATR.
    # ATR period is now part of strategy params from get_strategy_params.
    atr_period_from_params = int(ewmac_params_for_min_bars_phase3.get("atr_period", settings.ewmac_atr_period))
    long_window_from_params = int(ewmac_params_for_min_bars_phase3.get("longwindow", settings.ewmac_long_window))
    min_bars_needed = max(long_window_from_params, atr_period_from_params) + 5 # Buffer

    historical_bars = db_manager.retrieve_bar_data(symbol, timeframe, start_date, end_date)
    if not historical_bars or len(historical_bars) < min_bars_needed:
        logger.info(f"Fetching fresh data for Phase 3 backtest (need ~{min_bars_needed} bars)...")
        historical_bars = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_date, end_date)
        if historical_bars: db_manager.store_bar_data(historical_bars)
        else: logger.error(f"Failed to fetch data for {symbol}. Aborting."); await data_fetcher.close(); db_manager.close(); return
    
    await data_fetcher.close()
    db_manager.close()

    if not historical_bars or len(historical_bars) < min_bars_needed:
        logger.error(f"Not enough data ({len(historical_bars)} bars vs {min_bars_needed} needed) for Phase 3 backtest. Aborting."); return
    
    data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
    data_df.set_index('timestamp', inplace=True)
    
    # Strategy will calculate ATR internally if 'atr_period' is in its params
    ewmac_params = settings.get_strategy_params("EWMAC")
    ewmac_strategy = EWMACStrategy(symbol=symbol, timeframe=timeframe, params=ewmac_params)

    position_sizer: Optional[Any] = None
    if settings.position_sizer_type.lower() == 'atrbased':
        position_sizer = ATRBasedPositionSizer(
            risk_per_trade_fraction=settings.atr_based_risk_per_trade_fraction,
            atr_multiple_for_stop=settings.atr_based_atr_multiple_for_stop
        )
    else: # Default to FixedFractional
        position_sizer = FixedFractionalPositionSizer(fraction=settings.fixed_fractional_allocation_fraction)
    logger.info(f"Using Position Sizer: {position_sizer.__class__.__name__}")

    stop_manager: Optional[Any] = None
    if settings.stop_manager_type.lower() == 'atrbased':
        stop_manager = ATRStopManager(atr_multiple=settings.atr_stop_atr_multiple)
    else: # Default to PercentageBased
        stop_manager = PercentageStopManager(
            stop_loss_pct=settings.percentage_stop_loss_pct,
            take_profit_pct=settings.percentage_stop_take_profit_pct
        )
    logger.info(f"Using Stop Manager: {stop_manager.__class__.__name__}")

    initial_capital = 10000.00
    commission_bps = settings.commission_bps
    backtest_engine = BacktestingEngine(
        data_feed_df=data_df,
        strategy=ewmac_strategy,
        initial_capital=initial_capital,
        commission_bps=commission_bps,
        position_sizer=position_sizer,
        stop_manager=stop_manager
    )
    logger.info(f"Running Phase 3 backtest (EWMAC with Risk Management) on {symbol}...")
    trades_log, final_portfolio = backtest_engine.run()

    if trades_log:
        logger.info(f"Backtest (Phase 3) completed. Generated {len(trades_log)} trades.")
        performance_analyzer = PerformanceAnalyzer(trades=trades_log, initial_capital=initial_capital, final_capital=final_portfolio['final_portfolio_value'])
        metrics = performance_analyzer.calculate_metrics()
        performance_analyzer.print_summary(metrics)
    else:
        logger.info(f"Backtest (Phase 3) completed. No trades executed. Final portfolio: ${final_portfolio['final_portfolio_value']:.2f}")
    root_logger.info("Phase 3 Demonstration completed.")


async def run_phase4_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 4: AI News & Sentiment Integration")
    if not settings: root_logger.critical("Settings failed to load."); return

    # 1. News Scraping (Optional live scrape for demo, or assume pre-populated DB)
    if settings.news_scraper_enable:
        logger.info("--- Running News Scraper (Phase 4 Demo) ---")
        try:
            news_scraper = NewsScraper() # Configured from settings
            # Scrape recent articles, e.g., last 48 hours for RSS, limit per source
            scraped_articles = await news_scraper.scrape_all(limit_per_source=5, since_hours_rss=48)
            if scraped_articles:
                logger.info(f"Scraped {len(scraped_articles)} unique articles.")
                
                # 2. Sentiment Analysis (Optional live analysis for demo)
                if settings.enable_sentiment_analysis and settings.sentiment_llm_provider == "VertexAI" and settings.vertex_ai_project_id:
                    logger.info("--- Running Sentiment Analyzer on Scraped Articles (Phase 4 Demo) ---")
                    try:
                        sentiment_analyzer = SentimentAnalyzer() # Configured from settings
                        analyzed_articles: List[NewsArticle] = []
                        for article_to_analyze in scraped_articles[:5]: # Analyze first 5 for demo speed
                            logger.info(f"Analyzing sentiment for: {article_to_analyze.title[:50]}...")
                            updated_article = await sentiment_analyzer.get_sentiment_for_article(article_to_analyze)
                            analyzed_articles.append(updated_article)
                            if updated_article.sentiment_label:
                                logger.info(f"  -> Sentiment: {updated_article.sentiment_label} ({updated_article.sentiment_score:.2f})")
                        
                        # Store analyzed articles (including sentiment) in DB
                        if analyzed_articles:
                            db_manager_news = DatabaseManager()
                            db_manager_news.store_news_articles(analyzed_articles)
                            logger.info(f"Stored {len(analyzed_articles)} analyzed articles in the database.")
                            db_manager_news.close()
                    except Exception as e_sa_live:
                        logger.error(f"Error during live sentiment analysis demo: {e_sa_live}", exc_info=True)
                elif not settings.enable_sentiment_analysis:
                    logger.info("Live sentiment analysis is disabled in settings.")
                elif settings.sentiment_llm_provider != "VertexAI" or not settings.vertex_ai_project_id:
                     logger.warning("Live sentiment analysis configured for non-VertexAI or VertexAI not fully configured (Project ID missing). Skipping live analysis demo.")
            else:
                logger.info("No articles scraped in this run.")
        except Exception as e_scrape_live:
            logger.error(f"Error during live news scraping demo: {e_scrape_live}", exc_info=True)
    else:
        logger.info("News Scraper is disabled in settings. Skipping live scraping/analysis for Phase 4 demo.")


    # 3. Backtesting with Simulated Sentiment Data (Primary focus of Phase 4 completion)
    logger.info("--- Proceeding to backtest with sentiment integration using simulated data ---")
    db_manager_bt = DatabaseManager()
    data_fetcher_bt = DataFetcher()
    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    hist_days = settings.historical_data_days if settings.historical_data_days > 0 else 90 # Shorter for faster demo if needed
    start_date_bt = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_date_bt = datetime.now(timezone.utc)

    ewmac_params_phase4 = settings.get_strategy_params("EWMAC")
    atr_period_phase4 = int(ewmac_params_phase4.get("atr_period", settings.ewmac_atr_period))
    long_window_phase4 = int(ewmac_params_phase4.get("longwindow", settings.ewmac_long_window))
    min_bars_needed_phase4 = max(long_window_phase4, atr_period_phase4) + 5

    historical_bars_bt = db_manager_bt.retrieve_bar_data(symbol, timeframe, start_date_bt, end_date_bt)
    if not historical_bars_bt or len(historical_bars_bt) < min_bars_needed_phase4:
        logger.info(f"Fetching fresh data for Phase 4 backtest (need ~{min_bars_needed_phase4} bars)...")
        historical_bars_bt = await data_fetcher_bt.fetch_historical_data_for_period(symbol, timeframe, start_date_bt, end_date_bt)
        if historical_bars_bt: db_manager_bt.store_bar_data(historical_bars_bt)
        else: logger.error(f"Failed to fetch data for {symbol}. Aborting Phase 4 backtest."); await data_fetcher_bt.close(); db_manager_bt.close(); return
    
    await data_fetcher_bt.close()
    db_manager_bt.close()

    if not historical_bars_bt or len(historical_bars_bt) < min_bars_needed_phase4:
        logger.error(f"Not enough data ({len(historical_bars_bt)} bars) for Phase 4 backtest. Aborting."); return

    data_df_bt = pd.DataFrame([bar.model_dump() for bar in historical_bars_bt])
    data_df_bt['timestamp'] = pd.to_datetime(data_df_bt['timestamp'])
    data_df_bt.set_index('timestamp', inplace=True)

    # Load Simulated Sentiment Data
    sentiment_df: Optional[pd.DataFrame] = None
    if settings.simulated_sentiment_data_path and settings.enable_sentiment_analysis:
        sentiment_csv_path = settings.simulated_sentiment_data_path
        logger.info(f"Attempting to load simulated sentiment data from: {sentiment_csv_path}")
        if os.path.exists(sentiment_csv_path):
            try:
                sentiment_df = pd.read_csv(sentiment_csv_path, parse_dates=['timestamp'], index_col='timestamp')
                if sentiment_df.empty:
                    logger.warning(f"Simulated sentiment data file is empty: {sentiment_csv_path}"); sentiment_df = None
                else:
                    if sentiment_df.index.tz is None: sentiment_df.index = sentiment_df.index.tz_localize('UTC')
                    else: sentiment_df.index = sentiment_df.index.tz_convert('UTC')
                    if 'sentiment_score' not in sentiment_df.columns:
                        logger.error(f"'sentiment_score' column not found in {sentiment_csv_path}. Sentiment will not be used."); sentiment_df = None
                    else:
                        logger.info(f"Successfully loaded {len(sentiment_df)} simulated sentiment entries from: {sentiment_csv_path}")
            except Exception as e_csv:
                logger.error(f"Error loading simulated sentiment data from {sentiment_csv_path}: {e_csv}. Proceeding without sentiment.", exc_info=True); sentiment_df = None
        else:
            logger.warning(f"Simulated sentiment data file NOT FOUND at: {sentiment_csv_path}. Proceeding without external sentiment.")
    elif not settings.enable_sentiment_analysis:
        logger.info("Sentiment analysis is disabled in settings. Backtest will not use external sentiment data.")
    else: # Path not provided
        logger.info("No simulated sentiment data path provided. Proceeding without external sentiment.")

    ewmac_strategy_sentiment = EWMACStrategy(symbol=symbol, timeframe=timeframe, params=ewmac_params_phase4)
    
    position_sizer_sent: Optional[Any] = None
    if settings.position_sizer_type.lower() == 'atrbased':
        position_sizer_sent = ATRBasedPositionSizer(settings.atr_based_risk_per_trade_fraction, settings.atr_based_atr_multiple_for_stop)
    else:
        position_sizer_sent = FixedFractionalPositionSizer(settings.fixed_fractional_allocation_fraction)

    stop_manager_sent: Optional[Any] = None
    if settings.stop_manager_type.lower() == 'atrbased':
        stop_manager_sent = ATRStopManager(settings.atr_stop_atr_multiple)
    else:
        stop_manager_sent = PercentageStopManager(settings.percentage_stop_loss_pct, settings.percentage_stop_take_profit_pct)

    initial_capital = 10000.00
    commission_bps = settings.commission_bps
    
    backtest_engine_sentiment = BacktestingEngine(
        data_feed_df=data_df_bt,
        strategy=ewmac_strategy_sentiment,
        initial_capital=initial_capital,
        commission_bps=commission_bps,
        position_sizer=position_sizer_sent,
        stop_manager=stop_manager_sent,
        sentiment_data_df=sentiment_df # Pass loaded sentiment data
    )
    logger.info(f"Running Phase 4 backtest (EWMAC with Sentiment & Risk Mgmt) on {symbol}...")
    trades_log, final_portfolio = backtest_engine_sentiment.run()

    if trades_log:
        logger.info(f"Backtest (Phase 4) completed. Generated {len(trades_log)} trades.")
        performance_analyzer = PerformanceAnalyzer(trades=trades_log, initial_capital=initial_capital, final_capital=final_portfolio['final_portfolio_value'])
        metrics = performance_analyzer.calculate_metrics()
        performance_analyzer.print_summary(metrics)
    else:
        logger.info(f"Backtest (Phase 4) completed. No trades executed. Final portfolio: ${final_portfolio['final_portfolio_value']:.2f}")
    root_logger.info("Phase 4 Demonstration completed.")


async def main():
    root_logger.info("Kamikaze Komodo Program Starting...")
    if not settings:
        root_logger.critical("Settings failed to load. Application cannot start.")
        return

    # --- Select phases to run ---
    # await run_phase1_demonstration()
    # await run_phase2_demonstration()
    # await run_phase3_demonstration()
    await run_phase4_demonstration()
    
    root_logger.info("Kamikaze Komodo Program Finished.")

if __name__ == "__main__":
    try:
        # Ensure GOOGLE_APPLICATION_CREDENTIALS is set in your environment for Vertex AI
        # e.g., export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-file.json"
        # Also, ensure playwright browsers are installed if using BrowserAgent:
        # playwright install --with-deps chromium
        if settings and settings.sentiment_llm_provider == "VertexAI" and not settings.vertex_ai_project_id:
             root_logger.warning("Vertex AI is selected, but Project ID is not set in config.ini. AI features may fail.")
             root_logger.warning("Please set your GCP Project ID in kamikaze_komodo/config/config.ini ([VertexAI] -> ProjectID)")
             root_logger.warning("And ensure GOOGLE_APPLICATION_CREDENTIALS environment variable is set.")


        asyncio.run(main())
    except KeyboardInterrupt:
        root_logger.info("Kamikaze Komodo program terminated by user.")
    except Exception as e:
        root_logger.critical(f"Critical error in main execution: {e}", exc_info=True)
</code>

kamikaze_komodo/__init__.py:
<code>
# kamikaze_komodo/__init__.py
# This file makes the 'root' directory a Python package.
</code>

kamikaze_komodo/ai_news_analysis_agent_module/browser_agent.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/browser_agent.py
import asyncio
from typing import Optional, Any, Dict
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Import global settings

logger = get_logger(__name__)

class BrowserAgent:
    """
    Uses browser-use with a configured LLM (e.g., Vertex AI's Gemini)
    to perform targeted market research.
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. BrowserAgent cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.llm_provider = settings.browser_agent_llm_provider
        self.llm: Optional[Any] = None # To be initialized
        self.agent_is_ready = False

        try:
            self._initialize_llm()
            # Dynamically import browser_use only if LLM init is successful
            global BrowserUseAgent # Make it global for the method if loaded
            from browser_use import Agent as BrowserUseAgent
            self.agent_is_ready = True
            logger.info("browser-use Agent component dynamically imported.")
        except ImportError:
            logger.error("browser-use library not found. Please install it: pip install browser-use")
            logger.error("Also run: playwright install --with-deps chromium")
        except Exception as e:
            logger.error(f"BrowserAgent initialization failed: {e}")


    def _initialize_llm(self):
        logger.info(f"Initializing LLM for BrowserAgent with provider: {self.llm_provider}")
        if self.llm_provider == "VertexAI":
            if not settings.vertex_ai_project_id or not settings.vertex_ai_location:
                logger.error("Vertex AI project ID or location is not configured. BrowserAgent LLM will not work.")
                raise ValueError("Vertex AI project ID or location missing for BrowserAgent.")
            try:
                from langchain_google_vertexai import ChatVertexAI
                self.llm = ChatVertexAI(
                    project=settings.vertex_ai_project_id,
                    location=settings.vertex_ai_location,
                    model_name=settings.vertex_ai_browser_agent_model_name, # Use specific model for browser agent
                    temperature=0.2, # Slightly higher temp for research/summarization
                    # max_output_tokens=2048, # Optional
                )
                logger.info(f"BrowserAgent initialized with Vertex AI model: {settings.vertex_ai_browser_agent_model_name}")
                logger.info("Ensure Google Cloud credentials (GOOGLE_APPLICATION_CREDENTIALS) are set for Vertex AI.")
            except ImportError:
                logger.error("langchain-google-vertexai not found. pip install langchain-google-vertexai")
                raise
            except Exception as e:
                logger.error(f"Failed to initialize Vertex AI LLM for BrowserAgent: {e}", exc_info=True)
                raise

        else:
            logger.error(f"Unsupported LLM provider for BrowserAgent: {self.llm_provider}")
            raise ValueError(f"Unsupported LLM provider for BrowserAgent: {self.llm_provider}")
        
        if self.llm is None:
            raise ValueError("BrowserAgent LLM initialization failed.")


    async def conduct_research(self, research_task: str, max_steps: int = 20, use_vision: bool = False) -> Optional[Dict[str, Any]]:
        """
        Conducts research based on the given task using browser-use.
        """
        if not self.agent_is_ready or self.llm is None:
            logger.error("BrowserAgent or its LLM not initialized. Cannot conduct research.")
            return None

        logger.info(f"Starting browser-use research task: '{research_task[:100]}...' (Max steps: {max_steps})")
        try:
            agent = BrowserUseAgent( # This is the dynamically imported class
                llm=self.llm,
                task=research_task,
                use_vision=use_vision,
                # verbose=True,
            )
            result = await agent.run(max_steps=max_steps)
            logger.info("Browser-use research task completed.")

            final_output_text = ""
            if isinstance(result, dict):
                logger.debug(f"Browser agent raw result dictionary keys: {result.keys()}")
                final_output_text = result.get('output', result.get('answer', str(result)))
            elif isinstance(result, str):
                final_output_text = result
            else:
                final_output_text = str(result)
                logger.warning(f"Unexpected result type from browser-use agent: {type(result)}. Content: {final_output_text[:500]}")

            logger.info(f"Browser Agent Output: {final_output_text[:500]}...")
            return {"output": final_output_text, "full_result": result}

        except Exception as e:
            logger.error(f"An error occurred while running the browser-use agent: {e}", exc_info=True)
            logger.error("Possible issues: LLM server, network, task complexity, or website automation challenges.")
            return None

async def main_browser_agent_example():
    if not settings or not settings.browser_agent_enable:
        logger.info("BrowserAgent is not enabled in settings or settings not loaded.")
        return
    if not settings.vertex_ai_project_id and settings.browser_agent_llm_provider == "VertexAI":
        logger.error("Vertex AI Project ID not set for BrowserAgent. Set GOOGLE_APPLICATION_CREDENTIALS.")
        return

    try:
        browser_agent = BrowserAgent()
        if not browser_agent.agent_is_ready:
            logger.error("Browser agent could not be initialized. Exiting example.")
            return
    except Exception as e:
        logger.error(f"Failed to create BrowserAgent: {e}")
        return

    task = (
        "What is the latest news regarding Ethereum's price action and upcoming upgrades in June 2025? "
        "Visit 2 reputable crypto news websites (e.g., Decrypt, Cointelegraph, but NOT CoinDesk). "
        "Summarize findings and list article URLs. Limit Browse to 4 steps per site."
    )
    result_data = await browser_agent.conduct_research(task, max_steps=settings.browser_agent_max_steps or 25)

    if result_data and "output" in result_data:
        logger.info("\n--- Browser Agent Research Result ---")
        print(result_data["output"])
    elif result_data:
        logger.info("\n--- Browser Agent Raw Research Result ---")
        print(result_data)
    else:
        logger.warning("Browser agent research did not produce a result or failed.")

if __name__ == "__main__":
    # Ensure GOOGLE_APPLICATION_CREDENTIALS is set for Vertex AI
    # Ensure Playwright browsers are installed: playwright install --with-deps chromium
    # This example is best run if BrowserAgent_Enable is true in config.
    if settings and settings.browser_agent_enable:
        asyncio.run(main_browser_agent_example())
    else:
        print("BrowserAgent is not enabled in settings, or settings failed to load. Skipping example.")
        print("To run, set BrowserAgent_Enable = True in config.ini and ensure Vertex AI is configured.")
</code>

kamikaze_komodo/ai_news_analysis_agent_module/news_scraper.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/news_scraper.py
import asyncio
import feedparser
import newspaper # type: ignore
import httpx
from typing import List, Optional, Dict, Any
# from bs4 import BeautifulSoup # Keep for potential future direct HTML parsing needs
from kamikaze_komodo.core.models import NewsArticle
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone, timedelta
from kamikaze_komodo.config.settings import settings # Import global settings

logger = get_logger(__name__)

class NewsScraper:
    """
    Scrapes news from specified sources (RSS feeds, websites).
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. NewsScraper cannot be initialized.")
            raise ValueError("Settings not loaded.")

        scraper_config = settings.get_news_scraper_config()
        self.rss_feeds: List[Dict[str, str]] = scraper_config.get("rss_feeds", [])
        self.websites_to_scrape: List[Dict[str, str]] = scraper_config.get("websites", []) # For Newspaper3k or custom BS4

        if not self.rss_feeds and not self.websites_to_scrape:
            logger.warning("NewsScraper initialized, but no RSS feeds or websites are configured in settings.")
        else:
            logger.info(f"NewsScraper initialized. RSS feeds: {len(self.rss_feeds)}, Websites: {len(self.websites_to_scrape)}")
            if self.rss_feeds:
                logger.debug(f"Configured RSS Feeds: {[feed['name'] for feed in self.rss_feeds]}")

    async def _fetch_url_content(self, url: str) -> Optional[str]:
        try:
            async with httpx.AsyncClient(timeout=20.0, follow_redirects=True) as client:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 KamikazeKomodoBot/1.0'
                }
                response = await client.get(url, headers=headers)
                response.raise_for_status()
                return response.text
        except httpx.RequestError as e:
            logger.error(f"HTTP request error fetching URL {url}: {e}")
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP status error fetching URL {url}: {e.response.status_code} - {e.response.text[:200]}")
        except Exception as e_gen:
            logger.error(f"Generic error fetching URL {url}: {e_gen}", exc_info=True)
        return None

    async def scrape_rss_feed(self, feed_name: str, feed_url: str, limit: int = 15) -> List[NewsArticle]:
        articles: List[NewsArticle] = []
        logger.info(f"Scraping RSS feed: {feed_name} from {feed_url}")
        
        feed_content = await self._fetch_url_content(feed_url)
        if not feed_content:
            logger.warning(f"Could not fetch content for RSS feed {feed_name} ({feed_url}). Skipping.")
            return articles

        try:
            loop = asyncio.get_event_loop()
            # feedparser is synchronous
            parsed_feed = await loop.run_in_executor(None, feedparser.parse, feed_content)

            if parsed_feed.bozo:
                logger.warning(f"Error parsing RSS feed {feed_name} ({feed_url}): {parsed_feed.bozo_exception}")
            
            if not parsed_feed.entries:
                logger.info(f"No entries found in RSS feed: {feed_name} ({feed_url}).")
                return articles

            for entry in parsed_feed.entries[:limit]:
                title = entry.get("title")
                link = entry.get("link")
                if not title or not link:
                    logger.debug(f"Skipping entry with missing title or link in {feed_name}: {entry.get('id', 'N/A')}")
                    continue

                published_time_struct = entry.get("published_parsed")
                updated_time_struct = entry.get("updated_parsed")
                
                pub_date: Optional[datetime] = None
                time_struct_to_use = published_time_struct or updated_time_struct

                if time_struct_to_use:
                    try:
                        pub_date = datetime(*time_struct_to_use[:6], tzinfo=timezone.utc)
                    except Exception as e_date:
                        logger.warning(f"Could not parse date for article '{title}' from {feed_name}: {time_struct_to_use}, error: {e_date}")
                
                # Fallback if date parsing fails or not present
                if pub_date is None:
                    pub_date = datetime.now(timezone.utc) # Use retrieval time as a last resort
                    logger.debug(f"Using current time as publication date for '{title}' from {feed_name} due to missing/unparseable date.")


                article_id = link # Use URL as a unique ID
                
                content_summary = entry.get("summary") or entry.get("description")
                
                # Attempt to extract related symbols from title or summary (basic)
                related_symbols = []
                text_for_symbols = (title + " " + (content_summary if content_summary else "")).lower()
                # This is very basic; a proper NER would be better
                common_crypto_symbols = {"btc", "bitcoin", "eth", "ethereum", "sol", "solana", "xrp", "ada", "cardano", "doge", "shib"}
                for sym in common_crypto_symbols:
                    if sym in text_for_symbols:
                        related_symbols.append(sym.upper())
                
                articles.append(NewsArticle(
                    id=article_id,
                    url=link,
                    title=title,
                    publication_date=pub_date,
                    retrieval_date=datetime.now(timezone.utc),
                    source=feed_name,
                    content=None, # Full content fetch can be added here or later by newspaper3k
                    summary=content_summary,
                    related_symbols=list(set(related_symbols)) # Unique symbols
                ))
            logger.info(f"Found {len(articles)} articles from RSS feed: {feed_name}")
        except Exception as e:
            logger.error(f"Failed to process RSS feed {feed_name} ({feed_url}): {e}", exc_info=True)
        return articles

    async def scrape_website_with_newspaper(self, site_name: str, site_url: str, limit_articles: int = 5) -> List[NewsArticle]:
        """Scrapes a website using Newspaper3k. Be mindful of terms of service."""
        articles_data: List[NewsArticle] = []
        logger.info(f"Scraping website: {site_name} ({site_url}) with Newspaper3k (limit: {limit_articles})")
        
        # Newspaper3k config
        config_np = newspaper.Config()
        config_np.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 KamikazeKomodoBot/1.0'
        config_np.request_timeout = 15
        config_np.memoize_articles = False # Disable caching for fresh data
        config_np.fetch_images = False # Don't need images
        config_np.verbose = False # newspaper's own verbosity

        try:
            loop = asyncio.get_event_loop()
            paper = await loop.run_in_executor(None, newspaper.build, site_url, config_np)
            
            count = 0
            for article_raw in paper.articles:
                if count >= limit_articles:
                    break
                try:
                    # Download and parse article content
                    await loop.run_in_executor(None, article_raw.download)
                    if not article_raw.is_downloaded:
                        logger.warning(f"Failed to download article: {article_raw.url} from {site_name}")
                        continue
                    await loop.run_in_executor(None, article_raw.parse)

                    title = article_raw.title
                    url = article_raw.url
                    if not title or not url:
                        logger.debug(f"Skipping article with no title/url from {site_name}")
                        continue

                    content = article_raw.text
                    summary_np = article_raw.summary # newspaper3k summary
                    
                    pub_date_dt = article_raw.publish_date
                    if pub_date_dt and pub_date_dt.tzinfo is None:
                        pub_date_dt = pub_date_dt.replace(tzinfo=timezone.utc) # Assume UTC if naive, or local if known
                    elif pub_date_dt is None:
                        pub_date_dt = datetime.now(timezone.utc) # Fallback
                    
                    related_symbols_np = []
                    text_for_symbols_np = (title + " " + (summary_np if summary_np else "") + " " + (content if content else "")).lower()
                    common_crypto_symbols_np = {"btc", "bitcoin", "eth", "ethereum", "sol", "solana", "xrp", "ada", "cardano", "doge", "shib"}
                    for sym_np in common_crypto_symbols_np:
                        if sym_np in text_for_symbols_np:
                            related_symbols_np.append(sym_np.upper())

                    articles_data.append(NewsArticle(
                        id=url, url=url, title=title,
                        publication_date=pub_date_dt,
                        retrieval_date=datetime.now(timezone.utc),
                        source=site_name,
                        content=content if content else None,
                        summary=summary_np if summary_np else None,
                        related_symbols=list(set(related_symbols_np))
                    ))
                    count += 1
                    logger.debug(f"Successfully scraped: {url} from {site_name}")
                except Exception as e_article:
                    logger.warning(f"Error processing article {article_raw.url} from {site_name} with Newspaper3k: {e_article}", exc_info=True)
            
            logger.info(f"Scraped {len(articles_data)} articles from {site_name} using Newspaper3k.")
        except Exception as e:
            logger.error(f"Failed to scrape website {site_name} ({site_url}) with Newspaper3k: {e}", exc_info=True)
        return articles_data

    async def scrape_all(self, limit_per_source: int = 10, since_hours_rss: Optional[int] = 24) -> List[NewsArticle]:
        """
        Scrapes all configured RSS feeds and websites.
        For RSS, optionally filters articles published within `since_hours_rss`.
        """
        all_articles: List[NewsArticle] = []
        
        # Scrape RSS Feeds
        rss_tasks = []
        if self.rss_feeds:
            for feed_info in self.rss_feeds:
                rss_tasks.append(self.scrape_rss_feed(feed_info['name'], feed_info['url'], limit=limit_per_source))
        
            rss_results_list = await asyncio.gather(*rss_tasks, return_exceptions=True)
            for result in rss_results_list:
                if isinstance(result, list):
                    all_articles.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"RSS scraping task failed: {result}", exc_info=True) # Log exception details
        else:
            logger.info("No RSS feeds configured to scrape.")

        # Filter RSS articles by publication date if since_hours_rss is provided
        if since_hours_rss is not None:
            cutoff_date = datetime.now(timezone.utc) - timedelta(hours=since_hours_rss)
            filtered_articles = []
            for article in all_articles:
                if article.publication_date and article.publication_date >= cutoff_date:
                    filtered_articles.append(article)
                elif not article.publication_date: # If no pub date, include it (conservative)
                    filtered_articles.append(article)
            count_removed = len(all_articles) - len(filtered_articles)
            if count_removed > 0:
                logger.info(f"Filtered out {count_removed} RSS articles older than {since_hours_rss} hours.")
            all_articles = filtered_articles

        # Scrape Websites (e.g., using Newspaper3k) - typically gets latest, less date control
        website_tasks = []
        if self.websites_to_scrape:
            for site_info in self.websites_to_scrape:
                website_tasks.append(self.scrape_website_with_newspaper(site_info['name'], site_info['url'], limit_articles=limit_per_source))
        
            website_results_list = await asyncio.gather(*website_tasks, return_exceptions=True)
            for result in website_results_list:
                if isinstance(result, list):
                    all_articles.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"Website scraping task failed: {result}", exc_info=True)
        else:
            logger.info("No direct websites configured to scrape with Newspaper3k.")
            
        # Deduplicate articles by URL (ID)
        unique_articles_dict: Dict[str, NewsArticle] = {}
        for article in all_articles:
            if article.id not in unique_articles_dict:
                 unique_articles_dict[article.id] = article
            else: # If duplicate, prefer the one with more content or later retrieval
                existing_article = unique_articles_dict[article.id]
                if (article.content and not existing_article.content) or \
                   (article.retrieval_date > existing_article.retrieval_date):
                    unique_articles_dict[article.id] = article

        unique_articles_list = sorted(list(unique_articles_dict.values()), key=lambda x: x.publication_date or x.retrieval_date, reverse=True)
        
        logger.info(f"Total unique articles scraped from all sources: {len(unique_articles_list)}")
        return unique_articles_list

async def main_scraper_example():
    if not settings or not settings.news_scraper_enable:
        logger.info("NewsScraper is not enabled in settings or settings not loaded.")
        return

    scraper = NewsScraper()
    
    # Scrape all configured sources, limiting to 5 articles per source,
    # and only RSS articles from the last 48 hours
    all_scraped_articles = await scraper.scrape_all(limit_per_source=5, since_hours_rss=48)
    
    logger.info(f"--- All Scraped Articles ({len(all_scraped_articles)}) ---")
    if not all_scraped_articles:
        logger.info("No articles were scraped.")
        return

    for i, article in enumerate(all_scraped_articles[:10]): # Log details for first 10
        logger.info(f"{i+1}. Source: {article.source}, Title: {article.title}")
        logger.info(f"   URL: {article.url}")
        logger.info(f"   Date: {article.publication_date}, Retrieved: {article.retrieval_date}")
        logger.info(f"   Symbols: {article.related_symbols}")
        if article.summary:
            logger.info(f"   Summary: {article.summary[:150]}...")
        # if article.content: # Content can be very long
            # logger.info(f" Content Preview: {article.content[:100]}...")
    
    # Example: Store articles in DB
    if all_scraped_articles:
        from kamikaze_komodo.data_handling.database_manager import DatabaseManager
        db_manager = DatabaseManager()
        db_manager.store_news_articles(all_scraped_articles)
        logger.info(f"Stored {len(all_scraped_articles)} articles in the database.")
        
        # Retrieve and show some from DB
        retrieved = db_manager.retrieve_news_articles(limit=5)
        logger.info(f"--- Retrieved {len(retrieved)} articles from DB ---")
        for art_db in retrieved:
            logger.info(f"DB: {art_db.title} (Source: {art_db.source}, Date: {art_db.publication_date})")
        db_manager.close()

if __name__ == "__main__":
    if settings and settings.news_scraper_enable:
        asyncio.run(main_scraper_example())
    else:
        print("NewsScraper is not enabled in settings, or settings failed to load. Skipping example.")
</code>

kamikaze_komodo/ai_news_analysis_agent_module/notification_listener.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/notification_listener.py
from kamikaze_komodo.app_logger import get_logger
# from jeepney import DBusAddress, new_method_call # If fully implementing
# from jeepney.io.asyncio import open_dbus_connection # If fully implementing
import asyncio
from typing import Callable, Awaitable, Dict, Any, Optional

logger = get_logger(__name__)

class NotificationListener:
    """
    Listens for desktop notifications using D-Bus (via Jeepney).
    Basic implementation for Phase 4. Full D-Bus interaction is complex and OS-dependent.
    """
    def __init__(self, callback_on_notification: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None):
        """
        Args:
            callback_on_notification: An async function to call when a notification is received.
                                      It should accept a dictionary with notification details.
        """
        self.callback_on_notification = callback_on_notification
        self._running = False
        logger.info("NotificationListener initialized.")
        if self.callback_on_notification is None:
            logger.warning("No callback provided for NotificationListener. It will not perform any actions on notifications.")
        logger.warning("Full D-Bus notification listening with Jeepney is a complex, OS-dependent feature and is currently a placeholder.")
        logger.warning("To enable, ensure Jeepney is installed and D-Bus is correctly configured on your Linux desktop.")


    async def start_listening(self):
        """
        Starts listening for D-Bus notifications. Placeholder for Jeepney implementation.
        """
        self._running = True
        logger.info("Notification listener started (simulated/placeholder).")
        if self.callback_on_notification is None:
            logger.error("NotificationListener started but no callback is set. It will be idle.")
            # We can simply return or let the placeholder loop run idly.

        # --- Conceptual Jeepney Implementation (Highly OS/Setup Dependent) ---
        # try:
        #     from jeepney import DBusAddress, new_method_call
        #     from jeepney.io.asyncio import open_dbus_connection
        #     logger.info("Attempting to connect to D-Bus for notifications...")
        #     conn = await open_dbus_connection()
        #     logger.info("Connected to D-Bus. Setting up match rule for org.freedesktop.Notifications.Notify.")
        #
        #     # Interface to call AddMatch on (DBUS daemon itself)
        #     dbus_daemon_addr = DBusAddress('org.freedesktop.DBus', '/org/freedesktop/DBus', 'org.freedesktop.DBus')
        #     match_rule = "type='signal',interface='org.freedesktop.Notifications',member='Notify'"
        #     add_match_msg = new_method_call(dbus_daemon_addr, 'AddMatch', 's', (match_rule,))
        #     await conn.send_and_get_reply(add_match_msg) # No reply expected or needed for AddMatch usually
        #     logger.info(f"Match rule added: {match_rule}")
        #
        #     while self._running:
        #         try:
        #             msg_received = await asyncio.wait_for(conn.receive(), timeout=1.0) # Add timeout
        #             if msg_received and msg_received.member == 'Notify' and msg_received.interface == 'org.freedesktop.Notifications':
        #                 parsed_notification = self.parse_notification_data(msg_received.body)
        #                 if self.callback_on_notification and parsed_notification:
        #                     logger.info(f"Received D-Bus Notification: {parsed_notification.get('summary')}")
        #                     await self.callback_on_notification(parsed_notification)
        #             elif msg_received:
        #                 logger.debug(f"Received other D-Bus message: Member={msg_received.member}, Interface={msg_received.interface}")
        #         except asyncio.TimeoutError:
        #             continue # Just to allow checking self._running
        #         except Exception as e_recv:
        #             logger.error(f"Error receiving/processing D-Bus message: {e_recv}", exc_info=True)
        #             await asyncio.sleep(5) # Avoid rapid error loops
        # except ImportError:
        #     logger.error("Jeepney library not found. D-Bus notification listener cannot run. Please install it.")
        # except ConnectionRefusedError:
        #     logger.error("Could not connect to D-Bus. Ensure D-Bus daemon is running and accessible.")
        # except Exception as e:
        #     logger.error(f"An error occurred in D-Bus notification listener setup: {e}", exc_info=True)
        # finally:
        #     if 'conn' in locals() and conn:
        #         # Optionally remove match rule before closing
        #         # remove_match_msg = new_method_call(dbus_daemon_addr, 'RemoveMatch', 's', (match_rule,))
        #         # await conn.send_and_get_reply(remove_match_msg)
        #         await conn.close()
        #         logger.info("D-Bus connection closed.")
        #     self._running = False
        # --- End Conceptual Jeepney ---

        # Current Placeholder Loop
        while self._running:
            await asyncio.sleep(30)
            if self.callback_on_notification is not None: # Only log if it's supposed to be doing something
                logger.debug("Notification listener placeholder task running (no actual D-Bus listening)...")

    def stop_listening(self):
        self._running = False
        logger.info("Notification listener stopped (simulated/placeholder).")

    def parse_notification_data(self, notification_body: tuple) -> Optional[Dict[str, Any]]:
        """
        Parses the D-Bus notification data (signal body for Notify) into a structured dictionary.
        Standard Notify signature: (app_name, replaces_id, app_icon, summary, body, actions, hints, expire_timeout)
                                  (s, u, s, s, s, as, a{sv}, i)
        """
        try:
            if isinstance(notification_body, tuple) and len(notification_body) == 8:
                return {
                    "app_name": notification_body[0],
                    "replaces_id": notification_body[1], # uint32
                    "app_icon": notification_body[2],
                    "summary": notification_body[3],  # Title
                    "body": notification_body[4],     # Message
                    "actions": notification_body[5], # List of strings (action identifiers)
                    "hints": notification_body[6],   # Dict of variant hints
                    "expire_timeout": notification_body[7] # int32
                }
            else:
                logger.warning(f"Received notification body with unexpected format or length: {notification_body}")
        except Exception as e:
            logger.error(f"Error parsing notification data: {e}. Body was: {notification_body}", exc_info=True)
        return None

async def dummy_notification_callback(notification_details: Dict[str, Any]):
    logger.info(f"Dummy Callback: Received Notification - Summary: '{notification_details.get('summary')}', Body: '{notification_details.get('body')}'")
    # Here, you might trigger news analysis, sentiment analysis, or other actions.

# Example:
# if __name__ == "__main__":
#     listener = NotificationListener(callback_on_notification=dummy_notification_callback)
#     # To test this, you would need a D-Bus environment (Linux desktop) and send a notification:
#     # e.g., using `notify-send "Test Summary" "This is a test notification body."` in terminal.
#     try:
#         asyncio.run(listener.start_listening())
#     except KeyboardInterrupt:
#         listener.stop_listening()
#         logger.info("Notification listener example stopped by user.")
</code>

kamikaze_komodo/ai_news_analysis_agent_module/sentiment_analyzer.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/sentiment_analyzer.py
from typing import List, Dict, Optional, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field as PydanticField
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.core.models import NewsArticle
from kamikaze_komodo.config.settings import settings # Import global settings

logger = get_logger(__name__)

# Pydantic model for structured sentiment output
class SentimentAnalysisOutput(BaseModel):
    sentiment_label: str = PydanticField(description="The overall sentiment (e.g., 'very bullish', 'bullish', 'neutral', 'bearish', 'very bearish', 'mixed').")
    sentiment_score: float = PydanticField(description="A numerical score from -1.0 (very negative) to 1.0 (very positive). Neutral is 0.0.")
    key_themes: Optional[List[str]] = PydanticField(default_factory=list, description="List of key themes or topics identified in the text related to sentiment.")
    confidence: Optional[float] = PydanticField(description="Confidence score of the sentiment analysis (0.0 to 1.0).")

class SentimentAnalyzer:
    """
    Analyzes text for sentiment using a configured LLM via Langchain.
    Supports Google Vertex AI.
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. SentimentAnalyzer cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.llm_provider = settings.sentiment_llm_provider
        self.llm: Any = None # Will be initialized in _initialize_llm

        self._initialize_llm()

        # Define a structured prompt for sentiment analysis
        self.prompt_template = ChatPromptTemplate.from_messages(
            [
                ("system",
                 "You are an expert financial sentiment analyst specializing in cryptocurrency markets. "
                 "Analyze the provided text for its sentiment towards the cryptocurrency or market mentioned. "
                 "Consider factors like news events, market reactions, technological developments, and regulatory news. "
                 "Your output MUST be in JSON format, adhering to the following Pydantic model: "
                 "```json\n"
                 "{\n"
                 "  \"sentiment_label\": \"<label: 'very bullish'|'bullish'|'neutral'|'bearish'|'very bearish'|'mixed'>\",\n"
                 "  \"sentiment_score\": <score_float: -1.0 to 1.0>,\n"
                 "  \"key_themes\": [\"<theme1>\", \"<theme2>\"],\n"
                 "  \"confidence\": <confidence_float: 0.0 to 1.0>\n"
                 "}\n"
                 "```"
                 "sentiment_score should range from -1.0 (very bearish/negative) to 1.0 (very bullish/positive). Neutral is 0.0. "
                 "key_themes should highlight important topics influencing the sentiment. confidence is your perceived accuracy of this analysis (0.0 to 1.0)."
                 ),
                ("human", "Please analyze the sentiment of the following text regarding {asset_context}:\n\n---\n{text_to_analyze}\n---"),
            ]
        )
        self.output_parser = JsonOutputParser(pydantic_object=SentimentAnalysisOutput)
        self.chain = self.prompt_template | self.llm | self.output_parser

    def _initialize_llm(self):
        logger.info(f"Initializing LLM for SentimentAnalyzer with provider: {self.llm_provider}")
        if self.llm_provider == "VertexAI":
            if not settings.vertex_ai_project_id or not settings.vertex_ai_location:
                logger.error("Vertex AI project ID or location is not configured in settings.py. Sentiment analysis will not work.")
                raise ValueError("Vertex AI project ID or location missing.")
            try:
                from langchain_google_vertexai import ChatVertexAI
                self.llm = ChatVertexAI(
                    project=settings.vertex_ai_project_id,
                    location=settings.vertex_ai_location,
                    model_name=settings.vertex_ai_sentiment_model_name,
                    temperature=0.1,
                    # max_output_tokens=1024, # Optional: if needed
                )
                logger.info(f"SentimentAnalyzer initialized with Vertex AI model: {settings.vertex_ai_sentiment_model_name}")
                logger.info("Ensure Google Cloud credentials (GOOGLE_APPLICATION_CREDENTIALS) are set in your environment.")
            except ImportError:
                logger.error("langchain-google-vertexai is not installed. Please install it: pip install langchain-google-vertexai")
                raise
            except Exception as e:
                logger.error(f"Failed to initialize Vertex AI LLM ({settings.vertex_ai_sentiment_model_name}): {e}", exc_info=True)
                raise

        else:
            logger.error(f"Unsupported LLM provider: {self.llm_provider}")
            raise ValueError(f"Unsupported LLM provider: {self.llm_provider}")

        if self.llm is None:
            logger.error("LLM could not be initialized.")
            raise ValueError("LLM initialization failed.")

    async def analyze_sentiment_structured(self, text: str, asset_context: str = "the market") -> Optional[SentimentAnalysisOutput]:
        """
        Analyzes text and returns a structured sentiment analysis including score and label.
        """
        if not text or not text.strip():
            logger.warning("No text provided for sentiment analysis.")
            return None
        
        if self.llm is None:
            logger.error("LLM not initialized. Cannot analyze sentiment.")
            return None

        logger.debug(f"Analyzing sentiment for text (context: {asset_context}): '{text[:200]}...'")
        try:
            # A more robust solution would use token counting. This is a simple character limit.
            # Gemini Flash models usually have a large context window (e.g., 128k tokens for gemini-1.5-flash variants)
            # but the older flash might be 32k. Max input tokens for gemini-2.5-flash-preview is high, let's be generous.
            # The prompt itself consumes tokens.
            max_chars = 25000 # Approx 6k-8k tokens.
            if len(text) > max_chars:
                logger.warning(f"Text too long ({len(text)} chars), truncating to {max_chars} for sentiment analysis.")
                text = text[:max_chars]

            response = await self.chain.ainvoke({"text_to_analyze": text, "asset_context": asset_context})
            
            if isinstance(response, dict):
                try:
                    validated_output = SentimentAnalysisOutput(**response)
                    logger.info(f"Structured sentiment for '{asset_context}': Label: {validated_output.sentiment_label}, Score: {validated_output.sentiment_score:.2f}, Confidence: {validated_output.confidence}")
                    return validated_output
                except Exception as p_exc: 
                    logger.error(f"Pydantic validation failed for LLM JSON output: {response}. Error: {p_exc}")
                    return None
            else:
                logger.error(f"Unexpected structured sentiment analysis output type: {type(response)}. Content: {response}")
                return None
        except Exception as e:
            logger.error(f"Error during structured sentiment analysis with {self.llm_provider} model: {e}", exc_info=True)
            return None

    async def get_sentiment_for_article(self, article: NewsArticle, asset_context: Optional[str] = None) -> NewsArticle:
        """
        Analyzes sentiment for a NewsArticle object and updates its sentiment fields.
        Uses article title and summary/content.
        """
        if not asset_context and article.related_symbols:
            asset_context = ", ".join(article.related_symbols)
        elif not asset_context:
            # Try to infer from title if no symbols
            # This is a simple heuristic; more advanced would be NER
            if "bitcoin" in article.title.lower() or "btc" in article.title.lower():
                 asset_context = "Bitcoin"
            elif "ethereum" in article.title.lower() or "eth" in article.title.lower():
                 asset_context = "Ethereum"
            else:
                 asset_context = "the cryptocurrency market"

        text_to_analyze = article.title
        if article.summary:
            text_to_analyze += "\n\n" + article.summary
        elif article.content: # Fallback to content if no summary
            text_to_analyze += "\n\n" + article.content

        if not text_to_analyze.strip():
            logger.warning(f"No text content found in article {article.id} to analyze.")
            return article # Return original article if no text

        sentiment_result = await self.analyze_sentiment_structured(text_to_analyze, asset_context=asset_context)

        if sentiment_result:
            article.sentiment_label = sentiment_result.sentiment_label
            article.sentiment_score = sentiment_result.sentiment_score
            article.key_themes = sentiment_result.key_themes
            article.sentiment_confidence = sentiment_result.confidence
            # article.raw_llm_response can store the full dict if needed for debugging
            # article.raw_llm_response = sentiment_result.model_dump() # Stores the Pydantic model as dict
        return article

# Example Usage
async def main_sentiment_example():
    """ Example of using the SentimentAnalyzer """
    if not settings or not settings.vertex_ai_project_id:
        logger.error("Vertex AI settings (Project ID) not loaded for sentiment example. Set GOOGLE_APPLICATION_CREDENTIALS.")
        return

    try:
        analyzer = SentimentAnalyzer()
    except Exception as e:
        logger.error(f"Could not start SentimentAnalyzer: {e}")
        return

    example_texts = [
        ("Bitcoin surges past $70,000, analysts predict further upside due to ETF inflows and positive market structure.", "Bitcoin"),
        ("Regulatory crackdown imminent? SEC chair issues stark warning on crypto staking, leading to market jitters.", "Cryptocurrency Regulation"),
        ("Ethereum's Dencun upgrade successfully goes live on mainnet, promising significantly lower fees for Layer 2 solutions and boosting scalability.", "Ethereum"),
        ("The crypto market remains flat this week with low volatility and trading volume, investors seem hesitant.", "the crypto market"),
        ("Solana's network outage causes temporary panic, but recovery was swift. Developers are addressing the root cause.", "Solana")
    ]

    for text, context in example_texts:
        logger.info(f"\n--- Analyzing text for '{context}' ---")
        logger.info(f"Text: {text}")
        result = await analyzer.analyze_sentiment_structured(text, asset_context=context)
        if result:
            logger.info(f"  Sentiment Label: {result.sentiment_label}")
            logger.info(f"  Sentiment Score: {result.sentiment_score:.3f}")
            logger.info(f"  Key Themes: {result.key_themes}")
            logger.info(f"  Confidence: {result.confidence}")
        else:
            logger.warning("  Failed to get structured sentiment analysis.")

    sample_article = NewsArticle(
        id="test_article_sol_123",
        url="http://example.com/news_sol_1",
        title="Solana Ecosystem Sees Major Investment for DeFi Growth",
        summary="The Solana Foundation has announced a new $100 million fund dedicated to fostering DeFi projects on its blockchain. This move is expected to attract more developers and users, with SOL token price reacting positively.",
        source="Crypto News Daily",
        related_symbols=["SOL", "Solana"]
    )
    logger.info(f"\n--- Analyzing NewsArticle: {sample_article.title} ---")
    updated_article = await analyzer.get_sentiment_for_article(sample_article)
    logger.info(f"  Analyzed Article Sentiment: Label='{updated_article.sentiment_label}', Score={updated_article.sentiment_score}, Themes: {updated_article.key_themes}")

if __name__ == "__main__":
    import asyncio
    # Ensure GOOGLE_APPLICATION_CREDENTIALS is set in your environment
    # e.g., export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-file.json"
    asyncio.run(main_sentiment_example())
</code>

kamikaze_komodo/ai_news_analysis_agent_module/__init__.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/__init__.py
# This file makes the 'ai_news_analysis_agent_module' directory a Python package.
</code>

kamikaze_komodo/backtesting_engine/engine.py:
<code>
# kamikaze_komodo/backtesting_engine/engine.py
# Significantly updated to integrate PositionSizer and StopManager
# And to handle sentiment data (conceptual for now)

import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from kamikaze_komodo.core.models import BarData, Trade, Order # Order not fully used
from kamikaze_komodo.core.enums import SignalType, OrderSide, TradeResult
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone

# Phase 3 imports
from kamikaze_komodo.risk_control_module.position_sizer import BasePositionSizer, FixedFractionalPositionSizer # Default
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager, PercentageStopManager # Default
from kamikaze_komodo.portfolio_constructor.asset_allocator import BaseAssetAllocator # For future use

logger = get_logger(__name__)

class BacktestingEngine:
    def __init__(
        self,
        data_feed_df: pd.DataFrame,
        strategy: BaseStrategy,
        initial_capital: float = 10000.0,
        commission_bps: float = 0.0, # e.g., 10 bps = 0.1%
        position_sizer: Optional[BasePositionSizer] = None,
        stop_manager: Optional[BaseStopManager] = None,
        # For Phase 4: sentiment data
        sentiment_data_df: Optional[pd.DataFrame] = None # Timestamp-indexed series/df with sentiment scores
    ):
        if data_feed_df.empty: raise ValueError("Data feed DataFrame cannot be empty.")
        if not isinstance(data_feed_df.index, pd.DatetimeIndex):
            raise ValueError("Data feed DataFrame must be indexed by pd.DatetimeIndex.")
        
        self.data_feed_df = data_feed_df.sort_index()
        self.strategy = strategy
        self.initial_capital = initial_capital
        self.commission_rate = commission_bps / 10000.0

        # Initialize Risk and Portfolio Components (Phase 3)
        self.position_sizer = position_sizer if position_sizer else FixedFractionalPositionSizer(fraction=0.1) # Default 10% equity allocation
        self.stop_manager = stop_manager if stop_manager else PercentageStopManager(stop_loss_pct=None, take_profit_pct=None) # Default no SL/TP from engine
        
        # Phase 4: Sentiment Data
        self.sentiment_data_df = sentiment_data_df # Expects 'timestamp' index and 'sentiment_score' column
        if self.sentiment_data_df is not None and not self.sentiment_data_df.empty:
            if not isinstance(self.sentiment_data_df.index, pd.DatetimeIndex):
                logger.warning("Sentiment data DataFrame must be indexed by pd.DatetimeIndex. Sentiment will not be used.")
                self.sentiment_data_df = None
            else: # Ensure timezone consistency (UTC)
                if self.sentiment_data_df.index.tz is None:
                    self.sentiment_data_df.index = self.sentiment_data_df.index.tz_localize('UTC')
                else:
                    self.sentiment_data_df.index = self.sentiment_data_df.index.tz_convert('UTC')
                logger.info(f"Sentiment data loaded with {len(self.sentiment_data_df)} entries.")


        self.portfolio_history: List[Dict[str, Any]] = []
        self.trades_log: List[Trade] = []
        
        self.current_cash = initial_capital
        self.current_asset_value = 0.0 # Mark-to-market value of assets held
        self.current_portfolio_value = initial_capital # Total equity (cash + asset_value)
        
        self.active_trade: Optional[Trade] = None # Stores the currently open trade object
        self.trade_id_counter = 0

        logger.info(
            f"BacktestingEngine initialized for strategy '{strategy.name}' on symbol '{strategy.symbol}'. "
            f"Initial Capital: ${initial_capital:,.2f}, Commission: {commission_bps} bps."
        )
        logger.info(f"Position Sizer: {self.position_sizer.__class__.__name__}")
        logger.info(f"Stop Manager: {self.stop_manager.__class__.__name__}")
        if self.sentiment_data_df is not None and not self.sentiment_data_df.empty :
            logger.info("Sentiment data will be used in this backtest.")


    def _get_next_trade_id(self) -> str:
        self.trade_id_counter += 1
        return f"trade_{self.trade_id_counter:05d}"

    def _update_portfolio_value(self, current_bar_close_price: Optional[float] = None):
        """Updates current_asset_value and current_portfolio_value."""
        if self.active_trade and current_bar_close_price is not None:
            self.current_asset_value = self.active_trade.amount * current_bar_close_price
        else:
            self.current_asset_value = 0.0
        self.current_portfolio_value = self.current_cash + self.current_asset_value


    def _execute_trade(
        self, 
        signal_type: SignalType, 
        timestamp: datetime, 
        price: float, # Execution price
        current_bar_for_atr_calc: Optional[BarData] = None # For ATR based sizer/stop
    ):
        commission_cost = 0.0
        trade_executed = False

        # --- POSITION ENTRY ---
        if signal_type == SignalType.LONG and self.active_trade is None:
            # Use PositionSizer
            atr_value_for_sizing = current_bar_for_atr_calc.atr if current_bar_for_atr_calc and hasattr(current_bar_for_atr_calc, 'atr') else None
            
            position_size_units = self.position_sizer.calculate_size(
                symbol=self.strategy.symbol,
                current_price=price,
                available_capital=self.current_cash,
                current_portfolio_value=self.current_portfolio_value,
                latest_bar=current_bar_for_atr_calc, # Pass current bar for potential ATR calc by sizer
                atr_value=atr_value_for_sizing # Pass ATR if strategy calculated it
            )

            if position_size_units is None or position_size_units <= 0:
                logger.debug(f"{timestamp} - Cannot enter LONG trade for {self.strategy.symbol}. PositionSizer returned no size or zero size.")
                return

            cost_of_assets = position_size_units * price
            commission_cost = cost_of_assets * self.commission_rate

            if cost_of_assets + commission_cost > self.current_cash:
                logger.warning(f"{timestamp} - Insufficient cash for LONG trade on {self.strategy.symbol}. Need {cost_of_assets + commission_cost:.2f}, have {self.current_cash:.2f}. Reducing size or skipping.")
                # Optionally, try to resize with available cash, or just skip
                adjusted_size_units = (self.current_cash - commission_cost) / price # Simplified adjustment attempt
                if adjusted_size_units * price * (1 + self.commission_rate) > self.current_cash or adjusted_size_units <= 0:
                     logger.warning(f"{timestamp} - Still insufficient cash after adjustment. Skipping trade.")
                     return
                position_size_units = adjusted_size_units
                cost_of_assets = position_size_units * price
                commission_cost = cost_of_assets * self.commission_rate


            self.current_cash -= (cost_of_assets + commission_cost)
            
            # ATR at entry for ATRStopManager (if strategy provides it on BarData or calculates it)
            atr_at_entry = current_bar_for_atr_calc.atr if current_bar_for_atr_calc and current_bar_for_atr_calc.atr else None

            self.active_trade = Trade(
                id=self._get_next_trade_id(),
                symbol=self.strategy.symbol,
                entry_order_id=f"entry_{self.trade_id_counter}",
                side=OrderSide.BUY,
                entry_price=price,
                amount=position_size_units,
                entry_timestamp=timestamp,
                commission=commission_cost,
                custom_fields={"atr_at_entry": atr_at_entry} if atr_at_entry else {}
            )
            self._update_portfolio_value(price) # Update portfolio value with new asset holding
            logger.info(
                f"{timestamp} - EXECUTE LONG: {position_size_units:.6f} {self.strategy.symbol} @ ${price:.2f}. "
                f"Cost: ${cost_of_assets:.2f}, Comm: ${commission_cost:.2f}. Cash Left: ${self.current_cash:.2f}. Equity: ${self.current_portfolio_value:.2f}"
            )
            trade_executed = True

        # --- POSITION EXIT (e.g. from strategy signal) ---
        elif signal_type == SignalType.CLOSE_LONG and self.active_trade is not None and self.active_trade.side == OrderSide.BUY:
            exit_value = self.active_trade.amount * price
            commission_cost = exit_value * self.commission_rate

            self.current_cash += (exit_value - commission_cost)
            
            pnl_for_this_trade = (price - self.active_trade.entry_price) * self.active_trade.amount - self.active_trade.commission - commission_cost
            initial_trade_value = self.active_trade.entry_price * self.active_trade.amount
            pnl_percentage = (pnl_for_this_trade / initial_trade_value) * 100 if initial_trade_value != 0 else 0

            self.active_trade.exit_price = price
            self.active_trade.exit_timestamp = timestamp
            self.active_trade.pnl = pnl_for_this_trade
            self.active_trade.pnl_percentage = pnl_percentage
            self.active_trade.commission += commission_cost
            self.active_trade.result = TradeResult.WIN if pnl_for_this_trade > 0 else (TradeResult.LOSS if pnl_for_this_trade < 0 else TradeResult.BREAKEVEN)
            self.active_trade.exit_order_id = f"exit_{self.active_trade.id.split('_')[1]}"
            
            self.trades_log.append(self.active_trade.model_copy(deep=True)) # Log a copy
            
            logger.info(
                f"{timestamp} - EXECUTE CLOSE LONG (Signal): {self.active_trade.amount:.6f} {self.strategy.symbol} @ ${price:.2f}. "
                f"PnL: ${pnl_for_this_trade:.2f} ({pnl_percentage:.2f}%). Total Comm: ${self.active_trade.commission:.2f}. "
                f"Cash Now: ${self.current_cash:.2f}."
            )
            self.active_trade = None
            self._update_portfolio_value() # Assets back to 0
            trade_executed = True
        
        # Note: SHORT and CLOSE_SHORT signals are not handled here for simplicity
        elif signal_type in (SignalType.SHORT, SignalType.CLOSE_SHORT):
            logger.debug(f"{timestamp} - {signal_type.name} signals are not handled by this basic backtesting engine setup.")

        if trade_executed:
            self._update_portfolio_value(price if self.active_trade else None) # Recalculate equity


    def _handle_stop_take_profit(self, current_bar: BarData):
        """Checks and executes SL/TP using StopManager. Modifies self.active_trade and portfolio."""
        if self.active_trade is None or self.stop_manager is None:
            return

        # 1. Check Stop Loss
        stop_loss_trigger_price = self.stop_manager.check_stop_loss(self.active_trade, current_bar)
        if stop_loss_trigger_price is not None:
            logger.info(f"{current_bar.timestamp} - STOP LOSS triggered for trade {self.active_trade.id} at derived price {stop_loss_trigger_price:.2f}")
            # Simulate exit at the stop_loss_trigger_price (or bar.low/high depending on side and realism)
            # For simplicity, assume it's hit at the trigger_price itself.
            self._execute_exit(current_bar.timestamp, stop_loss_trigger_price, "StopLoss")
            return # Exit, don't check for TP if SL hit

        # 2. Check Take Profit (only if SL not triggered)
        if self.active_trade: # SL might have closed the trade
            take_profit_trigger_price = self.stop_manager.check_take_profit(self.active_trade, current_bar)
            if take_profit_trigger_price is not None:
                logger.info(f"{current_bar.timestamp} - TAKE PROFIT triggered for trade {self.active_trade.id} at derived price {take_profit_trigger_price:.2f}")
                self._execute_exit(current_bar.timestamp, take_profit_trigger_price, "TakeProfit")
                return

    def _execute_exit(self, timestamp: datetime, price: float, exit_reason: str):
        """ Helper to execute an exit for an active trade (SL, TP, or EOD). """
        if not self.active_trade: return

        exit_value = self.active_trade.amount * price
        commission_cost = exit_value * self.commission_rate
        self.current_cash += (exit_value - commission_cost)

        pnl_for_this_trade = (price - self.active_trade.entry_price) * self.active_trade.amount - self.active_trade.commission - commission_cost
        initial_trade_value = self.active_trade.entry_price * self.active_trade.amount
        pnl_percentage = (pnl_for_this_trade / initial_trade_value) * 100 if initial_trade_value != 0 else 0
        
        self.active_trade.exit_price = price
        self.active_trade.exit_timestamp = timestamp
        self.active_trade.pnl = pnl_for_this_trade
        self.active_trade.pnl_percentage = pnl_percentage
        self.active_trade.commission += commission_cost
        self.active_trade.result = TradeResult.WIN if pnl_for_this_trade > 0 else (TradeResult.LOSS if pnl_for_this_trade < 0 else TradeResult.BREAKEVEN)
        self.active_trade.notes = exit_reason
        self.active_trade.exit_order_id = f"{exit_reason.lower()}_{self.active_trade.id.split('_')[1]}"

        self.trades_log.append(self.active_trade.model_copy(deep=True))
        
        logger.info(
            f"{timestamp} - EXECUTE CLOSE LONG ({exit_reason}): {self.active_trade.amount:.6f} {self.strategy.symbol} @ ${price:.2f}. "
            f"PnL: ${pnl_for_this_trade:.2f} ({pnl_percentage:.2f}%). Total Comm: ${self.active_trade.commission:.2f}. "
            f"Cash Now: ${self.current_cash:.2f}."
        )
        self.active_trade = None
        self._update_portfolio_value() # Recalculate equity (assets become 0)


    def run(self) -> tuple[List[Trade], Dict[str, Any]]:
        logger.info(f"Starting backtest run for strategy '{self.strategy.name}'...")
        self.strategy.data_history = pd.DataFrame(columns=['open', 'high', 'low', 'close', 'volume', 'atr', 'sentiment_score']) # Reset history

        for timestamp, row in self.data_feed_df.iterrows():
            ts_aware = timestamp.tz_localize('UTC') if timestamp.tzinfo is None else timestamp.tz_convert('UTC')
            
            # Prepare BarData object
            bar_data_args = {
                "timestamp": ts_aware, "open": row['open'], "high": row['high'],
                "low": row['low'], "close": row['close'], "volume": row['volume'],
                "symbol": self.strategy.symbol, "timeframe": self.strategy.timeframe,
                "atr": row.get('atr') # If ATR is pre-calculated on data_feed_df
            }
            
            # Phase 4: Incorporate sentiment data
            current_sentiment_score = None
            if self.sentiment_data_df is not None and not self.sentiment_data_df.empty:
                # Try to get sentiment for the current bar's timestamp (or nearest past)
                # Using asof for exact or previous available
                try:
                    # sentiment_series = self.sentiment_data_df['sentiment_score']
                    # Ensure index is sorted for asof
                    if not self.sentiment_data_df.index.is_monotonic_increasing:
                         self.sentiment_data_df = self.sentiment_data_df.sort_index()
                    
                    sentiment_value = self.sentiment_data_df['sentiment_score'].asof(ts_aware)
                    if pd.notna(sentiment_value):
                        current_sentiment_score = sentiment_value
                        bar_data_args["sentiment_score"] = current_sentiment_score
                        # logger.debug(f"Sentiment score {current_sentiment_score} applied for bar {ts_aware}")
                except KeyError: # No 'sentiment_score' column or timestamp not found
                    # logger.debug(f"No sentiment data found for timestamp {ts_aware} using asof.")
                    pass # Keep current_sentiment_score as None
                except Exception as e_sentiment:
                    logger.warning(f"Error accessing sentiment data for {ts_aware}: {e_sentiment}")


            current_bar = BarData(**bar_data_args)
            
            # 1. Check SL/TP first based on H/L prices of current bar
            if self.active_trade:
                self._handle_stop_take_profit(current_bar)
            
            # 2. If no SL/TP triggered (or no active trade), get signal from strategy
            if self.active_trade is None or not (self.active_trade.exit_price is not None): # If trade still open or no trade
                # The strategy's on_bar_data uses its internal history which is updated within the method.
                # Pass sentiment score to strategy's on_bar_data if available
                signal = self.strategy.on_bar_data(current_bar, sentiment_score=current_sentiment_score) # Modified to pass sentiment
                
                if signal and signal != SignalType.HOLD:
                    execution_price = current_bar.close # Assume execution at close of signal bar
                    self._execute_trade(signal, current_bar.timestamp, execution_price, current_bar_for_atr_calc=current_bar)

            # 3. Log portfolio state at the end of each bar
            self._update_portfolio_value(current_bar.close) # Update with current bar's close for MTM
            self.portfolio_history.append({
                "timestamp": current_bar.timestamp,
                "cash": self.current_cash,
                "asset_value": self.current_asset_value,
                "total_value": self.current_portfolio_value, # This is equity
                "current_price": current_bar.close,
                "active_trade_pnl": (self.current_portfolio_value - self.active_trade.entry_price * self.active_trade.amount - self.active_trade.commission) if self.active_trade else 0.0
            })

        # End of backtest: close any open position
        if self.active_trade:
            last_bar_data = self.data_feed_df.iloc[-1]
            last_bar_timestamp = self.data_feed_df.index[-1].tz_localize('UTC') if self.data_feed_df.index[-1].tzinfo is None else self.data_feed_df.index[-1].tz_convert('UTC')
            last_bar_close = last_bar_data['close']
            logger.info(f"{last_bar_timestamp} - End of backtest. Closing open {self.active_trade.side.value} position for {self.strategy.symbol} at ${last_bar_close:.2f}")
            self._execute_exit(last_bar_timestamp, last_bar_close, "EndOfBacktest")
        
        self._update_portfolio_value() # Final update (asset value should be 0)
        final_portfolio_state = {
            "initial_capital": self.initial_capital,
            "final_portfolio_value": self.current_portfolio_value, # Should be mostly cash
            "final_cash": self.current_cash,
            "end_timestamp": self.data_feed_df.index[-1]
        }
        
        logger.info(f"Backtest run completed. Final Portfolio Value: ${final_portfolio_state['final_portfolio_value']:.2f}")
        return self.trades_log, final_portfolio_state
</code>

kamikaze_komodo/backtesting_engine/performance_analyzer.py:
<code>
# kamikaze_komodo/backtesting_engine/performance_analyzer.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
from kamikaze_komodo.core.models import Trade
from kamikaze_komodo.core.enums import TradeResult
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class PerformanceAnalyzer:
    """
    Calculates and analyzes performance metrics from a list of trades.
    """
    def __init__(self, trades: List[Trade], initial_capital: float, final_capital: float):
        if not trades:
            logger.warning("PerformanceAnalyzer initialized with no trades. Some metrics might be zero or NaN.")
        self.trades = pd.DataFrame([trade.model_dump() for trade in trades])
        if not self.trades.empty:
            self.trades['entry_timestamp'] = pd.to_datetime(self.trades['entry_timestamp'])
            self.trades['exit_timestamp'] = pd.to_datetime(self.trades['exit_timestamp'])
        
        self.initial_capital = initial_capital
        self.final_capital = final_capital # This should be the final total portfolio value
        logger.info(f"PerformanceAnalyzer initialized with {len(trades)} trades. Initial: ${initial_capital:,.2f}, Final: ${final_capital:,.2f}")


    def get_pnl_series(self) -> pd.Series:
        """Returns a Series of PnL for each trade."""
        if self.trades.empty or 'pnl' not in self.trades.columns:
            return pd.Series(dtype=float)
        return self.trades['pnl'].dropna()

    def get_equity_curve(self) -> pd.Series:
        """Generates an equity curve based on trade PnLs."""
        if self.trades.empty or 'pnl' not in self.trades.columns:
            equity = pd.Series([self.initial_capital], index=[pd.Timestamp(0)]) # Placeholder
            equity.name = "Equity"
            return equity
            
        # Ensure PnL is numeric and trades are sorted by exit time for accurate curve
        pnl_series = self.trades.set_index('exit_timestamp')['pnl'].dropna().astype(float)
        if pnl_series.empty:
             equity = pd.Series([self.initial_capital], index=[pd.Timestamp(0)])
             equity.name = "Equity"
             return equity

        cumulative_pnl = pnl_series.cumsum()
        equity = self.initial_capital + cumulative_pnl
        
        # Add initial capital point
        # Find the earliest entry time, or use a synthetic start time if no trades
        start_time = self.trades['entry_timestamp'].min() if not self.trades.empty else pd.Timestamp.now(tz='UTC') - pd.Timedelta(days=1)
        # Ensure start_time is before the first trade's exit_timestamp for proper plotting.
        # If pnl_series index is not empty, make sure start_time is before its first element.
        if not pnl_series.empty and start_time > pnl_series.index[0]:
            start_time = pnl_series.index[0] - pd.Timedelta(seconds=1)

        equity = pd.concat([pd.Series([self.initial_capital], index=[start_time]), equity])
        equity.name = "Equity"
        return equity


    def calculate_metrics(self) -> Dict[str, Any]:
        """
        Calculates a comprehensive suite of performance metrics.
        """
        metrics: Dict[str, Any] = {
            "initial_capital": self.initial_capital,
            "final_capital": self.final_capital,
            "total_net_profit": 0.0,
            "total_return_pct": 0.0,
            "total_trades": 0,
            "winning_trades": 0,
            "losing_trades": 0,
            "breakeven_trades": 0,
            "win_rate_pct": 0.0,
            "loss_rate_pct": 0.0,
            "average_pnl_per_trade": 0.0,
            "average_win_pnl": 0.0,
            "average_loss_pnl": 0.0,
            "profit_factor": np.nan, # Gross Profit / Gross Loss
            "max_drawdown_pct": 0.0, # Needs equity curve
            "sharpe_ratio": np.nan, # Needs daily/periodic returns & risk-free rate
            "sortino_ratio": np.nan, # Needs daily/periodic returns & risk-free rate & downside deviation
            "total_fees_paid": 0.0,
        }

        if self.trades.empty:
            metrics["total_net_profit"] = self.final_capital - self.initial_capital
            if self.initial_capital > 0:
                 metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
            logger.warning("No trades to analyze. Returning basic capital metrics.")
            return metrics

        # PnL calculations
        pnl_series = self.get_pnl_series()
        if pnl_series.empty and not self.trades.empty: # PnL might be all NaN if trades didn't close
            logger.warning("PnL series is empty or all NaN, cannot calculate detailed metrics.")
            metrics["total_net_profit"] = self.final_capital - self.initial_capital
            if self.initial_capital > 0:
                 metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
            metrics["total_trades"] = len(self.trades)
            metrics["total_fees_paid"] = self.trades['commission'].sum() if 'commission' in self.trades.columns else 0.0
            return metrics

        metrics["total_net_profit"] = pnl_series.sum()
        if self.initial_capital > 0: # Avoid division by zero
            metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
        
        metrics["total_trades"] = len(pnl_series)
        if metrics["total_trades"] == 0: # No closed trades with PnL
             metrics["total_fees_paid"] = self.trades['commission'].sum() if 'commission' in self.trades.columns else 0.0
             return metrics


        # Win/Loss Analysis
        wins = pnl_series[pnl_series > 0]
        losses = pnl_series[pnl_series < 0]
        breakevens = pnl_series[pnl_series == 0]

        metrics["winning_trades"] = len(wins)
        metrics["losing_trades"] = len(losses)
        metrics["breakeven_trades"] = len(breakevens)

        if metrics["total_trades"] > 0:
            metrics["win_rate_pct"] = (metrics["winning_trades"] / metrics["total_trades"]) * 100
            metrics["loss_rate_pct"] = (metrics["losing_trades"] / metrics["total_trades"]) * 100
            metrics["average_pnl_per_trade"] = pnl_series.mean()

        if not wins.empty:
            metrics["average_win_pnl"] = wins.mean()
        if not losses.empty:
            metrics["average_loss_pnl"] = losses.mean() # This will be negative

        # Profit Factor
        gross_profit = wins.sum()
        gross_loss = abs(losses.sum()) # Absolute sum of losses
        if gross_loss > 0:
            metrics["profit_factor"] = gross_profit / gross_loss
        elif gross_profit > 0 and gross_loss == 0 : # All wins, no losses
            metrics["profit_factor"] = np.inf


        # Max Drawdown (from equity curve)
        equity_curve = self.get_equity_curve()
        if not equity_curve.empty and len(equity_curve) > 1:
            peak = equity_curve.expanding(min_periods=1).max()
            drawdown = (equity_curve - peak) / peak
            metrics["max_drawdown_pct"] = abs(drawdown.min()) * 100
        
        # Sharpe and Sortino Ratios (Simplified: assumes daily returns if data allows)
        # These require more complex calculations of periodic returns and risk-free rate.
        # For a basic version, we can skip or use a simplified placeholder if PnL series represents portfolio value changes.
        # daily_returns = equity_curve.pct_change().dropna() # if equity curve represents daily values
        # if not daily_returns.empty and len(daily_returns) > 1:
        #     # Assuming risk-free rate of 0 for simplicity
        #     sharpe_avg_return = daily_returns.mean()
        #     sharpe_std_return = daily_returns.std()
        #     if sharpe_std_return != 0:
        #         metrics["sharpe_ratio"] = (sharpe_avg_return / sharpe_std_return) * np.sqrt(252) # Annualized (approx for crypto: 365)
            
        #     downside_returns = daily_returns[daily_returns < 0]
        #     if not downside_returns.empty:
        #         downside_std = downside_returns.std()
        #         if downside_std != 0:
        #             metrics["sortino_ratio"] = (sharpe_avg_return / downside_std) * np.sqrt(252) # Annualized

        metrics["total_fees_paid"] = self.trades['commission'].sum() if 'commission' in self.trades.columns else 0.0

        return metrics

    def print_summary(self, metrics: Optional[Dict[str, Any]] = None):
        """Prints a summary of the performance metrics."""
        if metrics is None:
            metrics = self.calculate_metrics()

        summary = f"""
        --------------------------------------------------
        |              Backtest Performance Summary      |
        --------------------------------------------------
        | Metric                      | Value            |
        --------------------------------------------------
        | Initial Capital             | ${metrics.get("initial_capital", 0):<15,.2f} |
        | Final Capital               | ${metrics.get("final_capital", 0):<15,.2f} |
        | Total Net Profit            | ${metrics.get("total_net_profit", 0):<15,.2f} |
        | Total Return                | {metrics.get("total_return_pct", 0):<15.2f}% |
        | Total Trades                | {metrics.get("total_trades", 0):<16} |
        | Winning Trades              | {metrics.get("winning_trades", 0):<16} |
        | Losing Trades               | {metrics.get("losing_trades", 0):<16} |
        | Breakeven Trades            | {metrics.get("breakeven_trades", 0):<16} |
        | Win Rate                    | {metrics.get("win_rate_pct", 0):<15.2f}% |
        | Loss Rate                   | {metrics.get("loss_rate_pct", 0):<15.2f}% |
        | Avg PnL per Trade           | ${metrics.get("average_pnl_per_trade", 0):<15,.2f} |
        | Avg Win PnL                 | ${metrics.get("average_win_pnl", 0):<15,.2f} |
        | Avg Loss PnL                | ${metrics.get("average_loss_pnl", 0):<15,.2f} |
        | Profit Factor               | {metrics.get("profit_factor", float('nan')):<16.2f} |
        | Max Drawdown                | {metrics.get("max_drawdown_pct", 0):<15.2f}% |
        | Total Fees Paid             | ${metrics.get("total_fees_paid", 0):<15,.2f} |
        | Sharpe Ratio (approx)       | {metrics.get("sharpe_ratio", float('nan')):<16.2f} |
        | Sortino Ratio (approx)      | {metrics.get("sortino_ratio", float('nan')):<16.2f} |
        --------------------------------------------------
        """
        print(summary)
        logger.info("Performance summary generated." + summary.replace("\n        |", "\n")) # Loggable format


# Example Usage:
if __name__ == '__main__':
    # Create some dummy trade data
    dummy_trades_data = [
        Trade(id="t1", symbol="BTC/USD", entry_order_id="e1", exit_order_id="ex1", side=OrderSide.BUY,
              entry_price=30000, exit_price=31000, amount=1, entry_timestamp=datetime(2023,1,1,10), exit_timestamp=datetime(2023,1,1,12),
              pnl=980, pnl_percentage=(1000/30000 - 0.002)*100, commission=20, result=TradeResult.WIN),
        Trade(id="t2", symbol="BTC/USD", entry_order_id="e2", exit_order_id="ex2", side=OrderSide.BUY,
              entry_price=31500, exit_price=31000, amount=1, entry_timestamp=datetime(2023,1,2,10), exit_timestamp=datetime(2023,1,2,15),
              pnl=-520, pnl_percentage=(-500/31500 - 0.002)*100, commission=20, result=TradeResult.LOSS),
        Trade(id="t3", symbol="BTC/USD", entry_order_id="e3", exit_order_id="ex3", side=OrderSide.BUY,
              entry_price=32000, exit_price=33000, amount=0.5, entry_timestamp=datetime(2023,1,3,10), exit_timestamp=datetime(2023,1,3,18),
              pnl=485, pnl_percentage=(1000/32000 * 0.5 - 0.002)*100, commission=15, result=TradeResult.WIN), # PnL adjusted for 0.5 amount logic
    ]
    # Correcting PnL for T3: (33000-32000)*0.5 = 500.  Net PnL = 500 - 15 (commission) = 485
    # Pct: ((33000/32000)-1)*100 = 3.125%. After commission approx.

    initial_cap = 100000
    final_cap = initial_cap + (980 - 520 + 485) # 100000 + 945 = 100945

    analyzer = PerformanceAnalyzer(trades=dummy_trades_data, initial_capital=initial_cap, final_capital=final_cap)
    metrics_calculated = analyzer.calculate_metrics()
    analyzer.print_summary(metrics_calculated)

    # equity = analyzer.get_equity_curve()
    # print("\nEquity Curve:")
    # print(equity)
</code>

kamikaze_komodo/backtesting_engine/__init__.py:
<code>
# kamikaze_komodo/backtesting_engine/__init__.py
# This file makes the 'backtesting_engine' directory a Python package.
</code>

kamikaze_komodo/config/config.ini:
<code>
# kamikaze_komodo/config/config.ini
# Added Phase 3 and Phase 4 settings, Vertex AI settings, and RSS feeds
[General]
LogLevel = INFO
LogFilePath = logs/kamikaze_komodo.log

[API]
ExchangeID = krakenfutures
KrakenTestnet = True

[DataFetching]
DefaultSymbol = PF_XBTUSD
DefaultTimeframe = 4h
HistoricalDataDays = 365
DataFetchLimitPerCall = 500
; For paginated historical data

[Trading]
MaxPortfolioRisk = 0.02
DefaultLeverage = 1.0
CommissionBPS = 10
; Basis points, e.g. 10 bps = 0.1% = 0.001 rate

[EWMAC_Strategy]
ShortWindow = 12
LongWindow = 26
SignalWindow = 9
; ATR period for strategy's internal ATR calculation (if any)
atr_period = 14

; --- Phase 3 Settings ---
[RiskManagement]
PositionSizer = ATRBased
; Options: FixedFractional, ATRBased
FixedFractional_AllocationFraction = 0.10
; Fraction of *equity* to allocate to a new position
ATRBased_RiskPerTradeFraction = 0.01
; Fraction of *equity* to risk per trade (requires ATR from strategy/bar)
ATRBased_ATRMultipleForStop = 2.0
; How many ATRs away the stop is placed for sizing calculation

StopManager_Default = ATRBased
; Options: PercentageBased, ATRBased (ATRBased needs ATR on trade/bar)
PercentageStop_LossPct = 0.02
; 2% stop loss from entry. Set to 0 or None to disable.
PercentageStop_TakeProfitPct = 0.05
; 5% take profit from entry. Set to 0 or None to disable.
ATRStop_ATRMultiple = 2.0
; For ATRStopManager if used. Needs ATR at trade entry.

[PortfolioConstructor]
AssetAllocator = FixedWeight
; For now, mainly conceptual for single asset backtest
DefaultAllocation_BTCUSD = 1.0
; Example: PF_XBTUSD if using Kraken Futures symbol
Rebalancer_DeviationThreshold = 0.05
; For BasicRebalancer (conceptual for now)

; --- Phase 4 Settings ---
[AI_NewsAnalysis]
EnableSentimentAnalysis = True
SentimentLLMProvider = VertexAI
; Options: VertexAI, Ollama (Ollama is now legacy for this project)
SentimentFilter_Threshold_Long = 0.1
; Example: Only take LONG if sentiment > 0.1
SentimentFilter_Threshold_Short = -0.1
; Example: Only take SHORT if sentiment < -0.1 (if shorting implemented)
SimulatedSentimentDataPath = data/simulated_sentiment_data.csv
; For backtesting
NewsScraper_Enable = True
NotificationListener_Enable = False
; Jeepney D-Bus listener is mostly placeholder
BrowserAgent_Enable = False
; browser-use agent is resource intensive, enable for specific tasks
BrowserAgent_LLMProvider = VertexAI
; Can be same or different from sentiment LLM provider

[VertexAI]
ProjectID = kamikazekomodo
Location = us-central1
SentimentModelName = gemini-2.5-flash-preview-05-20
BrowserAgentModelName = gemini-2.5-flash-preview-05-20
; You might use a more powerful model for browser agent if needed, e.g., gemini-1.5-pro-preview-0514

; RSS Feeds for NewsScraper (add as many as needed, use unique keys)
RSSFeed_Coindesk = https://www.coindesk.com/arc/outboundfeeds/rss/?outputType=xml
RSSFeed_Cointelegraph = https://cointelegraph.com/rss
RSSFeed_Decrypt = https://decrypt.co/feed/
RSSFeed_BitcoinComNews = https://news.bitcoin.com/feed/
RSSFeed_Bitcoinist = https://bitcoinist.com/feed/
RSSFeed_UToday = https://u.today/feed/
RSSFeed_CCNNews = https://www.ccn.com/news/crypto-news/feeds/
RSSFeed_CryptoPotato = https://cryptopotato.com/feed/
RSSFeed_CryptoSlate = https://cryptoslate.com/feed/
RSSFeed_TheDefiant = https://thedefiant.io/feed/
RSSFeed_ConsensysNews = https://consensys.io/category/news/feed/
; Add other relevant feeds:
; TheBlock: (User needs to find URL)
; BeInCrypto: (User needs to find URL)
; CCN Analysis: https://www.ccn.com/analysis/crypto-analysis/feeds/
; Consensys All: https://consensys.io/feed/


; --- LEGACY Ollama Settings (if ever needed for fallback, but primary is VertexAI) ---
; [Ollama_Legacy]
; BaseURL = http://localhost:11434
; SentimentLLMModel = gemma3:12b
; BrowserAgent_LLMModel = gemma3:12b
</code>

kamikaze_komodo/config/secrets.ini:
<code>
; kamikaze_komodo/config/secrets.ini
; This file should be in .gitignore and contain sensitive information.
[KRAKEN_API]
API_KEY = 'd27PYGi95tlsV4gVotVNXinHOTAxXY2usUta7kw3IogO9/9kpLHCHgcv'
SECRET_KEY = 'kB+i8be+l7J6Lr+RyjodrqNyQXrIn6reFeNfDsmMs01zsQg3KPGSSshd9l4KwvY92LQyYamDc1lMrHsnZ6+LaWQP'

[DATABASE]
User = db_user
Password = db_password
</code>

kamikaze_komodo/config/settings.py:
<code>
# kamikaze_komodo/config/settings.py
import configparser
import os
from kamikaze_komodo.app_logger import get_logger
from typing import Dict, List, Optional, Any

logger = get_logger(__name__)

class Config:
    """
    Manages application configuration using config.ini and secrets.ini.
    """
    def __init__(self, config_file_rel_path='config/config.ini', secrets_file_rel_path='config/secrets.ini'):
        self.config = configparser.ConfigParser()
        self.secrets = configparser.ConfigParser()

        script_dir = os.path.dirname(os.path.abspath(__file__)) # .../kamikaze_komodo/config
        project_module_dir = os.path.dirname(script_dir) # .../kamikaze_komodo

        self.config_file_path = os.path.join(project_module_dir, config_file_rel_path)
        self.secrets_file_path = os.path.join(project_module_dir, secrets_file_rel_path)

        if not os.path.exists(self.config_file_path):
            logger.error(f"Config file not found: {self.config_file_path}")
            raise FileNotFoundError(f"Config file not found: {self.config_file_path}")
        if not os.path.exists(self.secrets_file_path):
            logger.warning(f"Secrets file not found: {self.secrets_file_path}. Some features might be unavailable.")

        self.config.read(self.config_file_path)
        self.secrets.read(self.secrets_file_path)

        # General Settings
        self.log_level: str = self.config.get('General', 'LogLevel', fallback='INFO')
        self.log_file_path: str = self.config.get('General', 'LogFilePath', fallback='logs/kamikaze_komodo.log')

        # API Settings
        self.exchange_id_to_use: str = self.config.get('API', 'ExchangeID', fallback='krakenfutures') # Changed default for clarity with PF_XBTUSD
        self.kraken_api_key: Optional[str] = self.secrets.get('KRAKEN_API', 'API_KEY', fallback=None)
        self.kraken_secret_key: Optional[str] = self.secrets.get('KRAKEN_API', 'SECRET_KEY', fallback=None)
        self.kraken_testnet: bool = self.config.getboolean('API', 'KrakenTestnet', fallback=True)

        # Data Fetching Settings
        self.default_symbol: str = self.config.get('DataFetching', 'DefaultSymbol', fallback='PF_XBTUSD')
        self.default_timeframe: str = self.config.get('DataFetching', 'DefaultTimeframe', fallback='4h') # Changed from 1h to match config
        self.historical_data_days: int = self.config.getint('DataFetching', 'HistoricalDataDays', fallback=365)
        self.data_fetch_limit_per_call: int = self.config.getint('DataFetching', 'DataFetchLimitPerCall', fallback=500)

        # Trading Settings
        self.max_portfolio_risk: float = self.config.getfloat('Trading', 'MaxPortfolioRisk', fallback=0.02)
        self.default_leverage: float = self.config.getfloat('Trading', 'DefaultLeverage', fallback=1.0)
        self.commission_bps: float = self.config.getfloat('Trading', 'CommissionBPS', fallback=10.0)

        # EWMAC Strategy Settings
        self.ewmac_short_window: int = self.config.getint('EWMAC_Strategy', 'ShortWindow', fallback=12)
        self.ewmac_long_window: int = self.config.getint('EWMAC_Strategy', 'LongWindow', fallback=26)
        self.ewmac_signal_window: int = self.config.getint('EWMAC_Strategy', 'SignalWindow', fallback=9)
        self.ewmac_atr_period: int = self.config.getint('EWMAC_Strategy', 'atr_period', fallback=14)


        # --- Phase 3: Risk Management Settings ---
        self.position_sizer_type: str = self.config.get('RiskManagement', 'PositionSizer', fallback='FixedFractional')
        self.fixed_fractional_allocation_fraction: float = self.config.getfloat('RiskManagement', 'FixedFractional_AllocationFraction', fallback=0.10)
        self.atr_based_risk_per_trade_fraction: float = self.config.getfloat('RiskManagement', 'ATRBased_RiskPerTradeFraction', fallback=0.01)
        self.atr_based_atr_multiple_for_stop: float = self.config.getfloat('RiskManagement', 'ATRBased_ATRMultipleForStop', fallback=2.0)

        self.stop_manager_type: str = self.config.get('RiskManagement', 'StopManager_Default', fallback='PercentageBased')
        _sl_pct_str = self.config.get('RiskManagement', 'PercentageStop_LossPct', fallback='0.02')
        self.percentage_stop_loss_pct: Optional[float] = float(_sl_pct_str) if _sl_pct_str and _sl_pct_str.lower() not in ['none', '0', '0.0'] else None
        _tp_pct_str = self.config.get('RiskManagement', 'PercentageStop_TakeProfitPct', fallback='0.05')
        self.percentage_stop_take_profit_pct: Optional[float] = float(_tp_pct_str) if _tp_pct_str and _tp_pct_str.lower() not in ['none', '0', '0.0'] else None
        self.atr_stop_atr_multiple: float = self.config.getfloat('RiskManagement', 'ATRStop_ATRMultiple', fallback=2.0)

        # --- Phase 3: Portfolio Constructor Settings ---
        self.asset_allocator_type: str = self.config.get('PortfolioConstructor', 'AssetAllocator', fallback='FixedWeight')
        # Attempt to dynamically get the default allocation for the default symbol
        default_symbol_config_key = f'DefaultAllocation_{self.default_symbol.replace("/", "").replace(":", "")}'
        self.default_allocation_for_symbol: float = self.config.getfloat('PortfolioConstructor', default_symbol_config_key, fallback=1.0)
        self.rebalancer_deviation_threshold: float = self.config.getfloat('PortfolioConstructor', 'Rebalancer_DeviationThreshold', fallback=0.05)

        # --- Phase 4: AI News Analysis Settings ---
        self.enable_sentiment_analysis: bool = self.config.getboolean('AI_NewsAnalysis', 'EnableSentimentAnalysis', fallback=True)
        self.sentiment_llm_provider: str = self.config.get('AI_NewsAnalysis', 'SentimentLLMProvider', fallback='VertexAI')
        self.browser_agent_llm_provider: str = self.config.get('AI_NewsAnalysis', 'BrowserAgent_LLMProvider', fallback='VertexAI')

        self.sentiment_filter_threshold_long: float = self.config.getfloat('AI_NewsAnalysis', 'SentimentFilter_Threshold_Long', fallback=0.1)
        self.sentiment_filter_threshold_short: float = self.config.getfloat('AI_NewsAnalysis', 'SentimentFilter_Threshold_Short', fallback=-0.1)
        self.simulated_sentiment_data_path: Optional[str] = self.config.get('AI_NewsAnalysis', 'SimulatedSentimentDataPath', fallback=None)
        if self.simulated_sentiment_data_path and self.simulated_sentiment_data_path.lower() in ['none', '']: self.simulated_sentiment_data_path = None
        if self.simulated_sentiment_data_path and not os.path.isabs(self.simulated_sentiment_data_path):
            self.simulated_sentiment_data_path = os.path.join(project_module_dir, self.simulated_sentiment_data_path)

        self.news_scraper_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'NewsScraper_Enable', fallback=True)
        self.notification_listener_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'NotificationListener_Enable', fallback=False)
        self.browser_agent_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'BrowserAgent_Enable', fallback=False)
        
        # VertexAI Settings
        self.vertex_ai_project_id: Optional[str] = self.config.get('VertexAI', 'ProjectID', fallback=None)
        self.vertex_ai_location: Optional[str] = self.config.get('VertexAI', 'Location', fallback=None)
        self.vertex_ai_sentiment_model_name: str = self.config.get('VertexAI', 'SentimentModelName', fallback='gemini-2.5-flash-preview-05-20')
        self.vertex_ai_browser_agent_model_name: str = self.config.get('VertexAI', 'BrowserAgentModelName', fallback='gemini-2.5-flash-preview-05-20')

        if self.vertex_ai_project_id and self.vertex_ai_project_id.lower() == 'your-gcp-project-id':
            logger.warning("Vertex AI ProjectID is set to 'your-gcp-project-id'. Please update it in config.ini.")
            self.vertex_ai_project_id = None


        # Legacy Ollama Settings (can be removed if not needed for fallback)
        # self.ollama_base_url: Optional[str] = self.config.get('Ollama_Legacy', 'BaseURL', fallback="http://localhost:11434")
        # if self.ollama_base_url and self.ollama_base_url.lower() in ['none', '']: self.ollama_base_url = None
        # self.ollama_sentiment_llm_model: str = self.config.get('Ollama_Legacy', 'SentimentLLMModel', fallback='gemma3:12b')
        # self.ollama_browser_agent_llm_model: str = self.config.get('Ollama_Legacy', 'BrowserAgent_LLMModel', fallback='gemma3:12b')


        # RSS Feeds
        self.rss_feeds: List[Dict[str, str]] = []
        if self.config.has_section('AI_NewsAnalysis'):
            for key, value in self.config.items('AI_NewsAnalysis'):
                if key.startswith("RSSFeed_"):
                    feed_name = key.replace("rssfeed_", "").replace("_", " ").title()
                    self.rss_feeds.append({"name": feed_name, "url": value})
        if not self.rss_feeds:
            logger.warning("No RSS feeds configured in config.ini under [AI_NewsAnalysis] with 'RSSFeed_' prefix.")


    def get_strategy_params(self, strategy_name: str) -> dict:
        params = {}
        section_name = f"{strategy_name}_Strategy"
        if self.config.has_section(section_name):
            params = dict(self.config.items(section_name))
            for key, value in params.items():
                # Convert to int, float, bool, None where appropriate
                if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                    params[key] = int(value)
                else:
                    try:
                        params[key] = float(value)
                    except ValueError:
                        if value.lower() == 'true': params[key] = True
                        elif value.lower() == 'false': params[key] = False
                        elif value.lower() == 'none': params[key] = None
        else:
            logger.warning(f"No specific configuration section found for strategy: {section_name}. Using defaults or globally passed params.")
        
        # Add relevant global AI settings to strategy params if not already present
        # This allows strategies to easily access sentiment thresholds.
        if 'sentiment_filter_long_threshold' not in params:
            params['sentiment_filter_long_threshold'] = self.sentiment_filter_threshold_long
        if 'sentiment_filter_short_threshold' not in params:
            params['sentiment_filter_short_threshold'] = self.sentiment_filter_threshold_short
        if 'atr_period' not in params and strategy_name == "EWMAC": # Specifically for EWMAC
             params['atr_period'] = self.ewmac_atr_period


        return params

    def get_news_scraper_config(self) -> Dict[str, Any]:
        cfg = {"rss_feeds": self.rss_feeds, "websites": []} # Websites can be added similarly if needed
        # Example for websites:
        # if self.config.has_section('AI_NewsAnalysis'):
        # for key, value in self.config.items('AI_NewsAnalysis'):
        # if key.startswith("websitescrape_") and key.endswith("_url"):
        # site_id = key.replace("websitescrape_", "").replace("_url", "")
        # site_name = self.config.get('AI_NewsAnalysis', f'WebsiteScrape_{site_id}_Name', fallback=site_id.capitalize())
        # cfg["websites"].append({"name": site_name, "url": value})
        return cfg

try:
    settings = Config()
except FileNotFoundError as e:
    logger.critical(f"Could not initialize settings due to missing configuration file: {e}")
    settings = None # type: ignore
except Exception as e_global:
    logger.critical(f"Failed to initialize Config object: {e_global}", exc_info=True)
    settings = None # type: ignore

if settings and (not settings.kraken_api_key or "YOUR_API_KEY" in str(settings.kraken_api_key).upper() or "D27PYGI95TLS" in str(settings.kraken_api_key).upper()):
    logger.warning(f"API Key for '{settings.exchange_id_to_use}' appears to be a placeholder or is not configured in secrets.ini. Authenticated interaction will be limited/simulated.")
</code>

kamikaze_komodo/config/__init__.py:
<code>
# kamikaze_komodo/config/__init__.py
# This file makes the 'config' directory a Python package.
</code>

kamikaze_komodo/core/enums.py:
<code>
# kamikaze_komodo/core/enums.py
from enum import Enum

class OrderType(Enum):
    """
    Represents the type of an order.
    """
    MARKET = "market"
    LIMIT = "limit"
    STOP = "stop"
    STOP_LIMIT = "stop_limit"
    TAKE_PROFIT = "take_profit"
    TAKE_PROFIT_LIMIT = "take_profit_limit"

class OrderSide(Enum):
    """
    Represents the side of an order.
    """
    BUY = "buy"
    SELL = "sell"

class SignalType(Enum):
    """
    Represents the type of trading signal generated by a strategy.
    """
    LONG = "LONG"
    SHORT = "SHORT"
    HOLD = "HOLD"
    CLOSE_LONG = "CLOSE_LONG"
    CLOSE_SHORT = "CLOSE_SHORT"

class CandleInterval(Enum):
    """
    Represents common candle intervals for market data.
    Follows CCXT conventions where possible.
    """
    ONE_MINUTE = "1m"
    THREE_MINUTES = "3m"
    FIVE_MINUTES = "5m"
    FIFTEEN_MINUTES = "15m"
    THIRTY_MINUTES = "30m"
    ONE_HOUR = "1h"
    TWO_HOURS = "2h"
    FOUR_HOURS = "4h"
    SIX_HOURS = "6h"
    EIGHT_HOURS = "8h"
    TWELVE_HOURS = "12h"
    ONE_DAY = "1d"
    THREE_DAYS = "3d"
    ONE_WEEK = "1w"
    ONE_MONTH = "1M"

class TradeResult(Enum):
    """
    Represents the outcome of a trade.
    """
    WIN = "WIN"
    LOSS = "LOSS"
    BREAKEVEN = "BREAKEVEN"
</code>

kamikaze_komodo/core/models.py:
<code>
# kamikaze_komodo/core/models.py
# Added fields to NewsArticle and Trade as potentially needed by new modules.
from typing import Optional, List, Dict, Any # Added Any
from pydantic import BaseModel, Field
from datetime import datetime, timezone

import pydantic # Added timezone
from kamikaze_komodo.core.enums import OrderType, OrderSide, SignalType, TradeResult

class BarData(BaseModel):
    """
    Represents OHLCV market data for a specific time interval.
    """
    timestamp: datetime = Field(..., description="The start time of the candle, expected to be timezone-aware (UTC)")
    open: float = Field(..., gt=0, description="Opening price")
    high: float = Field(..., gt=0, description="Highest price")
    low: float = Field(..., gt=0, description="Lowest price")
    close: float = Field(..., gt=0, description="Closing price")
    volume: float = Field(..., ge=0, description="Trading volume")
    symbol: Optional[str] = Field(None, description="Trading symbol, e.g., BTC/USD")
    timeframe: Optional[str] = Field(None, description="Candle timeframe, e.g., 1h")
    # Optional fields for indicators or sentiment
    atr: Optional[float] = Field(None, description="Average True Range at this bar")
    sentiment_score: Optional[float] = Field(None, description="Sentiment score associated with this bar's timestamp")


    class Config:
        frozen = False # Changed to False to allow adding attributes like ATR dynamically if needed
                       # Or use `model_copy(update={...})` for immutable updates.
                       # For backtesting.py compatibility, mutable might be easier.
                       # Let's keep it True and see if backtesting.py / strategies handle it.
                       # If strategies create copies and add indicators, it's fine.
                       # For `BarData.sentiment_score` to be added by engine, it needs to be mutable or recreated.
                       # For now, making it mutable for easier integration.
        # frozen = True 

class Order(BaseModel):
    # ... (no changes from existing)
    id: str = Field(..., description="Unique order identifier (from exchange or internal)")
    symbol: str = Field(..., description="Trading symbol, e.g., BTC/USD")
    type: OrderType = Field(..., description="Type of order (market, limit, etc.)")
    side: OrderSide = Field(..., description="Order side (buy or sell)")
    amount: float = Field(..., gt=0, description="Quantity of the asset to trade")
    price: Optional[float] = Field(None, gt=0, description="Price for limit or stop orders")
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description="Time the order was created")
    status: str = Field("open", description="Current status of the order (e.g., open, filled, canceled)")
    filled_amount: float = Field(0.0, ge=0, description="Amount of the order that has been filled")
    average_fill_price: Optional[float] = Field(None, description="Average price at which the order was filled")
    exchange_id: Optional[str] = Field(None, description="Order ID from the exchange")


class Trade(BaseModel):
    """
    Represents an executed trade.
    """
    id: str = Field(..., description="Unique trade identifier")
    symbol: str = Field(..., description="Trading symbol, e.g., BTC/USD")
    entry_order_id: str = Field(..., description="ID of the order that opened the trade")
    exit_order_id: Optional[str] = Field(None, description="ID of the order that closed the trade")
    side: OrderSide = Field(..., description="Trade side (buy/long or sell/short)")
    entry_price: float = Field(..., gt=0, description="Price at which the trade was entered")
    exit_price: Optional[float] = Field(None, description="Price at which the trade was exited (must be >0 if set)")
    amount: float = Field(..., gt=0, description="Quantity of the asset traded")
    entry_timestamp: datetime = Field(..., description="Time the trade was entered")
    exit_timestamp: Optional[datetime] = Field(None, description="Time the trade was exited")
    pnl: Optional[float] = Field(None, description="Profit or Loss for the trade")
    pnl_percentage: Optional[float] = Field(None, description="Profit or Loss percentage for the trade")
    commission: float = Field(0.0, ge=0, description="Trading commission paid")
    result: Optional[TradeResult] = Field(None, description="Outcome of the trade (Win/Loss/Breakeven)")
    notes: Optional[str] = Field(None, description="Any notes related to the trade")
    # Added for ATR based stops or other context
    custom_fields: Dict[str, Any] = Field(default_factory=dict, description="Custom fields for additional trade data, e.g., atr_at_entry")

    @pydantic.field_validator('exit_price')
    def exit_price_must_be_positive(cls, v):
        if v is not None and v <= 0:
            raise ValueError('exit_price must be positive if set')
        return v

class NewsArticle(BaseModel):
    """
    Represents a news article relevant to market analysis.
    """
    id: str = Field(..., description="Unique identifier for the news article (e.g., URL hash or URL itself)")
    url: str = Field(..., description="Source URL of the article")
    title: str = Field(..., description="Headline or title of the article")
    publication_date: Optional[datetime] = Field(None, description="Date the article was published (UTC)")
    retrieval_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description="Date the article was retrieved (UTC)")
    source: str = Field(..., description="Source of the news (e.g., CoinDesk, CoinTelegraph, RSS feed name)")
    content: Optional[str] = Field(None, description="Full text content of the article")
    summary: Optional[str] = Field(None, description="AI-generated or scraped summary")
    
    # Sentiment related fields
    sentiment_score: Optional[float] = Field(None, description="Overall sentiment score (-1.0 to 1.0)")
    sentiment_label: Optional[str] = Field(None, description="Sentiment label (e.g., positive, negative, neutral, bullish, bearish)")
    sentiment_confidence: Optional[float] = Field(None, description="Confidence of the sentiment analysis (0.0 to 1.0)")
    key_themes: Optional[List[str]] = Field(default_factory=list, description="Key themes identified by sentiment analysis")
    
    related_symbols: Optional[List[str]] = Field(default_factory=list, description="Cryptocurrencies mentioned or related")
    raw_llm_response: Optional[Dict[str, Any]] = Field(None, description="Raw response from LLM for sentiment if available")


class PortfolioSnapshot(BaseModel):
    # ... (no changes from existing)
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    total_value_usd: float = Field(..., description="Total portfolio value in USD")
    cash_balance_usd: float = Field(..., description="Available cash in USD")
    positions: Dict[str, float] = Field(default_factory=dict, description="Asset quantities, e.g., {'BTC': 0.5, 'ETH': 10}") # symbol: quantity
    open_pnl_usd: float = Field(0.0, description="Total open Profit/Loss in USD for current positions")
</code>

kamikaze_komodo/core/utils.py:
<code>
# kamikaze_komodo/core/utils.py
from datetime import datetime, timezone

from kamikaze_komodo.core.models import BarData

def format_timestamp(ts: datetime, fmt: str = "%Y-%m-%d %H:%M:%S %Z") -> str:
    """
    Formats a datetime object into a string.
    Ensures timezone awareness, defaulting to UTC if naive.
    """
    if ts.tzinfo is None:
        ts = ts.replace(tzinfo=timezone.utc)
    return ts.strftime(fmt)

def current_timestamp_ms() -> int:
    """
    Returns the current UTC timestamp in milliseconds.
    """
    return int(datetime.now(timezone.utc).timestamp() * 1000)

def ohlcv_to_bardata(ohlcv: list, symbol: str, timeframe: str) -> BarData:
    """
    Converts a CCXT OHLCV list [timestamp_ms, open, high, low, close, volume]
    to a BarData object.
    """
    from kamikaze_komodo.core.models import BarData # Local import to avoid circular dependency
    
    if len(ohlcv) != 6:
        raise ValueError("OHLCV list must contain 6 elements: timestamp, open, high, low, close, volume")

    dt_object = datetime.fromtimestamp(ohlcv[0] / 1000, tz=timezone.utc)
    return BarData(
        timestamp=dt_object,
        open=float(ohlcv[1]),
        high=float(ohlcv[2]),
        low=float(ohlcv[3]),
        close=float(ohlcv[4]),
        volume=float(ohlcv[5]),
        symbol=symbol,
        timeframe=timeframe
    )

# Add other utility functions as needed, e.g.,
# - Mathematical helpers not in TA-Lib
# - Data validation functions
# - etc.
</code>

kamikaze_komodo/core/__init__.py:
<code>
# kamikaze_komodo/core/__init__.py
# This file makes the 'core' directory a Python package.
</code>

kamikaze_komodo/data_handling/database_manager.py:
<code>
# kamikaze_komodo/data_handling/database_manager.py
# Updated to include store/retrieve for NewsArticle

import sqlite3
from typing import List, Optional, Dict, Any # Added Dict, Any
from kamikaze_komodo.core.models import BarData, NewsArticle # Added NewsArticle
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone, UTC 
import json # For storing dicts/lists like related_symbols or key_themes

logger = get_logger(__name__)

class DatabaseManager:
    """
    Manages local storage of data (initially SQLite).
    Timestamps are stored as ISO 8601 TEXT.
    Lists/Dicts are stored as JSON TEXT.
    """
    def __init__(self, db_name: str = "kamikaze_komodo_data.db"):
        self.db_name = db_name
        self.conn: Optional[sqlite3.Connection] = None
        self._connect()
        self._create_tables()

    def _connect(self):
        try:
            self.conn = sqlite3.connect(self.db_name, detect_types=sqlite3.PARSE_COLNAMES)
            self.conn.row_factory = sqlite3.Row 
            logger.info(f"Successfully connected to database: {self.db_name}")
        except sqlite3.Error as e:
            logger.error(f"Error connecting to database {self.db_name}: {e}")
            self.conn = None

    def _create_tables(self):
        if not self.conn:
            logger.error("Cannot create tables, no database connection.")
            return

        try:
            cursor = self.conn.cursor()
            # BarData Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS bar_data (
                    timestamp TEXT NOT NULL, 
                    symbol TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    open REAL NOT NULL,
                    high REAL NOT NULL,
                    low REAL NOT NULL,
                    close REAL NOT NULL,
                    volume REAL NOT NULL,
                    atr REAL, 
                    sentiment_score REAL,
                    PRIMARY KEY (timestamp, symbol, timeframe)
                )
            """)
            # NewsArticle Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS news_articles (
                    id TEXT PRIMARY KEY,
                    url TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    publication_date TEXT, 
                    retrieval_date TEXT NOT NULL, 
                    source TEXT NOT NULL,
                    content TEXT,
                    summary TEXT,
                    sentiment_score REAL,
                    sentiment_label TEXT,
                    sentiment_confidence REAL,
                    key_themes TEXT,
                    related_symbols TEXT,
                    raw_llm_response TEXT 
                )
            """)
            # Trades Table (Example for future use)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS trades (
                    id TEXT PRIMARY KEY, symbol TEXT NOT NULL, entry_order_id TEXT, exit_order_id TEXT,
                    side TEXT NOT NULL, entry_price REAL NOT NULL, exit_price REAL, amount REAL NOT NULL,
                    entry_timestamp TEXT NOT NULL, exit_timestamp TEXT, pnl REAL, pnl_percentage REAL,
                    commission REAL DEFAULT 0.0, result TEXT, notes TEXT, custom_fields TEXT
                )
            """)
            self.conn.commit()
            logger.info("Tables checked/created successfully (timestamps as TEXT, complex fields as JSON TEXT).")
        except sqlite3.Error as e:
            logger.error(f"Error creating tables: {e}")

    def _to_iso_format(self, dt: Optional[datetime]) -> Optional[str]:
        if dt is None: return None
        if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:
            dt = dt.replace(tzinfo=UTC)
        else:
            dt = dt.astimezone(UTC)
        return dt.isoformat()

    def _from_iso_format(self, iso_str: Optional[str]) -> Optional[datetime]:
        if iso_str is None: return None
        try:
            dt = datetime.fromisoformat(iso_str)
            if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:
                return dt.replace(tzinfo=UTC)
            return dt.astimezone(UTC)
        except ValueError:
            logger.warning(f"Could not parse ISO timestamp string: {iso_str}")
            return None

    def store_bar_data(self, bar_data_list: List[BarData]):
        if not self.conn: logger.error("No DB connection for bar data."); return False
        if not bar_data_list: logger.info("No bar data to store."); return True
        try:
            cursor = self.conn.cursor()
            data_to_insert = [
                (
                    self._to_iso_format(bd.timestamp), bd.symbol, bd.timeframe,
                    bd.open, bd.high, bd.low, bd.close, bd.volume,
                    bd.atr, bd.sentiment_score # Added new fields
                ) for bd in bar_data_list
            ]
            cursor.executemany("""
                INSERT OR REPLACE INTO bar_data 
                (timestamp, symbol, timeframe, open, high, low, close, volume, atr, sentiment_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?) 
            """, data_to_insert) # Added placeholders for new fields
            self.conn.commit()
            logger.info(f"Stored/Replaced {len(data_to_insert)} bar data entries. ({cursor.rowcount} affected)")
            return True
        except Exception as e:
            logger.error(f"Error storing bar data: {e}", exc_info=True); return False

    def retrieve_bar_data(self, symbol: str, timeframe: str, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> List[BarData]:
        if not self.conn: logger.error("No DB connection for bar data."); return []
        try:
            cursor = self.conn.cursor()
            query = "SELECT timestamp, open, high, low, close, volume, symbol, timeframe, atr, sentiment_score FROM bar_data WHERE symbol = ? AND timeframe = ?"
            params = [symbol, timeframe]

            if start_date: query += " AND timestamp >= ?"; params.append(self._to_iso_format(start_date))
            if end_date: query += " AND timestamp <= ?"; params.append(self._to_iso_format(end_date))
            query += " ORDER BY timestamp ASC"

            cursor.execute(query, tuple(params))
            rows = cursor.fetchall()
            bar_data_list = []
            for row in rows:
                dt_object = self._from_iso_format(row['timestamp'])
                if not dt_object: continue
                bar_data_list.append(BarData(
                    timestamp=dt_object, open=row['open'], high=row['high'], low=row['low'],
                    close=row['close'], volume=row['volume'], symbol=row['symbol'], timeframe=row['timeframe'],
                    atr=row['atr'], sentiment_score=row['sentiment_score'] # Added new fields
                ))
            logger.info(f"Retrieved {len(bar_data_list)} bar data entries for {symbol} ({timeframe}).")
            return bar_data_list
        except Exception as e:
            logger.error(f"Error retrieving bar data for {symbol} ({timeframe}): {e}", exc_info=True); return []

    def store_news_articles(self, articles: List[NewsArticle]):
        if not self.conn: logger.error("No DB connection for news articles."); return False
        if not articles: logger.info("No news articles to store."); return True
        try:
            cursor = self.conn.cursor()
            data_to_insert = []
            for article in articles:
                data_to_insert.append((
                    article.id, article.url, article.title,
                    self._to_iso_format(article.publication_date),
                    self._to_iso_format(article.retrieval_date),
                    article.source, article.content, article.summary,
                    article.sentiment_score, article.sentiment_label,
                    article.sentiment_confidence,
                    json.dumps(article.key_themes) if article.key_themes else None,
                    json.dumps(article.related_symbols) if article.related_symbols else None,
                    json.dumps(article.raw_llm_response) if article.raw_llm_response else None
                ))
            
            cursor.executemany("""
                INSERT OR REPLACE INTO news_articles
                (id, url, title, publication_date, retrieval_date, source, content, summary,
                 sentiment_score, sentiment_label, sentiment_confidence, key_themes, related_symbols, raw_llm_response)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, data_to_insert)
            self.conn.commit()
            logger.info(f"Stored/Replaced {len(data_to_insert)} news articles. ({cursor.rowcount} affected)")
            return True
        except Exception as e:
            logger.error(f"Error storing news articles: {e}", exc_info=True); return False

    def retrieve_news_articles(self, symbol: Optional[str] = None, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None, source: Optional[str] = None, limit: int = 100) -> List[NewsArticle]:
        if not self.conn: logger.error("No DB connection for news articles."); return []
        try:
            cursor = self.conn.cursor()
            query = "SELECT * FROM news_articles WHERE 1=1"
            params = []

            if symbol: # Search in related_symbols (requires LIKE or a better FTS setup)
                query += " AND related_symbols LIKE ?"
                params.append(f"%\"{symbol}\"%") # Simple JSON array search, not very efficient
            if start_date: # Based on publication_date
                query += " AND publication_date >= ?"
                params.append(self._to_iso_format(start_date))
            if end_date:
                query += " AND publication_date <= ?"
                params.append(self._to_iso_format(end_date))
            if source:
                query += " AND source = ?"
                params.append(source)
            
            query += " ORDER BY publication_date DESC, retrieval_date DESC LIMIT ?"
            params.append(limit)

            cursor.execute(query, tuple(params))
            rows = cursor.fetchall()
            articles_list = []
            for row in rows:
                articles_list.append(NewsArticle(
                    id=row['id'], url=row['url'], title=row['title'],
                    publication_date=self._from_iso_format(row['publication_date']),
                    retrieval_date=self._from_iso_format(row['retrieval_date']),
                    source=row['source'], content=row['content'], summary=row['summary'],
                    sentiment_score=row['sentiment_score'], sentiment_label=row['sentiment_label'],
                    sentiment_confidence=row['sentiment_confidence'],
                    key_themes=json.loads(row['key_themes']) if row['key_themes'] else [],
                    related_symbols=json.loads(row['related_symbols']) if row['related_symbols'] else [],
                    raw_llm_response=json.loads(row['raw_llm_response']) if row['raw_llm_response'] else None
                ))
            logger.info(f"Retrieved {len(articles_list)} news articles with given criteria.")
            return articles_list
        except Exception as e:
            logger.error(f"Error retrieving news articles: {e}", exc_info=True); return []

    def close(self):
        if self.conn: self.conn.close(); logger.info("Database connection closed."); self.conn = None
    def __del__(self): self.close()
</code>

kamikaze_komodo/data_handling/data_fetcher.py:
<code>
# kamikaze_komodo/data_handling/data_fetcher.py
import ccxt.async_support as ccxt # Use async version for future compatibility
import asyncio
from typing import List, Optional
from datetime import datetime, timedelta, timezone

# Assuming these are correctly located relative to this file for your project structure
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.core.utils import ohlcv_to_bardata
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Ensure settings is loaded globally

logger = get_logger(__name__)

class DataFetcher:
    """
    Fetches historical and real-time market data using CCXT.
    """
    def __init__(self): # MODIFIED: No longer takes exchange_id as an argument
        if not settings:
            logger.critical("Settings not loaded. DataFetcher cannot be initialized.")
            raise ValueError("Settings not loaded. Ensure config files are present and correct.")

        self.exchange_id = settings.exchange_id_to_use # MODIFIED: Get from global settings
        exchange_class = getattr(ccxt, self.exchange_id, None)
        
        if not exchange_class:
            logger.error(f"Exchange '{self.exchange_id}' is not supported by CCXT.")
            raise ValueError(f"Exchange '{self.exchange_id}' is not supported by CCXT.")

        # API keys should be specific to the selected exchange_id 
        # (e.g., Kraken Spot keys for 'kraken', Kraken Futures Demo keys for 'krakenfutures')
        config = {
            'apiKey': settings.kraken_api_key, # This assumes kraken_api_key holds the relevant key
            'secret': settings.kraken_secret_key, # This assumes kraken_secret_key holds the relevant secret
            'enableRateLimit': True, # Recommended by CCXT
        }
        
        # Example: If your settings had distinct keys for different exchanges:
        # if self.exchange_id == 'krakenfutures':
        #     config['apiKey'] = settings.kraken_futures_api_key 
        #     config['secret'] = settings.kraken_futures_secret_key
        # elif self.exchange_id == 'kraken':
        #     config['apiKey'] = settings.kraken_spot_api_key
        #     config['secret'] = settings.kraken_spot_secret_key
        # For now, we use the general kraken_api_key/secret from settings.

        self.exchange = exchange_class(config)
        logger.info(f"Instantiated CCXT exchange class: {self.exchange_id}")

        if settings.kraken_testnet: # This flag now controls sandbox mode for the selected exchange
            if hasattr(self.exchange, 'set_sandbox_mode') and callable(self.exchange.set_sandbox_mode):
                try:
                    self.exchange.set_sandbox_mode(True)
                    logger.info(f"CCXT sandbox mode successfully enabled for {self.exchange_id}.")
                    # You can log the API URL to verify it changed, e.g.:
                    # logger.info(f"Using API URLs: {self.exchange.urls['api']}")
                except ccxt.NotSupported:
                    logger.warning(f"{self.exchange_id} does not support unified set_sandbox_mode via method. Testnet functionality might depend on specific API keys or default URLs for this exchange class.")
                except Exception as e:
                    logger.error(f"An error occurred while trying to set sandbox mode for {self.exchange_id}: {e}", exc_info=True)
            else:
                logger.warning(f"{self.exchange_id} CCXT class does not have a 'set_sandbox_mode' method. Testnet operation relies on correct API keys for the test environment and default URLs.")
        
        self.exchange.verbose = True # Essential for debugging API calls
        logger.info(f"Initialized DataFetcher for '{self.exchange_id}'. Configured Testnet (Sandbox) from settings: {settings.kraken_testnet}")


    async def fetch_historical_ohlcv(
        self,
        symbol: str,
        timeframe: str,
        since: Optional[datetime] = None,
        limit: Optional[int] = None,
        params: Optional[dict] = None
    ) -> List[BarData]:
        if not self.exchange.has['fetchOHLCV']:
            logger.error(f"{self.exchange_id} does not support fetchOHLCV.")
            # await self.close() # Closing here might be premature if other operations are pending
            return []

        since_timestamp_ms = None
        if since:
            if since.tzinfo is None: 
                since = since.replace(tzinfo=timezone.utc)
            since_timestamp_ms = int(since.timestamp() * 1000)

        ohlcv_data_list: List[BarData] = []
        try:
            logger.info(f"Fetching historical OHLCV for {symbol} ({timeframe}) from exchange {self.exchange_id} since {since} with limit {limit}")
            raw_ohlcv = await self.exchange.fetch_ohlcv(symbol, timeframe, since_timestamp_ms, limit, params or {})            
            # More robust check for raw_ohlcv
            if raw_ohlcv is not None and isinstance(raw_ohlcv, list):
                if not raw_ohlcv: # Empty list
                    logger.info(f"No OHLCV data returned (empty list) for {symbol} ({timeframe}) from {self.exchange_id} with the given parameters.")
                else:
                    for entry in raw_ohlcv:
                        try:
                            bar = ohlcv_to_bardata(entry, symbol, timeframe)
                            ohlcv_data_list.append(bar)
                        except ValueError as e_bar:
                            logger.warning(f"Skipping invalid OHLCV entry for {symbol} ({timeframe}): {entry}. Error: {e_bar}")
                    logger.info(f"Successfully fetched {len(ohlcv_data_list)} candles for {symbol} ({timeframe}) from {self.exchange_id}.")
            elif raw_ohlcv is None:
                 logger.info(f"No OHLCV data returned (got None) for {symbol} ({timeframe}) from {self.exchange_id} with the given parameters.")
            else: # It's something else, not None and not a list
                logger.warning(f"Unexpected data type received for OHLCV for {symbol} ({timeframe}) from {self.exchange_id}: {type(raw_ohlcv)}. Data: {str(raw_ohlcv)[:200]}")


        except ccxt.NetworkError as e:
            logger.error(f"Network error fetching OHLCV for {symbol} from {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error fetching OHLCV for {symbol} from {self.exchange_id}: {e}", exc_info=True)
        except Exception as e: # Generic catch-all
            logger.error(f"An unexpected error occurred in fetch_historical_ohlcv for {symbol} from {self.exchange_id}: {e}", exc_info=True)
        
        return ohlcv_data_list

    async def fetch_historical_data_for_period(
        self,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime = datetime.now(timezone.utc)
    ) -> List[BarData]:
        all_bars: List[BarData] = []
        current_start_date = start_date
        
        try:
            timeframe_duration_seconds = self.exchange.parse_timeframe(timeframe)
        except Exception as e_tf:
            logger.error(f"Failed to parse timeframe '{timeframe}' using CCXT for {self.exchange_id}: {e_tf}")
            timeframe_duration_seconds = None # Fallback if parse_timeframe itself errors

        if timeframe_duration_seconds is None: # Still None after try-except
            logger.error(f"Could not parse timeframe: {timeframe} for {self.exchange_id}. Cannot paginate effectively. Attempting single fetch.")
            return await self.fetch_historical_ohlcv(symbol, timeframe, since=start_date, limit=1000) # Example limit

        logger.info(f"Fetching historical period data for {symbol} ({timeframe}) on {self.exchange_id} from {start_date} to {end_date}")

        while current_start_date < end_date:
            limit_per_call = 500 # Adjust as needed
            
            logger.debug(f"Fetching batch for {symbol} from {current_start_date} with limit {limit_per_call}")
            bars = await self.fetch_historical_ohlcv(symbol, timeframe, since=current_start_date, limit=limit_per_call)
            
            if not bars: # Includes None or empty list after fetch_historical_ohlcv's logging
                logger.info(f"No more data found for {symbol} ({timeframe}) starting {current_start_date}, or an error occurred during fetch.")
                break 
            
            # Filter bars that are strictly before the overall end_date
            # The timestamp from OHLCV is the start of the candle.
            # If a candle's start is >= end_date, we don't need it or subsequent ones.
            relevant_bars = [b for b in bars if b.timestamp < end_date]
            
            if not relevant_bars:
                if bars and bars[0].timestamp >= end_date: # First fetched bar is already past our period
                     logger.debug(f"First bar fetched ({bars[0].timestamp}) is already at or after end_date ({end_date}). Stopping pagination.")
                break # No relevant bars in this batch

            all_bars.extend(relevant_bars)
            
            # Move to the next period: start after the last fetched relevant candle
            last_fetched_timestamp = relevant_bars[-1].timestamp
            # To get the start of the *next* candle, add the timeframe duration
            current_start_date = last_fetched_timestamp + timedelta(seconds=timeframe_duration_seconds)
            
            if current_start_date >= end_date: # Optimization: if next fetch starts at or after end_date
                logger.debug("Next calculated start_date is at or after end_date. Concluding pagination.")
                break
            
            logger.debug(f"Fetched {len(relevant_bars)} relevant bars. Next fetch for {symbol} will start from {current_start_date}. Total collected: {len(all_bars)}")
            
            # Respect rate limits (ensure rateLimit is a number)
            if isinstance(self.exchange.rateLimit, (int, float)) and self.exchange.rateLimit > 0:
                await asyncio.sleep(self.exchange.rateLimit / 1000.0) 
            else:
                await asyncio.sleep(0.2) # Default small delay if rateLimit is not standard

        # Remove duplicates (if any from overlapping fetches, though logic above tries to avoid it) and sort
        if all_bars:
            unique_bars_dict = {bar.timestamp: bar for bar in all_bars}
            all_bars = sorted(list(unique_bars_dict.values()), key=lambda b: b.timestamp)
            logger.info(f"Total unique historical bars fetched for {symbol} ({timeframe}) in period: {len(all_bars)}")
        
        return all_bars

    async def subscribe_to_realtime_trades(self, symbol: str):
        # (Your existing placeholder code for this method)
        if not self.exchange.has['watchTrades']:
            logger.warning(f"{self.exchange_id} does not support real-time trade watching via WebSockets in CCXT.")
            # await self.close() # Consider if closing here is always appropriate
            return
        logger.info(f"Attempting to subscribe to real-time trades for {symbol} on {self.exchange_id}...")
        logger.warning("Real-time data subscription is a placeholder and not fully implemented.")
        pass

    async def close(self):
        """Closes the CCXT exchange connection."""
        try:
            if hasattr(self.exchange, 'close') and callable(self.exchange.close):
                await self.exchange.close()
            logger.info(f"CCXT exchange connection for {self.exchange_id} closed.")
        except Exception as e:
            logger.error(f"Error closing CCXT exchange connection for {self.exchange_id}: {e}", exc_info=True)


if __name__ == '__main__':
    # This is just for isolated testing of data_fetcher.
    # In the main application, this would be integrated differently.
    # Ensure settings are loaded before running this.
    # For example, by running from the root kamikaze_komodo directory or adjusting paths.
    # `python -m kamikaze_komodo.data_handling.data_fetcher`
    
    # Need to load settings explicitly if running standalone for testing
    # from kamikaze_komodo.config.settings import Config
    # settings_instance = Config(config_file='../config/config.ini', secrets_file='../config/secrets.ini')
    # global settings # make it available globally in this module for the example
    # settings = settings_instance

    # if settings:
    #    asyncio.run(main_data_fetcher_example())
    # else:
    #    print("Failed to load settings for standalone data_fetcher example.")
    pass
</code>

kamikaze_komodo/data_handling/__init__.py:
<code>
# kamikaze_komodo/data_handling/__init__.py
# This file makes the 'data_handling' directory a Python package.
</code>

kamikaze_komodo/exchange_interaction/exchange_api.py:
<code>
# kamikaze_komodo/exchange_interaction/exchange_api.py
import ccxt.async_support as ccxt
import asyncio
from typing import Dict, Optional, List
from kamikaze_komodo.core.enums import OrderType, OrderSide
from kamikaze_komodo.core.models import Order
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Import global settings
from datetime import datetime, timezone # Ensure timezone is imported

logger = get_logger(__name__)

class ExchangeAPI:
    """
    Handles interactions with the cryptocurrency exchange.
    Manages order placement, cancellation, and fetching account information.
    """
    def __init__(self, exchange_id: Optional[str] = None): # exchange_id is now optional
        if not settings:
            logger.critical("Settings not loaded. ExchangeAPI cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.exchange_id = exchange_id if exchange_id else settings.exchange_id_to_use
        exchange_class = getattr(ccxt, self.exchange_id, None)
        if not exchange_class:
            logger.error(f"Exchange {self.exchange_id} is not supported by CCXT.")
            raise ValueError(f"Exchange {self.exchange_id} is not supported by CCXT.")

        # Determine API keys based on the exchange_id
        # This example assumes a single set of keys in settings (e.g., KRAKEN_API)
        # For a multi-exchange system, you'd fetch keys specific to self.exchange_id
        api_key = settings.kraken_api_key # Defaulting to Kraken keys for now
        secret_key = settings.kraken_secret_key # Defaulting to Kraken keys
        use_testnet = settings.kraken_testnet # Defaulting to Kraken testnet setting

        # Example for specific exchange key loading (if settings were structured differently)
        # if self.exchange_id == 'binance':
        #     api_key = settings.binance_api_key
        #     secret_key = settings.binance_secret_key
        #     use_testnet = settings.binance_testnet
        # elif self.exchange_id == 'krakenfutures':
        #     api_key = settings.kraken_futures_api_key # etc.

        config = {
            'apiKey': api_key,
            'secret': secret_key,
            'enableRateLimit': True,
        }

        self.exchange = exchange_class(config)
        logger.info(f"Initialized ExchangeAPI for {self.exchange_id}.")

        if use_testnet:
            if hasattr(self.exchange, 'set_sandbox_mode') and callable(self.exchange.set_sandbox_mode):
                try:
                    self.exchange.set_sandbox_mode(True)
                    logger.info(f"Sandbox mode enabled for {self.exchange_id}.")
                except ccxt.NotSupported:
                    logger.warning(f"{self.exchange_id} does not support unified set_sandbox_mode. Testnet functionality depends on API keys/URL.")
                except Exception as e_sandbox:
                    logger.error(f"Error setting sandbox mode for {self.exchange_id}: {e_sandbox}")
            else:
                logger.warning(f"{self.exchange_id} does not have set_sandbox_mode. Testnet relies on specific API keys or default URL pointing to sandbox.")
        else:
            logger.info(f"Running in live mode for {self.exchange_id}.")


        if not api_key or "YOUR_API_KEY" in str(api_key).upper() or (isinstance(api_key, str) and "D27PYGI95TLS" in api_key.upper()): # Check specific placeholder
            logger.warning(f"API key for {self.exchange_id} appears to be a placeholder or is not configured. Authenticated calls may fail.")

    async def fetch_balance(self) -> Optional[Dict]:
        if not self.exchange.has['fetchBalance']:
            logger.error(f"{self.exchange_id} does not support fetchBalance.")
            return None
        try:
            balance = await self.exchange.fetch_balance()
            logger.info(f"Successfully fetched balance from {self.exchange_id}.")
            return balance
        except ccxt.NetworkError as e:
            logger.error(f"Network error fetching balance: {e}", exc_info=True)
        except ccxt.AuthenticationError as e_auth:
            logger.error(f"Authentication error fetching balance from {self.exchange_id}. Check API keys and permissions: {e_auth}", exc_info=True)
            return None # Explicitly return None on auth error
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error fetching balance: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred fetching balance: {e}", exc_info=True)
        return None

    async def create_order(
        self,
        symbol: str,
        order_type: OrderType,
        side: OrderSide,
        amount: float,
        price: Optional[float] = None,
        params: Optional[Dict] = None
    ) -> Optional[Order]:
        if order_type == OrderType.LIMIT and price is None:
            logger.error("Price must be specified for a LIMIT order.")
            return None

        if not self.exchange.has['createOrder']:
            logger.error(f"{self.exchange_id} does not support createOrder.")
            return None

        order_type_str = order_type.value
        side_str = side.value

        try:
            logger.info(f"Attempting to place {side_str} {order_type_str} order for {amount} {symbol} at price {price if price else 'market'} on {self.exchange_id}")

            # Check for placeholder API keys again before actual call
            is_placeholder_key = not self.exchange.apiKey or "YOUR_API_KEY" in self.exchange.apiKey.upper() or "D27PYGI95TLS" in self.exchange.apiKey.upper()
            if settings.kraken_testnet and is_placeholder_key : # Use general testnet flag
                logger.warning(f"Simulating order creation for {self.exchange_id} due to testnet mode and placeholder API keys.")
                simulated_order_id = f"sim_{self.exchange_id}_{ccxt.Exchange.uuid()}"
                return Order(
                    id=simulated_order_id,
                    symbol=symbol,
                    type=order_type,
                    side=side,
                    amount=amount,
                    price=price if order_type == OrderType.LIMIT else None,
                    timestamp=datetime.now(timezone.utc), # Use timezone.utc
                    status="open",
                    exchange_id=simulated_order_id
                )

            exchange_order_response = await self.exchange.create_order(symbol, order_type_str, side_str, amount, price, params or {})
            logger.info(f"Successfully placed order on {self.exchange_id}. Order ID: {exchange_order_response.get('id')}")

            created_order = Order(
                id=str(exchange_order_response.get('id')),
                symbol=exchange_order_response.get('symbol'),
                type=OrderType(exchange_order_response.get('type', order_type_str).lower()),
                side=OrderSide(exchange_order_response.get('side', side_str).lower()),
                amount=float(exchange_order_response.get('amount', amount)),
                price=float(exchange_order_response['price']) if exchange_order_response.get('price') else None,
                timestamp=datetime.fromtimestamp(exchange_order_response['timestamp'] / 1000, tz=timezone.utc) if exchange_order_response.get('timestamp') else datetime.now(timezone.utc),
                status=exchange_order_response.get('status', 'open'),
                filled_amount=float(exchange_order_response.get('filled', 0.0)),
                average_fill_price=float(exchange_order_response.get('average')) if exchange_order_response.get('average') else None,
                exchange_id=str(exchange_order_response.get('id'))
            )
            return created_order

        except ccxt.InsufficientFunds as e:
            logger.error(f"Insufficient funds to place order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.InvalidOrder as e:
            logger.error(f"Invalid order parameters for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.AuthenticationError as e_auth:
            logger.error(f"Authentication error placing order for {symbol} on {self.exchange_id}. Check API keys: {e_auth}", exc_info=True)
        except ccxt.NetworkError as e:
            logger.error(f"Network error placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e: # Catch specific exchange errors before generic Exception
            logger.error(f"Exchange error placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        return None

    async def cancel_order(self, order_id: str, symbol: Optional[str] = None, params: Optional[Dict] = None) -> bool:
        if not self.exchange.has['cancelOrder']:
            logger.error(f"{self.exchange_id} does not support cancelOrder.")
            return False
        try:
            await self.exchange.cancel_order(order_id, symbol, params or {})
            logger.info(f"Successfully requested cancellation for order ID {order_id} on {self.exchange_id}.")
            return True
        except ccxt.OrderNotFound as e:
            logger.error(f"Order ID {order_id} not found for cancellation on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.NetworkError as e:
            logger.error(f"Network error canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        return False

    async def fetch_order(self, order_id: str, symbol: Optional[str] = None) -> Optional[Order]:
        if not self.exchange.has['fetchOrder']:
            logger.warning(f"{self.exchange_id} does not support fetching individual orders directly.")
            return None
        try:
            exchange_order_response = await self.exchange.fetch_order(order_id, symbol)
            fetched_order = Order(
                id=str(exchange_order_response.get('id')),
                symbol=exchange_order_response.get('symbol'),
                type=OrderType(exchange_order_response.get('type').lower()),
                side=OrderSide(exchange_order_response.get('side').lower()),
                amount=float(exchange_order_response.get('amount')),
                price=float(exchange_order_response['price']) if exchange_order_response.get('price') else None,
                timestamp=datetime.fromtimestamp(exchange_order_response['timestamp'] / 1000, tz=timezone.utc) if exchange_order_response.get('timestamp') else datetime.now(timezone.utc),
                status=exchange_order_response.get('status'),
                filled_amount=float(exchange_order_response.get('filled', 0.0)),
                average_fill_price=float(exchange_order_response.get('average')) if exchange_order_response.get('average') else None,
                exchange_id=str(exchange_order_response.get('id'))
            )
            return fetched_order
        except ccxt.OrderNotFound:
            logger.warning(f"Order {order_id} not found on {self.exchange_id}.")
        except Exception as e:
            logger.error(f"Error fetching order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        return None

    async def fetch_open_orders(self, symbol: Optional[str] = None, since: Optional[datetime] = None, limit: Optional[int] = None) -> List[Order]: # Changed since to datetime
        open_orders_list = []
        if not self.exchange.has['fetchOpenOrders']:
            logger.warning(f"{self.exchange_id} does not support fetching open orders.")
            return open_orders_list
        try:
            since_timestamp_ms = int(since.timestamp() * 1000) if since else None
            raw_orders = await self.exchange.fetch_open_orders(symbol, since_timestamp_ms, limit)
            for ex_order in raw_orders:
                order = Order(
                    id=str(ex_order.get('id')),
                    symbol=ex_order.get('symbol'),
                    type=OrderType(ex_order.get('type').lower()),
                    side=OrderSide(ex_order.get('side').lower()),
                    amount=float(ex_order.get('amount')),
                    price=float(ex_order['price']) if ex_order.get('price') else None,
                    timestamp=datetime.fromtimestamp(ex_order['timestamp'] / 1000, tz=timezone.utc) if ex_order.get('timestamp') else datetime.now(timezone.utc),
                    status=ex_order.get('status', 'open'),
                    filled_amount=float(ex_order.get('filled', 0.0)),
                    average_fill_price=float(ex_order.get('average')) if ex_order.get('average') else None,
                    exchange_id=str(ex_order.get('id'))
                )
                open_orders_list.append(order)
            logger.info(f"Fetched {len(open_orders_list)} open orders for symbol {symbol if symbol else 'all'} on {self.exchange_id}.")
        except Exception as e:
            logger.error(f"Error fetching open orders on {self.exchange_id}: {e}", exc_info=True)
        return open_orders_list

    async def close(self):
        try:
            if hasattr(self.exchange, 'close') and callable(self.exchange.close):
                await self.exchange.close()
                logger.info(f"CCXT exchange connection for {self.exchange_id} closed.")
        except Exception as e:
            logger.error(f"Error closing CCXT exchange connection for {self.exchange_id}: {e}", exc_info=True)

# Example Usage (run within an asyncio event loop):
async def main_exchange_api_example():
    if not settings:
        print("Settings could not be loaded. Exiting example.")
        return

    exchange_api = ExchangeAPI() # Uses exchange_id from settings

    balance = await exchange_api.fetch_balance()
    if balance:
        logger.info(f"Free USD Balance: {balance.get('USD', {}).get('free', 'N/A')}")
        logger.info(f"Free {settings.default_symbol.split('/')[0]} Balance: {balance.get(settings.default_symbol.split('/')[0], {}).get('free', 'N/A')}")

    # target_symbol = settings.default_symbol
    # order_to_place = await exchange_api.create_order(
    #     symbol=target_symbol,
    #     order_type=OrderType.LIMIT,
    #     side=OrderSide.BUY,
    #     amount=0.0001,
    #     price=15000.0
    # )
    # if order_to_place:
    #     logger.info(f"Practice order placed/simulated: ID {order_to_place.id}, Status {order_to_place.status}")
    # else:
    #     logger.warning("Practice order placement failed or was not attempted.")
    await exchange_api.close()

# if __name__ == '__main__':
#     asyncio.run(main_exchange_api_example())
</code>

kamikaze_komodo/exchange_interaction/__init__.py:
<code>
# kamikaze_komodo/exchange_interaction/__init__.py
# This file makes the 'exchange_interaction' directory a Python package.
</code>

kamikaze_komodo/portfolio_constructor/asset_allocator.py:
<code>
# kamikaze_komodo/portfolio_constructor/asset_allocator.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from kamikaze_komodo.core.models import BarData # Or other relevant models
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BaseAssetAllocator(ABC):
    """
    Abstract base class for asset allocation strategies.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def allocate(
        self,
        assets: List[str], # List of available asset symbols
        current_holdings: Dict[str, float], # Current quantity of each asset held
        market_data: Dict[str, BarData], # Current market data for available assets
        available_capital: float
    ) -> Dict[str, float]: # Target allocation in terms of capital or percentage
        """
        Determines the target allocation for assets.

        Args:
            assets (List[str]): List of asset symbols to consider for allocation.
            current_holdings (Dict[str, float]): Current holdings (e.g. {'BTC/USD': 0.5}).
            market_data (Dict[str, BarData]): Latest market data for each asset.
            available_capital (float): Total capital available for allocation.

        Returns:
            Dict[str, float]: Dictionary mapping asset symbols to target capital allocation.
                              (e.g., {'BTC/USD': 5000.0, 'ETH/USD': 5000.0})
                              or target percentage (e.g. {'BTC/USD': 0.5, 'ETH/USD': 0.5})
                              The interpretation depends on the portfolio manager.
        """
        pass

class FixedWeightAssetAllocator(BaseAssetAllocator):
    """
    Allocates assets based on predefined fixed weights.
    For a single asset strategy, this will allocate 100% to that asset if a signal is present.
    """
    def __init__(self, target_weights: Dict[str, float], params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.target_weights = target_weights
        if not self.target_weights:
            logger.warning("FixedWeightAssetAllocator initialized with no target weights.")
        elif abs(sum(self.target_weights.values()) - 1.0) > 1e-6 and sum(self.target_weights.values()) != 0 : # Allow 0 for no allocation
             logger.warning(f"Sum of target weights ({sum(self.target_weights.values())}) is not 1.0. Allocations will be normalized or may behave unexpectedly.")


    def allocate(
        self,
        assets: List[str],
        current_holdings: Dict[str, float],
        market_data: Dict[str, BarData],
        available_capital: float
    ) -> Dict[str, float]:
        """
        Returns the predefined target weights for allocation.
        In a single-asset backtest, if the asset is in target_weights, it implies full allocation for trades.
        The actual amount to buy/sell will be determined by the PositionSizer.
        This method primarily provides the desired *proportion* of the portfolio for each asset.
        """
        allocation_targets_capital: Dict[str, float] = {}
        total_weight = sum(self.target_weights.get(asset, 0.0) for asset in assets if asset in self.target_weights)

        if total_weight == 0: # No weights for available assets or all weights are zero
            logger.debug("No target weights specified for the given assets or total weight is zero. No allocation.")
            return {asset: 0.0 for asset in assets}

        for asset in assets:
            if asset in self.target_weights:
                normalized_weight = self.target_weights[asset] / total_weight # Normalize if sum is not 1
                allocation_targets_capital[asset] = available_capital * normalized_weight
            else:
                # Assets not in target_weights get zero allocation from this allocator's perspective
                allocation_targets_capital[asset] = 0.0

        logger.debug(f"FixedWeightAssetAllocator target capital allocation: {allocation_targets_capital}")
        return allocation_targets_capital

# Example for future more complex allocators:
# class OptimalFAllocator(BaseAssetAllocator): ...
# class HRPAllocator(BaseAssetAllocator): ...
</code>

kamikaze_komodo/portfolio_constructor/rebalancer.py:
<code>
# kamikaze_komodo/portfolio_constructor/rebalancer.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from kamikaze_komodo.core.models import PortfolioSnapshot # Or other relevant models
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BaseRebalancer(ABC):
    """
    Abstract base class for portfolio rebalancing logic.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def needs_rebalancing(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations: Dict[str, float] # e.g. {'BTC/USD': 0.6, 'ETH/USD': 0.4} as fractions
    ) -> bool:
        """
        Determines if the portfolio needs rebalancing based on current state and targets.

        Args:
            current_portfolio (PortfolioSnapshot): The current state of the portfolio.
            target_allocations (Dict[str, float]): The desired target allocations (e.g., asset: percentage).

        Returns:
            bool: True if rebalancing is needed, False otherwise.
        """
        pass

    @abstractmethod
    def generate_rebalancing_orders(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations: Dict[str, float]
    ) -> List[Dict[str, Any]]: # List of order parameters
        """
        Generates orders needed to rebalance the portfolio to target allocations.

        Args:
            current_portfolio (PortfolioSnapshot): The current state of the portfolio.
            target_allocations (Dict[str, float]): The desired target allocations.

        Returns:
            List[Dict[str, Any]]: A list of order parameters (e.g., for exchange_api.create_order).
                                  Example: [{'symbol': 'BTC/USD', 'type': OrderType.MARKET, 'side': OrderSide.SELL, 'amount': 0.1}, ...]
        """
        pass

class BasicRebalancer(BaseRebalancer):
    """
    A basic rebalancer that might trigger rebalancing if deviations exceed a threshold.
    For Phase 3, this might be very simple or just a placeholder structure.
    """
    def __init__(self, deviation_threshold: float = 0.05, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.deviation_threshold = deviation_threshold
        logger.info(f"BasicRebalancer initialized with deviation threshold: {self.deviation_threshold}")

    def needs_rebalancing(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations: Dict[str, float]
    ) -> bool:
        if not target_allocations:
            logger.debug("No target allocations provided, rebalancing not needed.")
            return False
        if current_portfolio.total_value_usd <= 0: # Avoid division by zero if portfolio has no value
            logger.debug("Portfolio total value is zero or negative, cannot calculate current weights.")
            # Rebalancing might be needed if there are target allocations and capital
            return any(target_allocations.get(asset,0) > 0 for asset in target_allocations)


        for asset, target_weight in target_allocations.items():
            current_asset_value = 0.0
            # This part needs current prices to evaluate asset value if not directly in portfolio snapshot
            # Assuming portfolio_snapshot.positions has quantities and we need market_data for prices
            # For simplicity, let's assume we'd get asset values directly or calculate them.
            # This is a conceptual check.
            # current_asset_quantity = current_portfolio.positions.get(asset, 0.0)
            # current_price = get_current_price(asset) # This function would be needed
            # current_asset_value = current_asset_quantity * current_price
            # current_weight = current_asset_value / current_portfolio.total_value_usd

            # Simplified: This check needs proper value calculation of current positions.
            # For now, let's assume this logic will be more fleshed out when multi-asset trading is live.
            # If we simply check if target exists and we don't have it, or vice versa:
            # Placeholder logic:
            if target_weight > 0 and current_portfolio.positions.get(asset, 0.0) == 0:
                 logger.info(f"Rebalancing needed: Target weight for {asset} is {target_weight} but position is zero.")
                 return True
            if target_weight == 0 and current_portfolio.positions.get(asset, 0.0) > 0:
                 logger.info(f"Rebalancing needed: Target weight for {asset} is zero but position exists.")
                 return True
        
        logger.debug("BasicRebalancer: No immediate rebalancing need detected based on simple checks.")
        return False # Placeholder

    def generate_rebalancing_orders(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations: Dict[str, float] # Target weights (e.g. {'BTC/USD': 0.5})
    ) -> List[Dict[str, Any]]:
        # This is highly conceptual for a single-asset backtester.
        # In a real multi-asset scenario, this would calculate trades to match target_allocations.
        logger.warning("generate_rebalancing_orders in BasicRebalancer is conceptual and not fully implemented for generating actual orders yet.")
        orders = []
        # Example logic:
        # total_value = current_portfolio.total_value_usd
        # for asset, target_weight in target_allocations.items():
        # target_value = total_value * target_weight
        # current_value = get_current_value_of_asset(asset, current_portfolio) # Needs market price
        # diff_value = target_value - current_value
        # if abs(diff_value) > some_minimum_trade_value:
        # amount_to_trade = diff_value / get_current_price(asset)
        # side = OrderSide.BUY if amount_to_trade > 0 else OrderSide.SELL
        # orders.append({'symbol': asset, 'type': OrderType.MARKET, 'side': side, 'amount': abs(amount_to_trade)})
        return orders
</code>

kamikaze_komodo/portfolio_constructor/__init__.py:
<code>
# kamikaze_komodo/portfolio_constructor/__init__.py
# This file makes the 'portfolio_constructor' directory a Python package.
</code>

kamikaze_komodo/risk_control_module/position_sizer.py:
<code>
# kamikaze_komodo/risk_control_module/position_sizer.py
from abc import ABC, abstractmethod
from typing import Dict, Optional, Any
from kamikaze_komodo.core.models import BarData # For ATR based sizers potentially
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BasePositionSizer(ABC):
    """
    Abstract base class for position sizing strategies.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float, # Total equity
        strategy_signal_strength: Optional[float] = None, # e.g. ML confidence
        latest_bar: Optional[BarData] = None, # For ATR or volatility based
        atr_value: Optional[float] = None # Explicit ATR if available
    ) -> Optional[float]: # Returns position size in asset units, or None if no trade
        """
        Calculates the size of the position to take.

        Args:
            symbol (str): The asset symbol.
            current_price (float): The current price of the asset.
            available_capital (float): The cash available for trading. (May not be used by all sizers)
            current_portfolio_value (float): The total current value of the portfolio (equity).
            strategy_signal_strength (Optional[float]): Confidence or strength of the signal.
            latest_bar (Optional[BarData]): Latest bar data for volatility calculation.
            atr_value (Optional[float]): Pre-calculated ATR value.

        Returns:
            Optional[float]: The quantity of the asset to trade. None if cannot size or no trade.
        """
        pass

class FixedFractionalPositionSizer(BasePositionSizer):
    """
    Sizes positions based on a fixed fraction of the total portfolio equity.
    """
    def __init__(self, fraction: float = 0.01, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        if not 0 < fraction <= 1.0:
            logger.error(f"Fraction must be between 0 (exclusive) and 1 (inclusive). Got {fraction}")
            raise ValueError("Fraction must be > 0 and <= 1.")
        self.fraction_to_risk = fraction # This is the fraction of *equity* to risk
        logger.info(f"FixedFractionalPositionSizer initialized with fraction: {self.fraction_to_risk}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float, # Cash
        current_portfolio_value: float, # Equity
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        if current_price <= 0:
            logger.warning(f"Current price for {symbol} is non-positive ({current_price}). Cannot calculate position size.")
            return None
        if current_portfolio_value <= 0:
            logger.warning(f"Current portfolio value is non-positive ({current_portfolio_value}). Cannot calculate position size.")
            return None

        # This sizer determines how much capital to *allocate* to this trade based on fixed fraction of equity.
        # It does not inherently consider a stop-loss to determine "risk per trade".
        # A more common "fixed fractional" risks a fraction of equity *based on a stop-loss distance*.
        # The current plan says "fixed fractional", so interpreting as fraction of equity to *allocate*.
        
        capital_to_allocate = current_portfolio_value * self.fraction_to_risk
        
        # Ensure we don't allocate more than available cash if that's a constraint.
        # This check is important because `current_portfolio_value` includes value of existing positions.
        # However, for a new trade, we typically use available cash.
        # Let's assume the "fraction" applies to overall equity to decide the *value* of the new position.
        
        if capital_to_allocate > available_capital :
            logger.warning(f"Calculated capital to allocate ({capital_to_allocate:.2f}) for {symbol} exceeds available cash ({available_capital:.2f}). Using available cash.")
            capital_to_allocate = available_capital

        if capital_to_allocate <= 0:
            logger.info(f"No capital to allocate for {symbol} based on fixed fraction or available cash.")
            return None

        position_size = capital_to_allocate / current_price
        logger.info(f"FixedFractional Sizing for {symbol}: Allocating ${capital_to_allocate:.2f} (Equity: ${current_portfolio_value:.2f}, Fraction: {self.fraction_to_risk}). Position Size: {position_size:.6f} units at ${current_price:.2f}.")
        return position_size


class ATRBasedPositionSizer(BasePositionSizer):
    """
    Sizes positions based on Average True Range (ATR) to normalize risk per trade.
    This implementation assumes you risk a fixed percentage of portfolio equity,
    and the stop loss is N * ATR away.
    """
    def __init__(self, risk_per_trade_fraction: float = 0.01, atr_multiple_for_stop: float = 2.0, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.risk_per_trade_fraction = risk_per_trade_fraction # e.g., 0.01 for 1% of equity
        self.atr_multiple_for_stop = atr_multiple_for_stop # e.g., stop loss is 2 * ATR
        logger.info(f"ATRBasedPositionSizer initialized with risk_fraction: {self.risk_per_trade_fraction}, atr_multiple: {self.atr_multiple_for_stop}")


    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None # Allow passing pre-calculated ATR
    ) -> Optional[float]:

        if atr_value is None:
            if latest_bar and 'atr_period' in self.params:
                # Simplified: This would typically need a history of bars to calculate ATR.
                # For a single 'latest_bar', ATR calculation isn't directly possible unless it's already on the bar.
                # We'd normally use pandas_ta on a series.
                # This is a placeholder for where one would calculate ATR if not provided.
                logger.warning("ATR value not provided and history for calculation from latest_bar is not implemented in sizer directly. ATR must be passed or calculated by strategy/engine.")
                return None # Cannot calculate ATR here with just one bar without more logic
            else:
                logger.warning(f"ATR value not provided for {symbol}, and cannot calculate it. Required for ATRBasedPositionSizer.")
                return None
        
        if atr_value <= 0:
            logger.warning(f"ATR value for {symbol} is non-positive ({atr_value}). Cannot size position.")
            return None
        if current_price <= 0:
            logger.warning(f"Current price for {symbol} is non-positive ({current_price}). Cannot size position.")
            return None
        if current_portfolio_value <= 0:
            logger.warning(f"Current portfolio value is non-positive ({current_portfolio_value}). Cannot size position.")
            return None

        # Amount of capital to risk on this trade
        capital_at_risk = current_portfolio_value * self.risk_per_trade_fraction
        
        # Stop distance in price terms
        stop_distance_per_unit = self.atr_multiple_for_stop * atr_value
        if stop_distance_per_unit == 0:
            logger.warning(f"Stop distance per unit is zero for {symbol} (ATR: {atr_value}, Multiple: {self.atr_multiple_for_stop}). Cannot size position.")
            return None

        # Number of units (position size)
        position_size = capital_at_risk / stop_distance_per_unit

        # Cost of this position
        position_cost = position_size * current_price

        if position_cost > available_capital:
            logger.warning(f"Calculated position cost (${position_cost:.2f}) for {symbol} exceeds available cash (${available_capital:.2f}). Reducing size.")
            position_size = available_capital / current_price # Adjust size to available cash
            if position_size == 0: return None

        logger.info(f"ATRBased Sizing for {symbol}: Risking ${capital_at_risk:.2f} (Equity: ${current_portfolio_value:.2f}). "
                    f"ATR: {atr_value:.4f}, StopDist: ${stop_distance_per_unit:.2f}. "
                    f"Size: {position_size:.6f} units at ${current_price:.2f}.")
        return position_size
</code>

kamikaze_komodo/risk_control_module/stop_manager.py:
<code>
# kamikaze_komodo/risk_control_module/stop_manager.py
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from kamikaze_komodo.core.models import BarData, Trade
from kamikaze_komodo.core.enums import OrderSide
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BaseStopManager(ABC):
    """
    Abstract base class for stop-loss and take-profit management.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def check_stop_loss(
        self,
        current_trade: Trade,
        latest_bar: BarData
    ) -> Optional[float]: # Returns stop price if triggered, else None
        """
        Checks if the stop-loss condition is met for the current trade.

        Args:
            current_trade (Trade): The active trade object.
            latest_bar (BarData): The latest market data bar.

        Returns:
            Optional[float]: The price at which the stop-loss was triggered, or None if not triggered.
        """
        pass

    @abstractmethod
    def check_take_profit(
        self,
        current_trade: Trade,
        latest_bar: BarData
    ) -> Optional[float]: # Returns take profit price if triggered, else None
        """
        Checks if the take-profit condition is met for the current trade.

        Args:
            current_trade (Trade): The active trade object.
            latest_bar (BarData): The latest market data bar.

        Returns:
            Optional[float]: The price at which the take-profit was triggered, or None if not triggered.
        """
        pass

class PercentageStopManager(BaseStopManager):
    """
    Manages stops based on a fixed percentage from the entry price.
    """
    def __init__(self, stop_loss_pct: Optional[float] = None, take_profit_pct: Optional[float] = None, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.stop_loss_pct = stop_loss_pct
        self.take_profit_pct = take_profit_pct

        if self.stop_loss_pct is not None and not (0 < self.stop_loss_pct < 1.0):
            raise ValueError("stop_loss_pct must be between 0 and 1 (exclusive).")
        if self.take_profit_pct is not None and not (0 < self.take_profit_pct): # TP can be > 100%
            raise ValueError("take_profit_pct must be greater than 0.")
            
        logger.info(f"PercentageStopManager initialized. SL: {self.stop_loss_pct*100 if self.stop_loss_pct else 'N/A'}%, TP: {self.take_profit_pct*100 if self.take_profit_pct else 'N/A'}%")

    def check_stop_loss(
        self,
        current_trade: Trade,
        latest_bar: BarData
    ) -> Optional[float]:
        if not self.stop_loss_pct or not current_trade or current_trade.entry_price <= 0:
            return None

        if current_trade.side == OrderSide.BUY:
            stop_price = current_trade.entry_price * (1 - self.stop_loss_pct)
            if latest_bar.low <= stop_price:
                logger.info(f"STOP LOSS (BUY) triggered for trade {current_trade.id} ({current_trade.symbol}) at SL price {stop_price:.2f} (Bar Low: {latest_bar.low:.2f}, Entry: {current_trade.entry_price:.2f})")
                return stop_price # Exit at the calculated stop price
        elif current_trade.side == OrderSide.SELL: # For short positions
            stop_price = current_trade.entry_price * (1 + self.stop_loss_pct)
            if latest_bar.high >= stop_price:
                logger.info(f"STOP LOSS (SELL) triggered for trade {current_trade.id} ({current_trade.symbol}) at SL price {stop_price:.2f} (Bar High: {latest_bar.high:.2f}, Entry: {current_trade.entry_price:.2f})")
                return stop_price
        return None

    def check_take_profit(
        self,
        current_trade: Trade,
        latest_bar: BarData
    ) -> Optional[float]:
        if not self.take_profit_pct or not current_trade or current_trade.entry_price <= 0:
            return None

        if current_trade.side == OrderSide.BUY:
            profit_price = current_trade.entry_price * (1 + self.take_profit_pct)
            if latest_bar.high >= profit_price:
                logger.info(f"TAKE PROFIT (BUY) triggered for trade {current_trade.id} ({current_trade.symbol}) at TP price {profit_price:.2f} (Bar High: {latest_bar.high:.2f}, Entry: {current_trade.entry_price:.2f})")
                return profit_price # Exit at the calculated profit price
        elif current_trade.side == OrderSide.SELL: # For short positions
            profit_price = current_trade.entry_price * (1 - self.take_profit_pct)
            if latest_bar.low <= profit_price:
                logger.info(f"TAKE PROFIT (SELL) triggered for trade {current_trade.id} ({current_trade.symbol}) at TP price {profit_price:.2f} (Bar Low: {latest_bar.low:.2f}, Entry: {current_trade.entry_price:.2f})")
                return profit_price
        return None


class ATRStopManager(BaseStopManager):
    """
    Manages stops based on ATR.
    """
    def __init__(self, atr_multiple: float = 2.0, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.atr_multiple = atr_multiple
        # This manager expects ATR to be provided, typically calculated elsewhere (e.g. by strategy or engine)
        # and stored/passed with BarData or Trade object.
        logger.info(f"ATRStopManager initialized with ATR multiple: {self.atr_multiple}")

    def check_stop_loss(
        self,
        current_trade: Trade, # current_trade needs a field like `atr_at_entry`
        latest_bar: BarData  # latest_bar could have a field like `current_atr`
    ) -> Optional[float]:
        # This is a more advanced stop. For Phase 3, PercentageStopManager is primary.
        # This requires `atr_at_entry` to be stored with the trade.
        atr_at_entry = current_trade.custom_fields.get("atr_at_entry") if hasattr(current_trade, 'custom_fields') and current_trade.custom_fields else None

        if atr_at_entry is None or atr_at_entry <= 0:
            logger.debug(f"ATR at entry not available or invalid for trade {current_trade.id}. Cannot apply ATR stop.")
            return None
        
        stop_distance = self.atr_multiple * atr_at_entry

        if current_trade.side == OrderSide.BUY:
            stop_price = current_trade.entry_price - stop_distance
            if latest_bar.low <= stop_price:
                logger.info(f"ATR STOP LOSS (BUY) for {current_trade.symbol} at {stop_price:.2f} (Entry: {current_trade.entry_price:.2f}, ATR@Entry: {atr_at_entry:.4f}, BarLow: {latest_bar.low:.2f})")
                return stop_price
        elif current_trade.side == OrderSide.SELL:
            stop_price = current_trade.entry_price + stop_distance
            if latest_bar.high >= stop_price:
                logger.info(f"ATR STOP LOSS (SELL) for {current_trade.symbol} at {stop_price:.2f} (Entry: {current_trade.entry_price:.2f}, ATR@Entry: {atr_at_entry:.4f}, BarHigh: {latest_bar.high:.2f})")
                return stop_price
        return None

    def check_take_profit(
        self,
        current_trade: Trade,
        latest_bar: BarData
    ) -> Optional[float]:
        # ATR stops are typically not used for take profit in the same way.
        # Take profit might be a multiple of risk (e.g. 2R, where R is ATR stop distance) or other logic.
        # For simplicity, this basic ATRStopManager doesn't implement TP.
        return None
</code>

kamikaze_komodo/risk_control_module/__init__.py:
<code>
# kamikaze_komodo/risk_control_module/__init__.py
# This file makes the 'risk_control_module' directory a Python package.
logger_name = "KamikazeKomodo.risk_control_module" # Satisfy linter
</code>

kamikaze_komodo/strategy_framework/base_strategy.py:
<code>
# kamikaze_komodo/strategy_framework/base_strategy.py
# Updated to include optional sentiment_score in on_bar_data

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import pandas as pd
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BaseStrategy(ABC):
    """
    Abstract base class for all trading strategies.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        self.symbol = symbol
        self.timeframe = timeframe
        self.params = params if params is not None else {}
        self.current_position_status: Optional[SignalType] = None # Tracks if currently LONG, SHORT or None (no position)
        self.data_history = pd.DataFrame()
        logger.info(f"Initialized BaseStrategy '{self.name}' for {symbol} ({timeframe}) with params: {self.params}")

    @abstractmethod
    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        """
        Generates trading signals based on the provided historical data.
        This method is typically called once during backtesting setup or for historical analysis.

        Args:
            data (pd.DataFrame): DataFrame with historical OHLCV data, indexed by timestamp.
                                 Expected columns: 'open', 'high', 'low', 'close', 'volume'.
                                 May also contain 'atr', 'sentiment_score'.
            sentiment_series (Optional[pd.Series]): Series with historical sentiment scores, indexed by timestamp.

        Returns:
            pd.Series: A Pandas Series indexed by timestamp, containing SignalType values.
        """
        pass

    @abstractmethod
    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None) -> Optional[SignalType]:
        """
        Processes a new bar of data and decides on a trading action.
        This method is typically called for each new data point in a live or simulated environment.

        Args:
            bar_data (BarData): The new BarData object, potentially with .atr or .sentiment_score.
            sentiment_score (Optional[float]): External sentiment score for the current bar.
                                               (Note: bar_data.sentiment_score might also be used if populated by engine)

        Returns:
            Optional[SignalType]: A signal (LONG, SHORT, HOLD, CLOSE_LONG, CLOSE_SHORT) or None if no action.
        """
        pass
        
    def update_data_history(self, new_bar_data: BarData):
        """Appends new bar data to the internal history including ATR and sentiment if available."""
        new_row_data = {
            'open': new_bar_data.open, 'high': new_bar_data.high,
            'low': new_bar_data.low, 'close': new_bar_data.close,
            'volume': new_bar_data.volume
        }
        # Add optional fields if they exist on BarData model
        if hasattr(new_bar_data, 'atr') and new_bar_data.atr is not None:
            new_row_data['atr'] = new_bar_data.atr
        if hasattr(new_bar_data, 'sentiment_score') and new_bar_data.sentiment_score is not None:
            new_row_data['sentiment_score'] = new_bar_data.sentiment_score
            
        new_row = pd.DataFrame([new_row_data], index=[new_bar_data.timestamp])
        
        # Ensure columns match, adding missing ones with NaN if this is the first row with new columns
        if not self.data_history.empty:
            for col in new_row.columns:
                if col not in self.data_history.columns:
                    self.data_history[col] = pd.NA # Or np.nan
            for col in self.data_history.columns:
                 if col not in new_row.columns:
                    new_row[col] = pd.NA


        self.data_history = pd.concat([self.data_history, new_row])
        
        # Optional: Keep only a certain number of recent rows to manage memory
        # max_history_length = self.params.get('max_history_length', 1000)
        # if len(self.data_history) > max_history_length:
        #     self.data_history = self.data_history.iloc[-max_history_length:]

    def get_parameters(self) -> Dict[str, Any]:
        return self.params

    def set_parameters(self, params: Dict[str, Any]):
        self.params.update(params)
        logger.info(f"Strategy {self.__class__.__name__} parameters updated: {self.params}")

    @property
    def name(self) -> str:
        return self.__class__.__name__
</code>

kamikaze_komodo/strategy_framework/strategy_manager.py:
<code>
# kamikaze_komodo/strategy_framework/strategy_manager.py
from typing import List, Dict, Any
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class StrategyManager:
    """
    Manages the loading, initialization, and execution of trading strategies.
    """
    def __init__(self):
        self.strategies: List[BaseStrategy] = []
        logger.info("StrategyManager initialized.")

    def add_strategy(self, strategy: BaseStrategy):
        """Adds a strategy instance to the manager."""
        if not isinstance(strategy, BaseStrategy):
            logger.error("Attempted to add an invalid strategy object.")
            raise ValueError("Strategy must be an instance of BaseStrategy.")
        
        self.strategies.append(strategy)
        logger.info(f"Strategy '{strategy.name}' for {strategy.symbol} ({strategy.timeframe}) added to StrategyManager.")

    def remove_strategy(self, strategy_name: str, symbol: str, timeframe: str):
        """Removes a strategy by its name, symbol, and timeframe."""
        initial_count = len(self.strategies)
        self.strategies = [
            s for s in self.strategies 
            if not (s.name == strategy_name and s.symbol == symbol and s.timeframe == timeframe)
        ]
        if len(self.strategies) < initial_count:
            logger.info(f"Strategy '{strategy_name}' for {symbol} ({timeframe}) removed.")
        else:
            logger.warning(f"Strategy '{strategy_name}' for {symbol} ({timeframe}) not found for removal.")


    def load_strategies_from_config(self, config: Dict[str, Any]):
        """
        Loads strategies based on a configuration dictionary.
        This is a placeholder for a more dynamic loading mechanism.
        For now, strategies are added manually or via specific calls.
        """
        # Example:
        # for strategy_config in config.get('strategies', []):
        #     strategy_class = resolve_strategy_class(strategy_config['name']) # Utility to get class from name
        #     params = strategy_config.get('params', {})
        #     symbol = strategy_config.get('symbol')
        #     timeframe = strategy_config.get('timeframe')
        #     if strategy_class and symbol and timeframe:
        #         self.add_strategy(strategy_class(symbol, timeframe, params))
        logger.warning("load_strategies_from_config is a placeholder and not fully implemented.")
        pass

    def on_bar_data_all(self, bar_data: BarData) -> Dict[str, SignalType]:
        """
        Distributes new bar data to all relevant strategies and collects signals.
        A strategy is relevant if the bar_data.symbol and bar_data.timeframe match.

        Returns:
            Dict[str, SignalType]: A dictionary where keys are strategy identifiers
                                   (e.g., "EWMACStrategy_BTC/USD_1h") and values are signals.
        """
        signals_from_strategies: Dict[str, SignalType] = {}
        for strategy in self.strategies:
            if strategy.symbol == bar_data.symbol and strategy.timeframe == bar_data.timeframe:
                signal = strategy.on_bar_data(bar_data)
                if signal: # Only record actual signals, not None or HOLD if not meaningful here
                    strategy_id = f"{strategy.name}_{strategy.symbol.replace('/', '')}_{strategy.timeframe}"
                    signals_from_strategies[strategy_id] = signal
                    logger.debug(f"Signal from {strategy_id}: {signal.name}")
        return signals_from_strategies

    def get_all_strategies(self) -> List[BaseStrategy]:
        return self.strategies

# Example Usage (Conceptual)
if __name__ == '__main__':
    from kamikaze_komodo.strategy_framework.strategies.ewmac import EWMACStrategy
    from kamikaze_komodo.config.settings import settings # Assuming settings are loaded

    if settings:
        manager = StrategyManager()
        
        # Create and add a strategy instance
        ewmac_params = {
            'short_window': settings.ewmac_short_window,
            'long_window': settings.ewmac_long_window
        }
        ewmac_btc_1h = EWMACStrategy(symbol="BTC/USD", timeframe="1h", params=ewmac_params)
        manager.add_strategy(ewmac_btc_1h)

        # Simulate receiving bar data
        # In a real system, this BarData would come from DataFetcher
        from datetime import datetime, timezone
        example_bar = BarData(
            timestamp=datetime.now(timezone.utc),
            open=40000, high=40500, low=39800, close=40200, volume=100,
            symbol="BTC/USD", timeframe="1h"
        )

        # To actually get a signal, the strategy needs historical data first.
        # This is a simplified call. `ewmac_btc_1h.update_data_history(bar)` would need to be called many times first.
        # For a single bar without history, it will likely return HOLD or an error if not enough data.
        # signals = manager.on_bar_data_all(example_bar)
        # logger.info(f"Signals received: {signals}")
        logger.info("StrategyManager example completed. For meaningful signals, strategies need historical data.")
    else:
        logger.error("Settings not loaded, cannot run StrategyManager example.")
</code>

kamikaze_komodo/strategy_framework/__init__.py:
<code>
# kamikaze_komodo/strategy_framework/__init__.py
# This file makes the 'strategy_framework' directory a Python package.
</code>

kamikaze_komodo/strategy_framework/strategies/ewmac.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/ewmac.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class EWMACStrategy(BaseStrategy):
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        # Access params using lowercase keys, as configparser.items() typically lowercases them
        self.short_window = int(self.params.get('shortwindow', 12)) # Changed to lowercase
        self.long_window = int(self.params.get('longwindow', 26))   # Changed to lowercase
        self.atr_period = int(self.params.get('atr_period', 14)) # Already lowercase in config by convention, but ensure consistency

        # Sentiment thresholds are often passed directly into params by main.py, preserving case.
        # If they were meant to be read from strategy's own config section, they'd also be lowercase.
        self.sentiment_filter_long_threshold = self.params.get('sentiment_filter_long_threshold')
        if isinstance(self.sentiment_filter_long_threshold, str):
            try:
                self.sentiment_filter_long_threshold = None if self.sentiment_filter_long_threshold.lower() == 'none' else float(self.sentiment_filter_long_threshold)
            except ValueError:
                logger.warning(f"Could not parse sentiment_filter_long_threshold '{self.params.get('sentiment_filter_long_threshold')}' to float. Defaulting to None.")
                self.sentiment_filter_long_threshold = None


        self.sentiment_filter_short_threshold = self.params.get('sentiment_filter_short_threshold')
        if isinstance(self.sentiment_filter_short_threshold, str):
            try:
                self.sentiment_filter_short_threshold = None if self.sentiment_filter_short_threshold.lower() == 'none' else float(self.sentiment_filter_short_threshold)
            except ValueError:
                logger.warning(f"Could not parse sentiment_filter_short_threshold '{self.params.get('sentiment_filter_short_threshold')}' to float. Defaulting to None.")
                self.sentiment_filter_short_threshold = None


        if not isinstance(self.short_window, int) or not isinstance(self.long_window, int):
            raise ValueError("EWMACStrategy: 'short_window' and 'long_window' must be integers.")
        if self.short_window >= self.long_window:
            raise ValueError("EWMACStrategy: 'short_window' must be less than 'long_window'.")

        logger.info(
            f"Initialized EWMACStrategy for {symbol} ({timeframe}) "
            f"with Short EMA: {self.short_window}, Long EMA: {self.long_window}, ATR Period: {self.atr_period}. "
            f"Sentiment Long Thresh: {self.sentiment_filter_long_threshold}, Short Thresh: {self.sentiment_filter_short_threshold}"
        )
        self.current_position_status: Optional[SignalType] = None

    # ... rest of the EWMACStrategy class remains the same as previous correct version ...
    # Ensure _calculate_indicators and on_bar_data methods are complete

    def _calculate_indicators(self, data_df: pd.DataFrame) -> pd.DataFrame: 
        if data_df.empty: return data_df
        df = data_df.copy()

        if 'close' not in df.columns or len(df) < self.long_window :
            return df 

        df[f'ema_short'] = ta.ema(df['close'], length=self.short_window)
        df[f'ema_long'] = ta.ema(df['close'], length=self.long_window)

        if all(col in df.columns for col in ['high', 'low', 'close']):
            if len(df) >= self.atr_period:
                df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)
            else:
                df['atr'] = pd.NA 
        else:
            df['atr'] = pd.NA 
        return df

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        if data.empty or len(data) < self.long_window:
            logger.warning(f"Not enough historical data for EWMAC signals. Need {self.long_window}, got {len(data)}.")
            return pd.Series(index=data.index, dtype='object')

        df_processed = self._calculate_indicators(data)
        if 'ema_short' not in df_processed.columns or 'ema_long' not in df_processed.columns:
            return pd.Series(index=data.index, dtype='object') 

        if sentiment_series is not None and not sentiment_series.empty:
            df_processed = df_processed.join(sentiment_series.rename('sentiment_score'), how='left')
            df_processed['sentiment_score'] = df_processed['sentiment_score'].fillna(0.0) 
        elif 'sentiment_score' not in df_processed.columns: 
            df_processed['sentiment_score'] = 0.0

        signals = pd.Series(index=df_processed.index, dtype='object').fillna(SignalType.HOLD)
        current_pos_state = None 

        for i in range(1, len(df_processed)): 
            prev_short_ema = df_processed['ema_short'].iloc[i-1]
            curr_short_ema = df_processed['ema_short'].iloc[i]
            prev_long_ema = df_processed['ema_long'].iloc[i-1]
            curr_long_ema = df_processed['ema_long'].iloc[i]
            
            current_sentiment = df_processed['sentiment_score'].iloc[i]

            if pd.isna(curr_short_ema) or pd.isna(curr_long_ema) or pd.isna(prev_short_ema) or pd.isna(prev_long_ema):
                signals.iloc[i] = SignalType.HOLD 
                continue

            is_golden_cross = curr_short_ema > curr_long_ema and prev_short_ema <= prev_long_ema
            is_death_cross = curr_short_ema < curr_long_ema and prev_short_ema >= prev_long_ema

            if current_pos_state != SignalType.LONG: 
                if is_golden_cross:
                    if self.sentiment_filter_long_threshold is None or current_sentiment >= self.sentiment_filter_long_threshold:
                        signals.iloc[i] = SignalType.LONG
                        current_pos_state = SignalType.LONG
                    else:
                        signals.iloc[i] = SignalType.HOLD 
                else: 
                    signals.iloc[i] = SignalType.HOLD
            
            elif current_pos_state == SignalType.LONG: 
                if is_death_cross:
                    signals.iloc[i] = SignalType.CLOSE_LONG
                    current_pos_state = None 
                else: 
                    signals.iloc[i] = SignalType.HOLD 

        logger.info(f"Generated EWMAC signals (vectorized). Longs: {signals.eq(SignalType.LONG).sum()}, CloseLongs: {signals.eq(SignalType.CLOSE_LONG).sum()}")
        return signals

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None) -> Optional[SignalType]:
        self.update_data_history(bar_data) 

        if len(self.data_history) < self.long_window + 1: 
            return SignalType.HOLD

        df_with_indicators = self._calculate_indicators(self.data_history)
        
        if 'atr' in df_with_indicators.columns and pd.notna(df_with_indicators['atr'].iloc[-1]):
            bar_data.atr = df_with_indicators['atr'].iloc[-1]
        
        current_sentiment = sentiment_score if sentiment_score is not None else bar_data.sentiment_score
        if current_sentiment is None: current_sentiment = 0.0 


        if df_with_indicators.empty or 'ema_short' not in df_with_indicators.columns or \
           'ema_long' not in df_with_indicators.columns or len(df_with_indicators) < 2:
            return SignalType.HOLD

        latest_ema_short = df_with_indicators['ema_short'].iloc[-1]
        prev_ema_short = df_with_indicators['ema_short'].iloc[-2]
        latest_ema_long = df_with_indicators['ema_long'].iloc[-1]
        prev_ema_long = df_with_indicators['ema_long'].iloc[-2]

        if pd.isna(latest_ema_short) or pd.isna(prev_ema_short) or \
           pd.isna(latest_ema_long) or pd.isna(prev_ema_long):
            return SignalType.HOLD

        signal_to_return = SignalType.HOLD

        is_golden_cross = latest_ema_short > latest_ema_long and prev_ema_short <= prev_ema_long
        if is_golden_cross and self.current_position_status != SignalType.LONG:
            if self.sentiment_filter_long_threshold is None or current_sentiment >= self.sentiment_filter_long_threshold:
                signal_to_return = SignalType.LONG
                self.current_position_status = SignalType.LONG
                logger.info(f"{bar_data.timestamp} - EWMAC LONG for {self.symbol}. Sent: {current_sentiment:.2f}. EMA_S: {latest_ema_short:.2f}, EMA_L: {latest_ema_long:.2f}. ATR: {bar_data.atr if bar_data.atr else 'N/A'}")
            else:
                logger.info(f"{bar_data.timestamp} - EWMAC LONG for {self.symbol} SUPPRESSED by sentiment ({current_sentiment:.2f} < {self.sentiment_filter_long_threshold}).")
                signal_to_return = SignalType.HOLD

        is_death_cross = latest_ema_short < latest_ema_long and prev_ema_short >= prev_ema_long
        if is_death_cross and self.current_position_status == SignalType.LONG:
            signal_to_return = SignalType.CLOSE_LONG
            self.current_position_status = None 
            logger.info(f"{bar_data.timestamp} - EWMAC CLOSE_LONG for {self.symbol}. EMA_S: {latest_ema_short:.2f}, EMA_L: {latest_ema_long:.2f}")
        
        elif self.current_position_status == SignalType.LONG and signal_to_return == SignalType.HOLD : 
            pass 

        elif self.current_position_status is None and signal_to_return == SignalType.HOLD:
            pass 

        return signal_to_return
</code>

kamikaze_komodo/strategy_framework/strategies/__init__.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/__init__.py
# This file makes the 'strategies' directory a Python package.
</code>

