kamikaze_komodo/app_logger.py:
<code>
# kamikaze_komodo/app_logger.py
import logging
import os
from logging.handlers import RotatingFileHandler

LOG_DIR = "logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

log_file_path = os.path.join(LOG_DIR, "kamikaze_komodo.log")

# Configure logging
logger = logging.getLogger("KamikazeKomodo")
logger.setLevel(logging.DEBUG)  # Set to DEBUG to capture all levels of messages

# Create handlers
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)  # Console logs info and above

file_handler = RotatingFileHandler(
    log_file_path, maxBytes=10*1024*1024, backupCount=5  # 10MB per file, 5 backups
)
file_handler.setLevel(logging.DEBUG)  # File logs debug and above

# Create formatters and add it to handlers
log_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s.%(funcName)s - %(message)s')
console_handler.setFormatter(log_format)
file_handler.setFormatter(log_format)

# Add handlers to the logger
if not logger.handlers:
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

def get_logger(module_name: str) -> logging.Logger:
    """
    Returns a logger instance for a specific module.
    """
    return logging.getLogger(f"KamikazeKomodo.{module_name}")
</code>

kamikaze_komodo/main.py:
<code>
# FILE: kamikaze_komodo/main.py
# kamikaze_komodo/main.py
import asyncio
import os
import pandas as pd
from datetime import datetime, timedelta, timezone
from typing import List, Optional, Dict, Any

from kamikaze_komodo.app_logger import get_logger, logger as root_logger
from kamikaze_komodo.config.settings import settings

# Phase 1 & 2 imports
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.exchange_interaction.exchange_api import ExchangeAPI
# from kamikaze_komodo.strategy_framework.strategy_manager import StrategyManager # Not directly used in demos
from kamikaze_komodo.strategy_framework.strategies.ewmac import EWMACStrategy
from kamikaze_komodo.backtesting_engine.engine import BacktestingEngine
from kamikaze_komodo.backtesting_engine.performance_analyzer import PerformanceAnalyzer
from kamikaze_komodo.core.models import BarData, NewsArticle
from kamikaze_komodo.core.enums import SignalType # For strategy checking

# Phase 3 imports
from kamikaze_komodo.risk_control_module.position_sizer import BasePositionSizer, FixedFractionalPositionSizer, ATRBasedPositionSizer, PairTradingPositionSizer, OptimalFPositionSizer, MLConfidencePositionSizer
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager, PercentageStopManager, ATRStopManager
from kamikaze_komodo.risk_control_module.volatility_band_stop_manager import VolatilityBandStopManager


# Phase 4 imports
from kamikaze_komodo.ai_news_analysis_agent_module.news_scraper import NewsScraper
from kamikaze_komodo.ai_news_analysis_agent_module.sentiment_analyzer import SentimentAnalyzer
# from kamikaze_komodo.ai_news_analysis_agent_module.browser_agent import BrowserAgent # Optional advanced feature
# from kamikaze_komodo.ai_news_analysis_agent_module.notification_listener import NotificationListener, dummy_notification_callback # Placeholder

# Phase 5 imports
from kamikaze_komodo.strategy_framework.strategies.ehlers_instantaneous_trendline import EhlersInstantaneousTrendlineStrategy
from kamikaze_komodo.strategy_framework.strategies.ml_forecaster_strategy import MLForecasterStrategy
from kamikaze_komodo.ml_models.training_pipelines.lightgbm_pipeline import LightGBMTrainingPipeline
# from kamikaze_komodo.ml_models.inference_pipelines.lightgbm_inference import LightGBMInference # Used by MLForecasterStrategy internally

# Phase 6 imports
from kamikaze_komodo.strategy_framework.strategies.bollinger_band_breakout_strategy import BollingerBandBreakoutStrategy
from kamikaze_komodo.strategy_framework.strategies.pair_trading_strategy import PairTradingStrategy
from kamikaze_komodo.ml_models.training_pipelines.xgboost_classifier_pipeline import XGBoostClassifierTrainingPipeline
from kamikaze_komodo.ml_models.training_pipelines.kmeans_regime_pipeline import KMeansRegimeTrainingPipeline

# Orchestration import (for Phase 6 demo of scheduler startup)
from kamikaze_komodo.orchestration.scheduler import TaskScheduler, example_data_polling_task, example_news_scraping_task


logger = get_logger(__name__)

async def run_phase1_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 1: Core Infrastructure & Data")
    if not settings: root_logger.critical("Settings failed to load."); return

    db_manager = DatabaseManager()
    data_fetcher = DataFetcher() # Uses exchange_id from settings
    exchange_api = ExchangeAPI() # Uses exchange_id from settings

    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    start_period = datetime.now(timezone.utc) - timedelta(days=settings.historical_data_days if settings.historical_data_days > 0 else 30)
    end_period = datetime.now(timezone.utc)

    historical_data = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_period, end_period)
    if historical_data:
        logger.info(f"Fetched {len(historical_data)} bars for {symbol} ({timeframe}).")
        db_manager.store_bar_data(historical_data)
        retrieved_data = db_manager.retrieve_bar_data(symbol, timeframe, start_date=start_period, end_date=end_period)
        logger.info(f"Retrieved {len(retrieved_data)} bars from DB for {symbol} ({timeframe}). First bar: {retrieved_data[0].timestamp if retrieved_data else 'N/A'}")
    else:
        logger.warning(f"No historical data fetched for {symbol}.")

    balance = await exchange_api.fetch_balance()
    if balance:
        # Updated currency extraction logic
        base_currency = symbol.split('/')[0].split(':')[0] if '/' in symbol or ':' in symbol else "N/A"
        quote_currency = symbol.split('/')[1] if '/' in symbol else (symbol.split(':')[1] if ':' in symbol else "USD")

        logger.info(f"Fetched balance. Free {base_currency}: {balance.get(base_currency, {}).get('free', 'N/A')}, Free {quote_currency}: {balance.get(quote_currency, {}).get('free', 'N/A')}")
    else:
        logger.warning("Could not fetch account balance.")

    await data_fetcher.close()
    await exchange_api.close()
    db_manager.close()
    root_logger.info("Phase 1 Demonstration completed.")

async def run_phase2_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 2: Basic Strategy & Backtesting")
    if not settings: root_logger.critical("Settings failed to load."); return

    db_manager = DatabaseManager()
    data_fetcher = DataFetcher()
    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    hist_days = settings.historical_data_days if settings.historical_data_days > 0 else 365
    start_date = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_date = datetime.now(timezone.utc)

    ewmac_params_for_min_bars = settings.get_strategy_params("EWMAC_Strategy")
    min_bars_needed = int(ewmac_params_for_min_bars.get('longwindow', settings.ewmac_long_window)) + 5 # Buffer

    historical_bars: List[BarData] = db_manager.retrieve_bar_data(symbol, timeframe, start_date, end_date)

    if not historical_bars or len(historical_bars) < min_bars_needed:
        logger.info(f"Insufficient data in DB for {symbol} ({len(historical_bars)}/{min_bars_needed}). Fetching fresh for {hist_days} days...")
        historical_bars = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_date, end_date)
        if historical_bars: db_manager.store_bar_data(historical_bars)
        else: logger.error(f"Failed to fetch sufficient data for {symbol}. Backtest cannot proceed."); await data_fetcher.close(); db_manager.close(); return

    await data_fetcher.close() # Close after fetching
    db_manager.close() # Close after retrieving/storing

    if not historical_bars or len(historical_bars) < min_bars_needed:
        logger.error(f"Still not enough historical data for {symbol} ({len(historical_bars)} bars vs {min_bars_needed} needed) after fetch attempt. Backtest aborted."); return

    logger.info(f"Using {len(historical_bars)} bars for backtesting {symbol}.")
    data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
    data_df.set_index('timestamp', inplace=True)

    if data_df.empty or len(data_df) < min_bars_needed:
        logger.error(f"DataFrame conversion error or insufficient points for EWMAC ({len(data_df)})."); return

    ewmac_params = settings.get_strategy_params("EWMAC_Strategy")
    ewmac_strategy = EWMACStrategy(symbol=symbol, timeframe=timeframe, params=ewmac_params)

    initial_capital = 10000.00
    commission_bps = settings.commission_bps
    backtest_engine = BacktestingEngine(data_feed_df=data_df, strategy=ewmac_strategy, initial_capital=initial_capital, commission_bps=commission_bps)

    logger.info(f"Running basic backtest for EWMAC on {symbol}...")
    trades_log, final_portfolio, equity_curve_df = backtest_engine.run() # Capture equity_curve

    if trades_log:
        logger.info(f"Backtest (Phase 2) completed. Generated {len(trades_log)} trades.")
        performance_analyzer = PerformanceAnalyzer(
            trades=trades_log,
            initial_capital=initial_capital,
            final_capital=final_portfolio['final_portfolio_value'],
            equity_curve_df=equity_curve_df # Pass equity curve here
            )
        metrics = performance_analyzer.calculate_metrics()
        performance_analyzer.print_summary(metrics)
    else:
        logger.info(f"Backtest (Phase 2) completed. No trades executed. Final portfolio value: ${final_portfolio['final_portfolio_value']:.2f}")
    root_logger.info("Phase 2 Demonstration completed.")


async def run_phase3_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 3: Risk Management Integration")
    if not settings: root_logger.critical("Settings failed to load."); return

    db_manager = DatabaseManager()
    data_fetcher = DataFetcher()
    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    hist_days = settings.historical_data_days if settings.historical_data_days > 0 else 365
    start_date = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_date = datetime.now(timezone.utc)

    ewmac_params_for_min_bars_phase3 = settings.get_strategy_params("EWMAC_Strategy")
    atr_period_from_params = int(ewmac_params_for_min_bars_phase3.get("atr_period", settings.ewmac_atr_period))
    long_window_from_params = int(ewmac_params_for_min_bars_phase3.get("longwindow", settings.ewmac_long_window))
    min_bars_needed = max(long_window_from_params, atr_period_from_params) + 5

    historical_bars = db_manager.retrieve_bar_data(symbol, timeframe, start_date, end_date)
    if not historical_bars or len(historical_bars) < min_bars_needed:
        logger.info(f"Fetching fresh data for Phase 3 backtest (need ~{min_bars_needed} bars)...")
        historical_bars = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_date, end_date)
        if historical_bars: db_manager.store_bar_data(historical_bars)
        else: logger.error(f"Failed to fetch data for {symbol}. Aborting."); await data_fetcher.close(); db_manager.close(); return

    await data_fetcher.close()
    db_manager.close()

    if not historical_bars or len(historical_bars) < min_bars_needed:
        logger.error(f"Not enough data ({len(historical_bars)} bars vs {min_bars_needed} needed) for Phase 3 backtest. Aborting."); return

    data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
    data_df.set_index('timestamp', inplace=True)

    ewmac_params = settings.get_strategy_params("EWMAC_Strategy")
    ewmac_strategy = EWMACStrategy(symbol=symbol, timeframe=timeframe, params=ewmac_params)

    position_sizer: Optional[Any] = None
    if settings.position_sizer_type.lower() == 'atrbased':
        position_sizer = ATRBasedPositionSizer(
            risk_per_trade_fraction=settings.atr_based_risk_per_trade_fraction,
            atr_multiple_for_stop=settings.atr_based_atr_multiple_for_stop
        )
    else:
        position_sizer = FixedFractionalPositionSizer(fraction=settings.fixed_fractional_allocation_fraction)
    logger.info(f"Using Position Sizer: {position_sizer.__class__.__name__}")

    stop_manager: Optional[Any] = None
    if settings.stop_manager_type.lower() == 'atrbased':
        stop_manager = ATRStopManager(atr_multiple=settings.atr_stop_atr_multiple)
    else:
        stop_manager = PercentageStopManager(
            stop_loss_pct=settings.percentage_stop_loss_pct,
            take_profit_pct=settings.percentage_stop_take_profit_pct
        )
    logger.info(f"Using Stop Manager: {stop_manager.__class__.__name__}")

    initial_capital = 10000.00
    commission_bps = settings.commission_bps
    backtest_engine = BacktestingEngine(
        data_feed_df=data_df,
        strategy=ewmac_strategy,
        initial_capital=initial_capital,
        commission_bps=commission_bps,
        position_sizer=position_sizer,
        stop_manager=stop_manager
    )
    logger.info(f"Running Phase 3 backtest (EWMAC with Risk Management) on {symbol}...")
    trades_log, final_portfolio, equity_curve_df = backtest_engine.run()

    if trades_log:
        logger.info(f"Backtest (Phase 3) completed. Generated {len(trades_log)} trades.")
        performance_analyzer = PerformanceAnalyzer(
            trades=trades_log,
            initial_capital=initial_capital,
            final_capital=final_portfolio['final_portfolio_value'],
            equity_curve_df=equity_curve_df
            )
        metrics = performance_analyzer.calculate_metrics()
        performance_analyzer.print_summary(metrics)
    else:
        logger.info(f"Backtest (Phase 3) completed. No trades executed. Final portfolio: ${final_portfolio['final_portfolio_value']:.2f}")
    root_logger.info("Phase 3 Demonstration completed.")


async def run_phase4_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 4: AI News & Sentiment Integration")
    if not settings: root_logger.critical("Settings failed to load."); return

    if settings.news_scraper_enable:
        logger.info("--- Running News Scraper (Phase 4 Demo) ---")
        try:
            news_scraper = NewsScraper()
            scraped_articles = await news_scraper.scrape_all(limit_per_source=5, since_hours_rss=48)
            if scraped_articles:
                logger.info(f"Scraped {len(scraped_articles)} unique articles.")

                if settings.enable_sentiment_analysis and settings.sentiment_llm_provider == "VertexAI" and settings.vertex_ai_project_id:
                    logger.info("--- Running Sentiment Analyzer on Scraped Articles (Phase 4 Demo) ---")
                    try:
                        sentiment_analyzer = SentimentAnalyzer()
                        analyzed_articles: List[NewsArticle] = []
                        for article_to_analyze in scraped_articles[:5]: # Only analyze first 5 for demo
                            logger.info(f"Analyzing sentiment for: {article_to_analyze.title[:50]}...")
                            updated_article = await sentiment_analyzer.get_sentiment_for_article(article_to_analyze)
                            analyzed_articles.append(updated_article)
                            if updated_article.sentiment_label:
                                logger.info(f"  -> Sentiment: {updated_article.sentiment_label} ({updated_article.sentiment_score:.2f})")

                        if analyzed_articles:
                            db_manager_news = DatabaseManager()
                            db_manager_news.store_news_articles(analyzed_articles)
                            logger.info(f"Stored {len(analyzed_articles)} analyzed articles in the database.")
                            db_manager_news.close()
                    except Exception as e_sa_live:
                        logger.error(f"Error during live sentiment analysis demo: {e_sa_live}", exc_info=True)
                elif not settings.enable_sentiment_analysis:
                    logger.info("Live sentiment analysis is disabled in settings.")
                elif settings.sentiment_llm_provider != "VertexAI" or not settings.vertex_ai_project_id:
                    logger.warning("Live sentiment analysis configured for non-VertexAI or VertexAI not fully configured (Project ID missing). Skipping live analysis demo.")
            else:
                logger.info("No articles scraped in this run.")
        except Exception as e_scrape_live:
            logger.error(f"Error during live news scraping demo: {e_scrape_live}", exc_info=True)
    else:
        logger.info("News Scraper is disabled in settings. Skipping live scraping/analysis for Phase 4 demo.")


    logger.info("--- Proceeding to backtest with sentiment integration using simulated data ---")
    db_manager_bt = DatabaseManager()
    data_fetcher_bt = DataFetcher()
    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    hist_days = settings.historical_data_days if settings.historical_data_days > 0 else 90
    start_date_bt = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_date_bt = datetime.now(timezone.utc)

    ewmac_params_phase4 = settings.get_strategy_params("EWMAC_Strategy")
    atr_period_phase4 = int(ewmac_params_phase4.get("atr_period", settings.ewmac_atr_period))
    long_window_phase4 = int(ewmac_params_phase4.get("longwindow", settings.ewmac_long_window))
    min_bars_needed_phase4 = max(long_window_phase4, atr_period_phase4) + 5

    historical_bars_bt = db_manager_bt.retrieve_bar_data(symbol, timeframe, start_date_bt, end_date_bt)
    if not historical_bars_bt or len(historical_bars_bt) < min_bars_needed_phase4:
        logger.info(f"Fetching fresh data for Phase 4 backtest (need ~{min_bars_needed_phase4} bars)...")
        historical_bars_bt = await data_fetcher_bt.fetch_historical_data_for_period(symbol, timeframe, start_date_bt, end_date_bt)
        if historical_bars_bt: db_manager_bt.store_bar_data(historical_bars_bt)
        else: logger.error(f"Failed to fetch data for {symbol}. Aborting Phase 4 backtest."); await data_fetcher_bt.close(); db_manager_bt.close(); return

    await data_fetcher_bt.close()
    db_manager_bt.close()

    if not historical_bars_bt or len(historical_bars_bt) < min_bars_needed_phase4:
        logger.error(f"Not enough data ({len(historical_bars_bt)} bars) for Phase 4 backtest. Aborting."); return

    data_df_bt = pd.DataFrame([bar.model_dump() for bar in historical_bars_bt])
    data_df_bt['timestamp'] = pd.to_datetime(data_df_bt['timestamp'])
    data_df_bt.set_index('timestamp', inplace=True)

    sentiment_df: Optional[pd.DataFrame] = None
    if settings.simulated_sentiment_data_path and settings.enable_sentiment_analysis:
        sentiment_csv_path = settings.simulated_sentiment_data_path
        logger.info(f"Attempting to load simulated sentiment data from: {sentiment_csv_path}")
        if os.path.exists(sentiment_csv_path):
            try:
                sentiment_df = pd.read_csv(sentiment_csv_path, parse_dates=['timestamp'], index_col='timestamp')
                if sentiment_df.empty:
                    logger.warning(f"Simulated sentiment data file is empty: {sentiment_csv_path}"); sentiment_df = None
                else:
                    if sentiment_df.index.tz is None: sentiment_df.index = sentiment_df.index.tz_localize('UTC')
                    else: sentiment_df.index = sentiment_df.index.tz_convert('UTC')
                    if 'sentiment_score' not in sentiment_df.columns:
                        logger.error(f"'sentiment_score' column not found in {sentiment_csv_path}. Sentiment will not be used."); sentiment_df = None
                    else:
                        logger.info(f"Successfully loaded {len(sentiment_df)} simulated sentiment entries from: {sentiment_csv_path}")
            except Exception as e_csv:
                logger.error(f"Error loading simulated sentiment data from {sentiment_csv_path}: {e_csv}. Proceeding without sentiment.", exc_info=True); sentiment_df = None
        else:
            logger.warning(f"Simulated sentiment data file NOT FOUND at: {sentiment_csv_path}. Proceeding without external sentiment.")
    elif not settings.enable_sentiment_analysis:
        logger.info("Sentiment analysis is disabled in settings. Backtest will not use external sentiment data.")
    else:
        logger.info("No simulated sentiment data path provided. Proceeding without external sentiment.")

    ewmac_strategy_sentiment = EWMACStrategy(symbol=symbol, timeframe=timeframe, params=ewmac_params_phase4)

    position_sizer_sent: Optional[Any] = None
    if settings.position_sizer_type.lower() == 'atrbased':
        position_sizer_sent = ATRBasedPositionSizer(settings.atr_based_risk_per_trade_fraction, settings.atr_based_atr_multiple_for_stop)
    else:
        position_sizer_sent = FixedFractionalPositionSizer(settings.fixed_fractional_allocation_fraction)

    stop_manager_sent: Optional[Any] = None
    if settings.stop_manager_type.lower() == 'atrbased':
        stop_manager_sent = ATRStopManager(settings.atr_stop_atr_multiple)
    else:
        stop_manager_sent = PercentageStopManager(settings.percentage_stop_loss_pct, settings.percentage_stop_take_profit_pct)

    initial_capital = 10000.00
    commission_bps = settings.commission_bps

    backtest_engine_sentiment = BacktestingEngine(
        data_feed_df=data_df_bt,
        strategy=ewmac_strategy_sentiment,
        initial_capital=initial_capital,
        commission_bps=commission_bps,
        position_sizer=position_sizer_sent,
        stop_manager=stop_manager_sent,
        sentiment_data_df=sentiment_df
    )
    logger.info(f"Running Phase 4 backtest (EWMAC with Sentiment & Risk Mgmt) on {symbol}...")
    trades_log, final_portfolio, equity_curve_df = backtest_engine_sentiment.run()

    if trades_log:
        logger.info(f"Backtest (Phase 4) completed. Generated {len(trades_log)} trades.")
        performance_analyzer = PerformanceAnalyzer(
            trades=trades_log,
            initial_capital=initial_capital,
            final_capital=final_portfolio['final_portfolio_value'],
            equity_curve_df=equity_curve_df
            )
        metrics = performance_analyzer.calculate_metrics()
        performance_analyzer.print_summary(metrics)
    else:
        logger.info(f"Backtest (Phase 4) completed. No trades executed. Final portfolio: ${final_portfolio['final_portfolio_value']:.2f}")
    root_logger.info("Phase 4 Demonstration completed.")


async def run_phase5_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 5: Advanced Strategies & ML Models")
    if not settings: root_logger.critical("Settings failed to load."); return

    # --- 1. Optional: Train ML Model (LightGBM Example) ---
    run_ml_training = True # Set to True to run training as part of demo. Ensures model is available.
    if run_ml_training:
        if not settings.config.has_section("LightGBM_Forecaster"):
            logger.error("Config section [LightGBM_Forecaster] not found. Skipping ML model training.")
        else:
            logger.info("--- Running LightGBM Training Pipeline (Phase 5 Demo) ---")
            try:
                training_pipeline = LightGBMTrainingPipeline(
                    symbol=settings.default_symbol,
                    timeframe=settings.default_timeframe
                )
                await training_pipeline.run_training()
                logger.info("LightGBM Training Pipeline completed for Phase 5 demo.")
            except Exception as e_train:
                logger.error(f"Error during LightGBM training demo: {e_train}", exc_info=True)
    else:
        logger.info("Skipping ML model training for this Phase 5 demonstration run. Ensure model exists if MLForecasterStrategy is used.")

    # --- 2. Backtest with Ehlers Instantaneous Trendline Strategy ---
    logger.info("--- Starting Backtest with Ehlers Instantaneous Trendline Strategy (Phase 5 Demo) ---")
    db_manager_ehlers = DatabaseManager()
    data_fetcher_ehlers = DataFetcher()
    symbol_ehlers = settings.default_symbol
    timeframe_ehlers = settings.default_timeframe
    hist_days_ehlers = settings.historical_data_days if settings.historical_data_days > 0 else 180 # Shorter for demo
    start_date_ehlers = datetime.now(timezone.utc) - timedelta(days=hist_days_ehlers)
    end_date_ehlers = datetime.now(timezone.utc)

    ehlers_params = settings.get_strategy_params("EhlersInstantaneousTrendline_Strategy")
    min_bars_needed_ehlers = int(ehlers_params.get("it_lag_trigger", 1)) + int(ehlers_params.get("atr_period", 14)) + 20 # Increased buffer

    historical_bars_ehlers = db_manager_ehlers.retrieve_bar_data(symbol_ehlers, timeframe_ehlers, start_date_ehlers, end_date_ehlers)
    if not historical_bars_ehlers or len(historical_bars_ehlers) < min_bars_needed_ehlers:
        logger.info(f"Fetching data for Ehlers IT backtest (need ~{min_bars_needed_ehlers} bars)...")
        historical_bars_ehlers = await data_fetcher_ehlers.fetch_historical_data_for_period(symbol_ehlers, timeframe_ehlers, start_date_ehlers, end_date_ehlers)
        if historical_bars_ehlers: db_manager_ehlers.store_bar_data(historical_bars_ehlers)

    await data_fetcher_ehlers.close()
    db_manager_ehlers.close()

    if not historical_bars_ehlers or len(historical_bars_ehlers) < min_bars_needed_ehlers:
        logger.error(f"Not enough data for Ehlers IT backtest ({len(historical_bars_ehlers)} bars). Aborting."); return

    data_df_ehlers = pd.DataFrame([bar.model_dump() for bar in historical_bars_ehlers])
    data_df_ehlers['timestamp'] = pd.to_datetime(data_df_ehlers['timestamp'])
    data_df_ehlers.set_index('timestamp', inplace=True)

    ehlers_strategy = EhlersInstantaneousTrendlineStrategy(symbol=symbol_ehlers, timeframe=timeframe_ehlers, params=ehlers_params)

    position_sizer_ehlers: Optional[Any] = None
    if settings.position_sizer_type.lower() == 'atrbased':
        position_sizer_ehlers = ATRBasedPositionSizer(settings.atr_based_risk_per_trade_fraction, settings.atr_based_atr_multiple_for_stop)
    else: position_sizer_ehlers = FixedFractionalPositionSizer(settings.fixed_fractional_allocation_fraction)

    stop_manager_ehlers: Optional[Any] = None
    if settings.stop_manager_type.lower() == 'atrbased':
        stop_manager_ehlers = ATRStopManager(settings.atr_stop_atr_multiple)
    else: stop_manager_ehlers = PercentageStopManager(settings.percentage_stop_loss_pct, settings.percentage_stop_take_profit_pct)

    initial_capital = 10000.00
    commission_bps = settings.commission_bps

    backtest_engine_ehlers = BacktestingEngine(
        data_feed_df=data_df_ehlers, strategy=ehlers_strategy, initial_capital=initial_capital,
        commission_bps=commission_bps, position_sizer=position_sizer_ehlers, stop_manager=stop_manager_ehlers
    )
    logger.info(f"Running Phase 5 backtest (Ehlers IT Strategy) on {symbol_ehlers}...")
    trades_log_ehlers, final_portfolio_ehlers, equity_curve_df_ehlers = backtest_engine_ehlers.run()

    if trades_log_ehlers:
        logger.info(f"Ehlers IT Backtest completed. Generated {len(trades_log_ehlers)} trades.")
        pa_ehlers = PerformanceAnalyzer(
            trades_log_ehlers,
            initial_capital,
            final_portfolio_ehlers['final_portfolio_value'],
            equity_curve_df_ehlers
            )
        metrics_ehlers = pa_ehlers.calculate_metrics()
        pa_ehlers.print_summary(metrics_ehlers)
    else:
        logger.info(f"Ehlers IT Backtest completed. No trades executed. Final portfolio: ${final_portfolio_ehlers['final_portfolio_value']:.2f}")


    # --- 3. Backtest with MLForecasterStrategy ---
    logger.info("--- Starting Backtest with MLForecasterStrategy (Phase 5 Demo) ---")

    db_manager_ml = DatabaseManager()
    data_fetcher_ml = DataFetcher()
    symbol_ml = settings.default_symbol
    timeframe_ml = settings.default_timeframe
    hist_days_ml = settings.historical_data_days if settings.historical_data_days > 0 else 180
    start_date_ml = datetime.now(timezone.utc) - timedelta(days=hist_days_ml)
    end_date_ml = datetime.now(timezone.utc)

    ml_strategy_params = settings.get_strategy_params("MLForecaster_Strategy")
    min_bars_for_pred = int(ml_strategy_params.get('min_bars_for_prediction', 50))
    min_bars_needed_ml = max(min_bars_for_pred, int(ml_strategy_params.get("atr_period", 14))) + 20 # Increased buffer

    historical_bars_ml = db_manager_ml.retrieve_bar_data(symbol_ml, timeframe_ml, start_date_ml, end_date_ml)
    if not historical_bars_ml or len(historical_bars_ml) < min_bars_needed_ml:
        logger.info(f"Fetching data for ML Strategy backtest (need ~{min_bars_needed_ml} bars)...")
        historical_bars_ml = await data_fetcher_ml.fetch_historical_data_for_period(symbol_ml, timeframe_ml, start_date_ml, end_date_ml)
        if historical_bars_ml: db_manager_ml.store_bar_data(historical_bars_ml)

    await data_fetcher_ml.close()
    db_manager_ml.close()

    if not historical_bars_ml or len(historical_bars_ml) < min_bars_needed_ml:
        logger.error(f"Not enough data for ML Strategy backtest ({len(historical_bars_ml)} bars). Aborting."); return

    data_df_ml = pd.DataFrame([bar.model_dump() for bar in historical_bars_ml])
    data_df_ml['timestamp'] = pd.to_datetime(data_df_ml['timestamp'])
    data_df_ml.set_index('timestamp', inplace=True)

    try:
        # Ensure MLForecaster_Strategy is configured for LightGBM for this demo part
        ml_strategy_params_lgbm = settings.get_strategy_params("MLForecaster_Strategy")
        ml_strategy_params_lgbm['forecastertype'] = 'lightgbm' # Explicitly set for demo
        ml_strategy_params_lgbm['modelconfigsection'] = 'LightGBM_Forecaster'

        ml_strategy = MLForecasterStrategy(symbol=symbol_ml, timeframe=timeframe_ml, params=ml_strategy_params_lgbm)
        if ml_strategy.inference_engine is None or ml_strategy.inference_engine.forecaster.model is None:
            logger.error(f"MLForecasterStrategy for {symbol_ml} ({timeframe_ml}) failed to load its LightGBM model. Cannot proceed with backtest.")
            root_logger.info("Phase 5 MLForecasterStrategy Backtest SKIPPED due to model loading issue.")
            return # Skip this part of the demo if model isn't loaded
    except Exception as e_strat_init:
        logger.error(f"Failed to initialize MLForecasterStrategy (LightGBM): {e_strat_init}", exc_info=True)
        root_logger.info("Phase 5 MLForecasterStrategy Backtest SKIPPED due to strategy initialization error.")
        return

    position_sizer_ml: Optional[Any] = None
    if settings.position_sizer_type.lower() == 'atrbased':
        position_sizer_ml = ATRBasedPositionSizer(settings.atr_based_risk_per_trade_fraction, settings.atr_based_atr_multiple_for_stop)
    else: position_sizer_ml = FixedFractionalPositionSizer(settings.fixed_fractional_allocation_fraction)

    stop_manager_ml: Optional[Any] = None
    if settings.stop_manager_type.lower() == 'atrbased':
        stop_manager_ml = ATRStopManager(settings.atr_stop_atr_multiple)
    else: stop_manager_ml = PercentageStopManager(settings.percentage_stop_loss_pct, settings.percentage_stop_take_profit_pct)

    sentiment_df_ml: Optional[pd.DataFrame] = None # Re-use sentiment logic from Phase 4 demo if needed

    backtest_engine_ml = BacktestingEngine(
        data_feed_df=data_df_ml, strategy=ml_strategy, initial_capital=initial_capital,
        commission_bps=commission_bps, position_sizer=position_sizer_ml, stop_manager=stop_manager_ml,
        sentiment_data_df=sentiment_df_ml
    )
    logger.info(f"Running Phase 5 backtest (MLForecasterStrategy - LightGBM) on {symbol_ml}...")
    trades_log_ml, final_portfolio_ml, equity_curve_df_ml = backtest_engine_ml.run()

    if trades_log_ml:
        logger.info(f"MLForecasterStrategy (LightGBM) Backtest completed. Generated {len(trades_log_ml)} trades.")
        pa_ml = PerformanceAnalyzer(
            trades_log_ml,
            initial_capital,
            final_portfolio_ml['final_portfolio_value'],
            equity_curve_df_ml
            )
        metrics_ml = pa_ml.calculate_metrics()
        pa_ml.print_summary(metrics_ml)
    else:
        logger.info(f"MLForecasterStrategy (LightGBM) Backtest completed. No trades executed. Final portfolio: ${final_portfolio_ml['final_portfolio_value']:.2f}")

    root_logger.info("Phase 5 Demonstration completed.")


async def run_phase6_demonstration():
    root_logger.info("Starting Kamikaze Komodo - Phase 6: Advanced Trading & Backtesting Demo")
    if not settings: root_logger.critical("Settings failed to load."); return

    initial_capital = 10000.00
    commission_bps = float(settings.config.get('Trading', 'CommissionBPS', fallback=0.0))
    slippage_bps = float(settings.config.get('Trading', 'SlippageBPS', fallback=0.0))
    funding_rate_annualized = float(settings.config.get('Trading', 'FundingRateAnnualized', fallback=0.0))

    db_manager = DatabaseManager()
    data_fetcher = DataFetcher()
    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    hist_days = settings.historical_data_days if settings.historical_data_days > 0 else 365
    start_date = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_date = datetime.now(timezone.utc)

    # --- Load Simulated Sentiment Data ---
    sentiment_df: Optional[pd.DataFrame] = None
    if settings.simulated_sentiment_data_path and settings.enable_sentiment_analysis:
        sentiment_csv_path = settings.simulated_sentiment_data_path
        logger.info(f"Attempting to load simulated sentiment data from: {sentiment_csv_path}")
        if os.path.exists(sentiment_csv_path):
            try:
                sentiment_df = pd.read_csv(sentiment_csv_path, parse_dates=['timestamp'], index_col='timestamp')
                if sentiment_df.empty:
                    logger.warning(f"Simulated sentiment data file is empty: {sentiment_csv_path}")
                    sentiment_df = None
                else:
                    if sentiment_df.index.tz is None:
                        sentiment_df.index = sentiment_df.index.tz_localize('UTC')
                    else:
                        sentiment_df.index = sentiment_df.index.tz_convert('UTC')
                    if 'sentiment_score' not in sentiment_df.columns:
                        logger.error(f"'sentiment_score' column not found in {sentiment_csv_path}. Sentiment will not be used.")
                        sentiment_df = None
                    else:
                        logger.info(f"Successfully loaded {len(sentiment_df)} simulated sentiment entries.")
            except Exception as e_csv:
                logger.error(f"Error loading simulated sentiment data from {sentiment_csv_path}: {e_csv}. Proceeding without sentiment.", exc_info=True)
                sentiment_df = None
        else:
            logger.warning(f"Simulated sentiment data file NOT FOUND at: {sentiment_csv_path}. Proceeding without external sentiment.")
    elif not settings.enable_sentiment_analysis:
        logger.info("Sentiment analysis is disabled in settings. Backtest will not use external sentiment data.")
    else:
        logger.info("No simulated sentiment data path provided. Proceeding without external sentiment.")

    # Common Position Sizer and Stop Manager for these demos
    main_pos_sizer_params = settings.get_strategy_params("RiskManagement") # Get general risk params
    main_position_sizer: Optional[BasePositionSizer] = None
    if settings.position_sizer_type.lower() == 'atrbased':
        main_position_sizer = ATRBasedPositionSizer(params=main_pos_sizer_params)
    elif settings.position_sizer_type.lower() == 'optimalf':
        main_position_sizer = OptimalFPositionSizer(params=main_pos_sizer_params)
    else: # Default to FixedFractional
        main_position_sizer = FixedFractionalPositionSizer(params=main_pos_sizer_params)

    main_stop_manager_params = settings.get_strategy_params("RiskManagement") # Get general risk params
    main_stop_manager: Optional[BaseStopManager] = None
    if settings.stop_manager_type.lower() == 'atrbased':
        main_stop_manager = ATRStopManager(params=main_stop_manager_params)
    elif settings.stop_manager_type.lower() == 'volatilitybandstop':
        main_stop_manager = VolatilityBandStopManager(params=main_stop_manager_params)
    else: # Default to Percentage
        main_stop_manager = PercentageStopManager(params=main_stop_manager_params)


    # --- 1. Backtest EWMAC Strategy with Shorting Enabled ---
    logger.info("--- Starting Backtest with EWMAC Strategy (Shorting Enabled - Phase 6 Demo) ---")
    ewmac_params = settings.get_strategy_params("EWMAC_Strategy")
    ewmac_params['enableshorting'] = True
    ewmac_strategy_shorting = EWMACStrategy(symbol=symbol, timeframe=timeframe, params=ewmac_params)
    min_bars_ewmac = int(ewmac_params.get('longwindow', 26)) + 5

    historical_bars_ewmac = db_manager.retrieve_bar_data(symbol, timeframe, start_date, end_date)
    if not historical_bars_ewmac or len(historical_bars_ewmac) < min_bars_ewmac:
        logger.info(f"Fetching data for EWMAC (shorting) backtest (need ~{min_bars_ewmac} bars)...")
        historical_bars_ewmac = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_date, end_date)
        if historical_bars_ewmac: db_manager.store_bar_data(historical_bars_ewmac)

    if historical_bars_ewmac and len(historical_bars_ewmac) >= min_bars_ewmac:
        data_df_ewmac = pd.DataFrame([bar.model_dump() for bar in historical_bars_ewmac])
        data_df_ewmac['timestamp'] = pd.to_datetime(data_df_ewmac['timestamp'])
        data_df_ewmac.set_index('timestamp', inplace=True)

        backtest_engine_ewmac = BacktestingEngine(
            data_feed_df=data_df_ewmac, strategy=ewmac_strategy_shorting, initial_capital=initial_capital,
            commission_bps=commission_bps, slippage_bps=slippage_bps, funding_rate_annualized=funding_rate_annualized,
            position_sizer=main_position_sizer, stop_manager=main_stop_manager,
            sentiment_data_df=sentiment_df  # Pass sentiment data
        )
        logger.info(f"Running EWMAC (Shorting) backtest on {symbol}...")
        trades_ewmac, final_pf_ewmac, equity_ewmac = backtest_engine_ewmac.run()
        pa_ewmac = PerformanceAnalyzer(trades_ewmac, initial_capital, final_pf_ewmac['final_portfolio_value'], equity_ewmac,
                                         risk_free_rate_annual=float(settings.config.get('BacktestingPerformance', 'RiskFreeRateAnnual')),
                                         annualization_factor=int(settings.config.get('BacktestingPerformance', 'AnnualizationFactor')))
        metrics_ewmac = pa_ewmac.calculate_metrics()
        pa_ewmac.print_summary(metrics_ewmac)
    else:
        logger.error(f"Not enough data for EWMAC (shorting) backtest ({len(historical_bars_ewmac if historical_bars_ewmac else [])} bars). Skipping.")

    # --- 2. Backtest BollingerBandBreakoutStrategy ---
    logger.info("--- Starting Backtest with BollingerBandBreakoutStrategy (Phase 6 Demo) ---")
    bb_params = settings.get_strategy_params("BollingerBandBreakout_Strategy")
    bb_params['enableshorting'] = True
    bb_strategy = BollingerBandBreakoutStrategy(symbol=symbol, timeframe=timeframe, params=bb_params)
    min_bars_bb = int(bb_params.get('bb_period', 20)) + int(bb_params.get('volume_sma_period', 20)) + 5

    historical_bars_bb = db_manager.retrieve_bar_data(symbol, timeframe, start_date, end_date)
    if not historical_bars_bb or len(historical_bars_bb) < min_bars_bb:
        logger.info(f"Fetching data for Bollinger Band backtest (need ~{min_bars_bb} bars)...")
        historical_bars_bb = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_date, end_date)
        if historical_bars_bb: db_manager.store_bar_data(historical_bars_bb)

    if historical_bars_bb and len(historical_bars_bb) >= min_bars_bb:
        data_df_bb = pd.DataFrame([bar.model_dump() for bar in historical_bars_bb])
        data_df_bb['timestamp'] = pd.to_datetime(data_df_bb['timestamp'])
        data_df_bb.set_index('timestamp', inplace=True)

        backtest_engine_bb = BacktestingEngine(
            data_feed_df=data_df_bb, strategy=bb_strategy, initial_capital=initial_capital,
            commission_bps=commission_bps, slippage_bps=slippage_bps, funding_rate_annualized=funding_rate_annualized,
            position_sizer=main_position_sizer, stop_manager=main_stop_manager,
            sentiment_data_df=sentiment_df  # Pass sentiment data
        )
        logger.info(f"Running BollingerBandBreakout backtest on {symbol}...")
        trades_bb, final_pf_bb, equity_bb = backtest_engine_bb.run()
        pa_bb = PerformanceAnalyzer(trades_bb, initial_capital, final_pf_bb['final_portfolio_value'], equity_bb,
                                      risk_free_rate_annual=float(settings.config.get('BacktestingPerformance', 'RiskFreeRateAnnual')),
                                      annualization_factor=int(settings.config.get('BacktestingPerformance', 'AnnualizationFactor')))
        metrics_bb = pa_bb.calculate_metrics()
        pa_bb.print_summary(metrics_bb)
    else:
        logger.error(f"Not enough data for Bollinger Band backtest ({len(historical_bars_bb if historical_bars_bb else [])} bars). Skipping.")


    # --- 3. Backtest PairTradingStrategy ---
    logger.info("--- Starting Backtest with PairTradingStrategy (Phase 6 Demo) ---")
    pair_params = settings.get_strategy_params("PairTrading_Strategy")
    asset1_sym = pair_params.get('asset1_symbol', settings.default_symbol)
    asset2_sym = pair_params.get('asset2_symbol')

    if not asset2_sym:
        logger.error("Asset2_Symbol not configured for PairTrading_Strategy. Skipping demo.")
    else:
        # FIX: Refactored data fetching and caching for pair trading
        # 1. Try to get data from DB first
        min_bars_pair = int(pair_params.get('cointegration_lookback_days', 90)) * (24 // (int(timeframe[:-1]) if timeframe[:-1].isdigit() else 4)) # Approx bars needed
        hist_bars_asset1 = db_manager.retrieve_bar_data(asset1_sym, timeframe, start_date, end_date)
        hist_bars_asset2 = db_manager.retrieve_bar_data(asset2_sym, timeframe, start_date, end_date)

        # 2. If not enough data, fetch from exchange and store
        if not hist_bars_asset1 or not hist_bars_asset2 or len(hist_bars_asset1) < min_bars_pair or len(hist_bars_asset2) < min_bars_pair:
            logger.info(f"Fetching pair data for {asset1_sym}/{asset2_sym}...")
            fetched_bars1, fetched_bars2 = await data_fetcher.fetch_historical_data_for_pair(asset1_sym, asset2_sym, timeframe, start_date, end_date)
            if fetched_bars1:
                db_manager.store_bar_data(fetched_bars1)
                hist_bars_asset1 = fetched_bars1
            if fetched_bars2:
                db_manager.store_bar_data(fetched_bars2)
                hist_bars_asset2 = fetched_bars2

        # 3. Proceed with backtest if data is available
        if hist_bars_asset1 and hist_bars_asset2:
            df_asset1 = pd.DataFrame([b.model_dump() for b in hist_bars_asset1]).set_index(pd.to_datetime([b.timestamp for b in hist_bars_asset1]))
            df_asset2 = pd.DataFrame([b.model_dump() for b in hist_bars_asset2]).set_index(pd.to_datetime([b.timestamp for b in hist_bars_asset2]))
            
            # The strategy now needs the data passed to it for the cointegration test
            pair_strategy = PairTradingStrategy(symbol=asset1_sym, timeframe=timeframe, params=pair_params)
            await pair_strategy.initialize_strategy_data(historical_data_asset1=df_asset1, historical_data_asset2=df_asset2)

            if pair_strategy.is_cointegrated:
                pair_pos_sizer_params = settings.get_strategy_params("RiskManagement")
                pair_pos_sizer_params['pairtradingpositionsizer_dollarneutral'] = True
                pair_position_sizer = PairTradingPositionSizer(params=pair_pos_sizer_params)

                backtest_engine_pair = BacktestingEngine(
                    data_feed_df=df_asset1, strategy=pair_strategy, initial_capital=initial_capital,
                    commission_bps=commission_bps, slippage_bps=slippage_bps, funding_rate_annualized=funding_rate_annualized,
                    position_sizer=pair_position_sizer, stop_manager=main_stop_manager,
                    data_feed_df_pair_asset2=df_asset2
                )
                logger.info(f"Running PairTradingStrategy backtest on {asset1_sym}/{asset2_sym}...")
                trades_pair, final_pf_pair, equity_pair = backtest_engine_pair.run()
                
                pa_pair = PerformanceAnalyzer(trades_pair, initial_capital, final_pf_pair['final_portfolio_value'], equity_pair,
                                                  risk_free_rate_annual=float(settings.config.get('BacktestingPerformance', 'RiskFreeRateAnnual')),
                                                  annualization_factor=int(settings.config.get('BacktestingPerformance', 'AnnualizationFactor')))
                metrics_pair = pa_pair.calculate_metrics()
                pa_pair.print_summary(metrics_pair)
            else:
                logger.warning(f"Assets {asset1_sym} and {asset2_sym} are not cointegrated based on current settings. Skipping PairTradingStrategy backtest.")
        else:
            logger.error(f"Not enough data for one or both assets for PairTrading backtest after fetch attempt. Skipping.")

    # --- 4. Train and Backtest MLForecasterStrategy with XGBoostClassifier ---
    logger.info("--- Starting Training & Backtest for MLForecasterStrategy with XGBoost (Phase 6 Demo) ---")
    if settings.config.has_section("XGBoost_Classifier_Forecaster"):
        xgb_train_pipeline = XGBoostClassifierTrainingPipeline(symbol=symbol, timeframe=timeframe)
        await xgb_train_pipeline.run_training()

        ml_xgb_params = settings.get_strategy_params("MLForecaster_Strategy")
        ml_xgb_params['forecastertype'] = 'xgboost_classifier'
        ml_xgb_params['modelconfigsection'] = 'XGBoost_Classifier_Forecaster'
        ml_xgb_strategy = MLForecasterStrategy(symbol=symbol, timeframe=timeframe, params=ml_xgb_params)

        if ml_xgb_strategy.inference_engine and ml_xgb_strategy.inference_engine.forecaster.model:
            historical_bars_ml_xgb = db_manager.retrieve_bar_data(symbol, timeframe, start_date, end_date)
            if not historical_bars_ml_xgb or len(historical_bars_ml_xgb) < int(ml_xgb_params.get('min_bars_for_prediction',50)) + 5:
                logger.info(f"Fetching fresh data for ML (XGBoost) backtest...")
                historical_bars_ml_xgb = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_date, end_date)
                if historical_bars_ml_xgb: db_manager.store_bar_data(historical_bars_ml_xgb) # Store if fetched

            if historical_bars_ml_xgb and len(historical_bars_ml_xgb) >= int(ml_xgb_params.get('min_bars_for_prediction',50)) +5 :
                data_df_ml_xgb = pd.DataFrame([bar.model_dump() for bar in historical_bars_ml_xgb])
                data_df_ml_xgb['timestamp'] = pd.to_datetime(data_df_ml_xgb['timestamp'])
                data_df_ml_xgb.set_index('timestamp', inplace=True)

                backtest_engine_ml_xgb = BacktestingEngine(
                    data_feed_df=data_df_ml_xgb, strategy=ml_xgb_strategy, initial_capital=initial_capital,
                    commission_bps=commission_bps, slippage_bps=slippage_bps, funding_rate_annualized=funding_rate_annualized,
                    position_sizer=main_position_sizer, stop_manager=main_stop_manager,
                    sentiment_data_df=sentiment_df # Pass sentiment data
                )
                logger.info(f"Running MLForecasterStrategy (XGBoost) backtest on {symbol}...")
                trades_ml_xgb, final_pf_ml_xgb, equity_ml_xgb = backtest_engine_ml_xgb.run()
                pa_ml_xgb = PerformanceAnalyzer(trades_ml_xgb, initial_capital, final_pf_ml_xgb['final_portfolio_value'], equity_ml_xgb,
                                                  risk_free_rate_annual=float(settings.config.get('BacktestingPerformance', 'RiskFreeRateAnnual')),
                                                  annualization_factor=int(settings.config.get('BacktestingPerformance', 'AnnualizationFactor')))
                metrics_ml_xgb = pa_ml_xgb.calculate_metrics()
                pa_ml_xgb.print_summary(metrics_ml_xgb)
            else:
                logger.error("Not enough data for ML (XGBoost) backtest. Skipping.")
        else:
            logger.error("MLForecasterStrategy (XGBoost) model not loaded after training. Skipping backtest.")
    else:
        logger.warning("XGBoost_Classifier_Forecaster section not in config. Skipping XGBoost demo.")


    # --- K-Means Regime Model Training (separate from strategy for this demo) ---
    if settings.config.has_section("KMeans_Regime_Model"):
        logger.info("--- Running K-Means Regime Model Training (Phase 6 Demo) ---")
        kmeans_pipeline = KMeansRegimeTrainingPipeline(symbol=symbol, timeframe=timeframe)
        await kmeans_pipeline.run_training() # Trains and saves the model
        logger.info("K-Means Regime Model training pipeline finished.")
    else:
        logger.info("KMeans_Regime_Model section not in config. Skipping K-Means training demo.")


    await data_fetcher.close()
    db_manager.close()
    root_logger.info("Phase 6 Demonstration completed.")


async def main():
    root_logger.info("Kamikaze Komodo Program Starting...")
    if not settings:
        root_logger.critical("Settings failed to load. Application cannot start.")
        return

    # --- Scheduler Integration (Example Startup) ---
    scheduler_manager = TaskScheduler() # Initialize scheduler
    # scheduler_manager.add_job(example_data_polling_task, 'interval', minutes=1, id='data_poll_main_app')
    # scheduler_manager.add_job(example_news_scraping_task, 'interval', minutes=5, id='news_scrape_main_app')
    try:
        scheduler_manager.start()
        logger.info("APScheduler started for application tasks (example).")
    except Exception as e_sched_start:
        logger.error(f"Failed to start APScheduler in main: {e_sched_start}")


    # --- Select phases to run ---
    # await run_phase1_demonstration()
    # await run_phase2_demonstration()
    # await run_phase3_demonstration()
    # await run_phase4_demonstration()
    # await run_phase5_demonstration()
    await run_phase6_demonstration()

    # Shutdown scheduler when main tasks are done
    if scheduler_manager.scheduler.running:
        scheduler_manager.shutdown(wait=False)

    root_logger.info("Kamikaze Komodo Program Finished.")

if __name__ == "__main__":
    try:
        if settings and settings.sentiment_llm_provider == "VertexAI" and not settings.vertex_ai_project_id:
            root_logger.warning("Vertex AI is selected, but Project ID is not set in config.ini. AI features may fail.")
            root_logger.warning("Please set your GCP Project ID in kamikaze_komodo/config/config.ini ([VertexAI] -> ProjectID)")
            root_logger.warning("And ensure GOOGLE_APPLICATION_CREDENTIALS environment variable is set.")

        if not os.path.exists("logs"): os.makedirs("logs")

        model_dir_from_config = "ml_models/trained_models"
        if settings and settings.config.has_section("LightGBM_Forecaster") and settings.config.get("LightGBM_Forecaster", "ModelSavePath", fallback=None):
             model_dir_from_config = settings.config.get("LightGBM_Forecaster", "ModelSavePath")
        
        asyncio.run(main())
    except KeyboardInterrupt:
        root_logger.info("Kamikaze Komodo program terminated by user.")
    except Exception as e:
        root_logger.critical(f"Critical error in main execution: {e}", exc_info=True)
</code>

kamikaze_komodo/__init__.py:
<code>
# kamikaze_komodo/__init__.py
# This file makes the 'root' directory a Python package.
</code>

kamikaze_komodo/ai_news_analysis_agent_module/browser_agent.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/browser_agent.py
import asyncio
from typing import Optional, Any, Dict
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Import global settings

logger = get_logger(__name__)

class BrowserAgent:
    """
    Uses browser-use with a configured LLM (Vertex AI's Gemini)
    to perform targeted market research.
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. BrowserAgent cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.llm_provider = settings.browser_agent_llm_provider
        self.llm: Optional[Any] = None # To be initialized
        self.agent_is_ready = False
        self.browser_use_agent_class: Optional[type] = None # For storing the imported class

        try:
            self._initialize_llm()
            # Dynamically import browser_use only if LLM init is successful
            # global BrowserUseAgent # Make it global for the method if loaded
            from browser_use import Agent as BrowserUseAgentLib
            self.browser_use_agent_class = BrowserUseAgentLib # Store the class
            self.agent_is_ready = True
            logger.info("browser-use Agent component dynamically imported.")
        except ImportError:
            logger.error("browser-use library not found. Please install it: pip install browser-use")
            logger.error("Also run: playwright install --with-deps chromium")
        except Exception as e:
            logger.error(f"BrowserAgent initialization failed: {e}", exc_info=True)


    def _initialize_llm(self):
        logger.info(f"Initializing LLM for BrowserAgent with provider: {self.llm_provider}")
        if self.llm_provider == "VertexAI":
            if not settings.vertex_ai_project_id or not settings.vertex_ai_location:
                logger.error("Vertex AI project ID or location is not configured. BrowserAgent LLM will not work.")
                raise ValueError("Vertex AI project ID or location missing for BrowserAgent.")
            try:
                from langchain_google_vertexai import ChatVertexAI
                self.llm = ChatVertexAI(
                    project=settings.vertex_ai_project_id,
                    location=settings.vertex_ai_location,
                    model_name=settings.vertex_ai_browser_agent_model_name, # Use specific model for browser agent
                    temperature=0.2, # Slightly higher temp for research/summarization
                    # max_output_tokens=2048, # Optional
                )
                logger.info(f"BrowserAgent initialized with Vertex AI model: {settings.vertex_ai_browser_agent_model_name}")
                logger.info("Ensure Google Cloud credentials (GOOGLE_APPLICATION_CREDENTIALS) are set for Vertex AI.")
            except ImportError:
                logger.error("langchain-google-vertexai not found. pip install langchain-google-vertexai")
                raise
            except Exception as e:
                logger.error(f"Failed to initialize Vertex AI LLM for BrowserAgent: {e}", exc_info=True)
                raise
        else:
            logger.error(f"Unsupported LLM provider for BrowserAgent: {self.llm_provider}")
            raise ValueError(f"Unsupported LLM provider for BrowserAgent: {self.llm_provider}")

        if self.llm is None:
            raise ValueError("BrowserAgent LLM initialization failed.")


    async def conduct_research(self, research_task: str, max_steps: int = 20, use_vision: bool = False) -> Optional[Dict[str, Any]]:
        """
        Conducts research based on the given task using browser-use.
        """
        if not self.agent_is_ready or self.llm is None or self.browser_use_agent_class is None:
            logger.error("BrowserAgent or its LLM not initialized, or browser_use class not loaded. Cannot conduct research.")
            return None

        logger.info(f"Starting browser-use research task: '{research_task[:100]}...' (Max steps: {max_steps})")
        try:
            agent = self.browser_use_agent_class( # Use the stored class
                llm=self.llm,
                task=research_task,
                use_vision=use_vision,
                # verbose=True, # Can be noisy
            )
            result = await agent.run(max_steps=max_steps)
            logger.info("Browser-use research task completed.")

            final_output_text = ""
            if isinstance(result, dict):
                logger.debug(f"Browser agent raw result dictionary keys: {result.keys()}")
                final_output_text = result.get('output', result.get('answer', str(result)))
            elif isinstance(result, str):
                final_output_text = result
            else:
                final_output_text = str(result)
                logger.warning(f"Unexpected result type from browser-use agent: {type(result)}. Content: {final_output_text[:500]}")

            logger.info(f"Browser Agent Output: {final_output_text[:500]}...")
            return {"output": final_output_text, "full_result": result}

        except Exception as e:
            logger.error(f"An error occurred while running the browser-use agent: {e}", exc_info=True)
            logger.error("Possible issues: LLM server, network, task complexity, or website automation challenges.")
            return None

async def main_browser_agent_example():
    if not settings or not settings.browser_agent_enable:
        logger.info("BrowserAgent is not enabled in settings or settings not loaded.")
        return
    if not settings.vertex_ai_project_id and settings.browser_agent_llm_provider == "VertexAI":
        logger.error("Vertex AI Project ID not set for BrowserAgent. Ensure it's in config.ini and GOOGLE_APPLICATION_CREDENTIALS env var is set.")
        return

    try:
        browser_agent = BrowserAgent()
        if not browser_agent.agent_is_ready:
            logger.error("Browser agent could not be initialized. Exiting example.")
            return
    except Exception as e:
        logger.error(f"Failed to create BrowserAgent: {e}")
        return

    task = (
        "What is the latest news regarding Solana's network performance and any major ecosystem developments in June 2025? "
        "Visit 2 reputable crypto news websites (e.g., Decrypt, Cointelegraph). "
        "Summarize findings and list article URLs. Limit Browse to 4 steps per site."
    )
    # Get max_steps from settings
    max_steps_for_research = settings.browser_agent_max_steps if settings.browser_agent_max_steps > 0 else 25

    result_data = await browser_agent.conduct_research(task, max_steps=max_steps_for_research)

    if result_data and "output" in result_data:
        logger.info("\n--- Browser Agent Research Result ---")
        print(result_data["output"])
    elif result_data:
        logger.info("\n--- Browser Agent Raw Research Result ---")
        print(result_data)
    else:
        logger.warning("Browser agent research did not produce a result or failed.")

if __name__ == "__main__":
    # Ensure GOOGLE_APPLICATION_CREDENTIALS is set for Vertex AI
    # Ensure Playwright browsers are installed: playwright install --with-deps chromium
    # This example is best run if BrowserAgent_Enable is true in config.
    if settings and settings.browser_agent_enable:
        asyncio.run(main_browser_agent_example())
    else:
        print("BrowserAgent is not enabled in settings, or settings failed to load. Skipping example.")
        print("To run, set BrowserAgent_Enable = True in config.ini and ensure Vertex AI is configured.")
</code>

kamikaze_komodo/ai_news_analysis_agent_module/news_scraper.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/news_scraper.py
import asyncio
import feedparser
import newspaper # type: ignore
import httpx
from typing import List, Optional, Dict, Any
# from bs4 import BeautifulSoup # Keep for potential future direct HTML parsing needs
from kamikaze_komodo.core.models import NewsArticle
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone, timedelta
from kamikaze_komodo.config.settings import settings # Import global settings

logger = get_logger(__name__)

class NewsScraper:
    """
    Scrapes news from specified sources (RSS feeds, websites).
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. NewsScraper cannot be initialized.")
            raise ValueError("Settings not loaded.")

        scraper_config = settings.get_news_scraper_config()
        self.rss_feeds: List[Dict[str, str]] = scraper_config.get("rss_feeds", [])
        self.websites_to_scrape: List[Dict[str, str]] = scraper_config.get("websites", []) # For Newspaper3k or custom BS4

        if not self.rss_feeds and not self.websites_to_scrape:
            logger.warning("NewsScraper initialized, but no RSS feeds or websites are configured in settings.")
        else:
            logger.info(f"NewsScraper initialized. RSS feeds: {len(self.rss_feeds)}, Websites: {len(self.websites_to_scrape)}")
            if self.rss_feeds:
                logger.debug(f"Configured RSS Feeds: {[feed['name'] for feed in self.rss_feeds]}")

    async def _fetch_url_content(self, url: str) -> Optional[str]:
        try:
            async with httpx.AsyncClient(timeout=20.0, follow_redirects=True) as client:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 KamikazeKomodoBot/1.0'
                }
                response = await client.get(url, headers=headers)
                response.raise_for_status()
                return response.text
        except httpx.RequestError as e:
            logger.error(f"HTTP request error fetching URL {url}: {e}")
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP status error fetching URL {url}: {e.response.status_code} - {e.response.text[:200]}")
        except Exception as e_gen:
            logger.error(f"Generic error fetching URL {url}: {e_gen}", exc_info=True)
        return None

    async def scrape_rss_feed(self, feed_name: str, feed_url: str, limit: int = 15) -> List[NewsArticle]:
        articles: List[NewsArticle] = []
        logger.info(f"Scraping RSS feed: {feed_name} from {feed_url}")

        feed_content = await self._fetch_url_content(feed_url)
        if not feed_content:
            logger.warning(f"Could not fetch content for RSS feed {feed_name} ({feed_url}). Skipping.")
            return articles

        try:
            loop = asyncio.get_event_loop()
            # feedparser is synchronous
            parsed_feed = await loop.run_in_executor(None, feedparser.parse, feed_content)

            if parsed_feed.bozo:
                logger.warning(f"Error parsing RSS feed {feed_name} ({feed_url}): {parsed_feed.bozo_exception}")

            if not parsed_feed.entries:
                logger.info(f"No entries found in RSS feed: {feed_name} ({feed_url}).")
                return articles

            for entry in parsed_feed.entries[:limit]:
                title = entry.get("title")
                link = entry.get("link")
                if not title or not link:
                    logger.debug(f"Skipping entry with missing title or link in {feed_name}: {entry.get('id', 'N/A')}")
                    continue

                published_time_struct = entry.get("published_parsed")
                updated_time_struct = entry.get("updated_parsed")

                pub_date: Optional[datetime] = None
                time_struct_to_use = published_time_struct or updated_time_struct

                if time_struct_to_use:
                    try:
                        pub_date = datetime(*time_struct_to_use[:6], tzinfo=timezone.utc)
                    except Exception as e_date:
                        logger.warning(f"Could not parse date for article '{title}' from {feed_name}: {time_struct_to_use}, error: {e_date}")

                # Fallback if date parsing fails or not present
                if pub_date is None:
                    pub_date = datetime.now(timezone.utc) # Use retrieval time as a last resort
                    logger.debug(f"Using current time as publication date for '{title}' from {feed_name} due to missing/unparseable date.")


                article_id = link # Use URL as a unique ID

                content_summary = entry.get("summary") or entry.get("description")

                # Attempt to extract related symbols from title or summary (basic)
                related_symbols = []
                text_for_symbols = (title + " " + (content_summary if content_summary else "")).lower()
                # This is very basic; a proper NER would be better
                common_crypto_symbols = {"btc", "bitcoin", "eth", "ethereum", "sol", "solana", "xrp", "ada", "cardano", "doge", "shib"}
                for sym in common_crypto_symbols:
                    if sym in text_for_symbols:
                        related_symbols.append(sym.upper())

                articles.append(NewsArticle(
                    id=article_id,
                    url=link,
                    title=title,
                    publication_date=pub_date,
                    retrieval_date=datetime.now(timezone.utc),
                    source=feed_name,
                    content=None, # Full content fetch can be added here or later by newspaper3k
                    summary=content_summary,
                    related_symbols=list(set(related_symbols)) # Unique symbols
                ))
            logger.info(f"Found {len(articles)} articles from RSS feed: {feed_name}")
        except Exception as e:
            logger.error(f"Failed to process RSS feed {feed_name} ({feed_url}): {e}", exc_info=True)
        return articles

    async def scrape_website_with_newspaper(self, site_name: str, site_url: str, limit_articles: int = 5) -> List[NewsArticle]:
        """Scrapes a website using Newspaper3k. Be mindful of terms of service."""
        articles_data: List[NewsArticle] = []
        logger.info(f"Scraping website: {site_name} ({site_url}) with Newspaper3k (limit: {limit_articles})")

        # Newspaper3k config
        config_np = newspaper.Config()
        config_np.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 KamikazeKomodoBot/1.0'
        config_np.request_timeout = 15
        config_np.memoize_articles = False # Disable caching for fresh data
        config_np.fetch_images = False # Don't need images
        config_np.verbose = False # newspaper's own verbosity

        try:
            loop = asyncio.get_event_loop()
            paper = await loop.run_in_executor(None, newspaper.build, site_url, config_np)

            count = 0
            for article_raw in paper.articles:
                if count >= limit_articles:
                    break
                try:
                    # Download and parse article content
                    await loop.run_in_executor(None, article_raw.download)
                    if not article_raw.is_downloaded:
                        logger.warning(f"Failed to download article: {article_raw.url} from {site_name}")
                        continue
                    await loop.run_in_executor(None, article_raw.parse)

                    title = article_raw.title
                    url = article_raw.url
                    if not title or not url:
                        logger.debug(f"Skipping article with no title/url from {site_name}")
                        continue

                    content = article_raw.text
                    summary_np = article_raw.summary # newspaper3k summary

                    pub_date_dt = article_raw.publish_date
                    if pub_date_dt and pub_date_dt.tzinfo is None:
                        pub_date_dt = pub_date_dt.replace(tzinfo=timezone.utc) # Assume UTC if naive, or local if known
                    elif pub_date_dt is None:
                        pub_date_dt = datetime.now(timezone.utc) # Fallback

                    related_symbols_np = []
                    text_for_symbols_np = (title + " " + (summary_np if summary_np else "") + " " + (content if content else "")).lower()
                    common_crypto_symbols_np = {"btc", "bitcoin", "eth", "ethereum", "sol", "solana", "xrp", "ada", "cardano", "doge", "shib"}
                    for sym_np in common_crypto_symbols_np:
                        if sym_np in text_for_symbols_np:
                            related_symbols_np.append(sym_np.upper())

                    articles_data.append(NewsArticle(
                        id=url, url=url, title=title,
                        publication_date=pub_date_dt,
                        retrieval_date=datetime.now(timezone.utc),
                        source=site_name,
                        content=content if content else None,
                        summary=summary_np if summary_np else None,
                        related_symbols=list(set(related_symbols_np))
                    ))
                    count += 1
                    logger.debug(f"Successfully scraped: {url} from {site_name}")
                except Exception as e_article:
                    logger.warning(f"Error processing article {article_raw.url} from {site_name} with Newspaper3k: {e_article}", exc_info=True)

            logger.info(f"Scraped {len(articles_data)} articles from {site_name} using Newspaper3k.")
        except Exception as e:
            logger.error(f"Failed to scrape website {site_name} ({site_url}) with Newspaper3k: {e}", exc_info=True)
        return articles_data

    async def scrape_all(self, limit_per_source: int = 10, since_hours_rss: Optional[int] = 24) -> List[NewsArticle]:
        """
        Scrapes all configured RSS feeds and websites.
        For RSS, optionally filters articles published within `since_hours_rss`.
        """
        all_articles: List[NewsArticle] = []

        # Scrape RSS Feeds
        rss_tasks = []
        if self.rss_feeds:
            for feed_info in self.rss_feeds:
                rss_tasks.append(self.scrape_rss_feed(feed_info['name'], feed_info['url'], limit=limit_per_source))

            rss_results_list = await asyncio.gather(*rss_tasks, return_exceptions=True)
            for result in rss_results_list:
                if isinstance(result, list):
                    all_articles.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"RSS scraping task failed: {result}", exc_info=True) # Log exception details
        else:
            logger.info("No RSS feeds configured to scrape.")

        # Filter RSS articles by publication date if since_hours_rss is provided
        if since_hours_rss is not None:
            cutoff_date = datetime.now(timezone.utc) - timedelta(hours=since_hours_rss)
            filtered_articles = []
            for article in all_articles:
                if article.publication_date and article.publication_date >= cutoff_date:
                    filtered_articles.append(article)
                elif not article.publication_date: # If no pub date, include it (conservative)
                    filtered_articles.append(article)
            count_removed = len(all_articles) - len(filtered_articles)
            if count_removed > 0:
                logger.info(f"Filtered out {count_removed} RSS articles older than {since_hours_rss} hours.")
            all_articles = filtered_articles

        # Scrape Websites (e.g., using Newspaper3k) - typically gets latest, less date control
        website_tasks = []
        if self.websites_to_scrape:
            for site_info in self.websites_to_scrape:
                website_tasks.append(self.scrape_website_with_newspaper(site_info['name'], site_info['url'], limit_articles=limit_per_source))

            website_results_list = await asyncio.gather(*website_tasks, return_exceptions=True)
            for result in website_results_list:
                if isinstance(result, list):
                    all_articles.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"Website scraping task failed: {result}", exc_info=True)
        else:
            logger.info("No direct websites configured to scrape with Newspaper3k.")

        # Deduplicate articles by URL (ID)
        unique_articles_dict: Dict[str, NewsArticle] = {}
        for article in all_articles:
            if article.id not in unique_articles_dict:
                unique_articles_dict[article.id] = article
            else: # If duplicate, prefer the one with more content or later retrieval
                existing_article = unique_articles_dict[article.id]
                if (article.content and not existing_article.content) or \
                   (article.retrieval_date > existing_article.retrieval_date):
                    unique_articles_dict[article.id] = article

        unique_articles_list = sorted(list(unique_articles_dict.values()), key=lambda x: x.publication_date or x.retrieval_date, reverse=True)

        logger.info(f"Total unique articles scraped from all sources: {len(unique_articles_list)}")
        return unique_articles_list

async def main_scraper_example():
    if not settings or not settings.news_scraper_enable:
        logger.info("NewsScraper is not enabled in settings or settings not loaded.")
        return

    scraper = NewsScraper()

    # Scrape all configured sources, limiting to 5 articles per source,
    # and only RSS articles from the last 48 hours
    all_scraped_articles = await scraper.scrape_all(limit_per_source=5, since_hours_rss=48)

    logger.info(f"--- All Scraped Articles ({len(all_scraped_articles)}) ---")
    if not all_scraped_articles:
        logger.info("No articles were scraped.")
        return

    for i, article in enumerate(all_scraped_articles[:10]): # Log details for first 10
        logger.info(f"{i+1}. Source: {article.source}, Title: {article.title}")
        logger.info(f"    URL: {article.url}")
        logger.info(f"    Date: {article.publication_date}, Retrieved: {article.retrieval_date}")
        logger.info(f"    Symbols: {article.related_symbols}")
        if article.summary:
            logger.info(f"    Summary: {article.summary[:150]}...")
        # if article.content: # Content can be very long
            # logger.info(f" Content Preview: {article.content[:100]}...")

    # Example: Store articles in DB
    if all_scraped_articles:
        from kamikaze_komodo.data_handling.database_manager import DatabaseManager
        db_manager = DatabaseManager()
        db_manager.store_news_articles(all_scraped_articles)
        logger.info(f"Stored {len(all_scraped_articles)} articles in the database.")

        # Retrieve and show some from DB
        retrieved = db_manager.retrieve_news_articles(limit=5)
        logger.info(f"--- Retrieved {len(retrieved)} articles from DB ---")
        for art_db in retrieved:
            logger.info(f"DB: {art_db.title} (Source: {art_db.source}, Date: {art_db.publication_date})")
        db_manager.close()

if __name__ == "__main__":
    if settings and settings.news_scraper_enable:
        asyncio.run(main_scraper_example())
    else:
        print("NewsScraper is not enabled in settings, or settings failed to load. Skipping example.")
</code>

kamikaze_komodo/ai_news_analysis_agent_module/notification_listener.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/notification_listener.py
from kamikaze_komodo.app_logger import get_logger
# from jeepney import DBusAddress, new_method_call # If fully implementing
# from jeepney.io.asyncio import open_dbus_connection # If fully implementing
import asyncio
from typing import Callable, Awaitable, Dict, Any, Optional

logger = get_logger(__name__)

class NotificationListener:
    """
    Listens for desktop notifications using D-Bus (via Jeepney).
    Basic implementation for Phase 4. Full D-Bus interaction is complex and OS-dependent.
    """
    def __init__(self, callback_on_notification: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None):
        """
        Args:
            callback_on_notification: An async function to call when a notification is received.
                                      It should accept a dictionary with notification details.
        """
        self.callback_on_notification = callback_on_notification
        self._running = False
        logger.info("NotificationListener initialized.")
        if self.callback_on_notification is None:
            logger.warning("No callback provided for NotificationListener. It will not perform any actions on notifications.")
        logger.warning("Full D-Bus notification listening with Jeepney is a complex, OS-dependent feature and is currently a placeholder.")
        logger.warning("To enable, ensure Jeepney is installed and D-Bus is correctly configured on your Linux desktop.")


    async def start_listening(self):
        """
        Starts listening for D-Bus notifications. Placeholder for Jeepney implementation.
        """
        self._running = True
        logger.info("Notification listener started (simulated/placeholder).")
        if self.callback_on_notification is None:
            logger.error("NotificationListener started but no callback is set. It will be idle.")
            # We can simply return or let the placeholder loop run idly.

        # --- Conceptual Jeepney Implementation (Highly OS/Setup Dependent) ---
        # try:
        #     from jeepney import DBusAddress, new_method_call
        #     from jeepney.io.asyncio import open_dbus_connection
        #     logger.info("Attempting to connect to D-Bus for notifications...")
        #     conn = await open_dbus_connection()
        #     logger.info("Connected to D-Bus. Setting up match rule for org.freedesktop.Notifications.Notify.")
        #
        #     # Interface to call AddMatch on (DBUS daemon itself)
        #     dbus_daemon_addr = DBusAddress('org.freedesktop.DBus', '/org/freedesktop/DBus', 'org.freedesktop.DBus')
        #     match_rule = "type='signal',interface='org.freedesktop.Notifications',member='Notify'"
        #     add_match_msg = new_method_call(dbus_daemon_addr, 'AddMatch', 's', (match_rule,))
        #     await conn.send_and_get_reply(add_match_msg) # No reply expected or needed for AddMatch usually
        #     logger.info(f"Match rule added: {match_rule}")
        #
        #     while self._running:
        #         try:
        #             msg_received = await asyncio.wait_for(conn.receive(), timeout=1.0) # Add timeout
        #             if msg_received and msg_received.member == 'Notify' and msg_received.interface == 'org.freedesktop.Notifications':
        #                 parsed_notification = self.parse_notification_data(msg_received.body)
        #                 if self.callback_on_notification and parsed_notification:
        #                     logger.info(f"Received D-Bus Notification: {parsed_notification.get('summary')}")
        #                     await self.callback_on_notification(parsed_notification)
        #             elif msg_received:
        #                 logger.debug(f"Received other D-Bus message: Member={msg_received.member}, Interface={msg_received.interface}")
        #         except asyncio.TimeoutError:
        #             continue # Just to allow checking self._running
        #         except Exception as e_recv:
        #             logger.error(f"Error receiving/processing D-Bus message: {e_recv}", exc_info=True)
        #             await asyncio.sleep(5) # Avoid rapid error loops
        # except ImportError:
        #     logger.error("Jeepney library not found. D-Bus notification listener cannot run. Please install it.")
        # except ConnectionRefusedError:
        #     logger.error("Could not connect to D-Bus. Ensure D-Bus daemon is running and accessible.")
        # except Exception as e:
        #     logger.error(f"An error occurred in D-Bus notification listener setup: {e}", exc_info=True)
        # finally:
        #     if 'conn' in locals() and conn:
        #         # Optionally remove match rule before closing
        #         # remove_match_msg = new_method_call(dbus_daemon_addr, 'RemoveMatch', 's', (match_rule,))
        #         # await conn.send_and_get_reply(remove_match_msg)
        #         await conn.close()
        #         logger.info("D-Bus connection closed.")
        #     self._running = False
        # --- End Conceptual Jeepney ---

        # Current Placeholder Loop
        while self._running:
            await asyncio.sleep(30)
            if self.callback_on_notification is not None: # Only log if it's supposed to be doing something
                logger.debug("Notification listener placeholder task running (no actual D-Bus listening)...")

    def stop_listening(self):
        self._running = False
        logger.info("Notification listener stopped (simulated/placeholder).")

    def parse_notification_data(self, notification_body: tuple) -> Optional[Dict[str, Any]]:
        """
        Parses the D-Bus notification data (signal body for Notify) into a structured dictionary.
        Standard Notify signature: (app_name, replaces_id, app_icon, summary, body, actions, hints, expire_timeout)
                                    (s, u, s, s, s, as, a{sv}, i)
        """
        try:
            if isinstance(notification_body, tuple) and len(notification_body) == 8:
                return {
                    "app_name": notification_body[0],
                    "replaces_id": notification_body[1], # uint32
                    "app_icon": notification_body[2],
                    "summary": notification_body[3],  # Title
                    "body": notification_body[4],     # Message
                    "actions": notification_body[5], # List of strings (action identifiers)
                    "hints": notification_body[6],   # Dict of variant hints
                    "expire_timeout": notification_body[7] # int32
                }
            else:
                logger.warning(f"Received notification body with unexpected format or length: {notification_body}")
        except Exception as e:
            logger.error(f"Error parsing notification data: {e}. Body was: {notification_body}", exc_info=True)
        return None

async def dummy_notification_callback(notification_details: Dict[str, Any]):
    logger.info(f"Dummy Callback: Received Notification - Summary: '{notification_details.get('summary')}', Body: '{notification_details.get('body')}'")
    # Here, you might trigger news analysis, sentiment analysis, or other actions.

# Example:
# if __name__ == "__main__":
#     listener = NotificationListener(callback_on_notification=dummy_notification_callback)
#     # To test this, you would need a D-Bus environment (Linux desktop) and send a notification:
#     # e.g., using `notify-send "Test Summary" "This is a test notification body."` in terminal.
#     try:
#         asyncio.run(listener.start_listening())
#     except KeyboardInterrupt:
#         listener.stop_listening()
#         logger.info("Notification listener example stopped by user.")
</code>

kamikaze_komodo/ai_news_analysis_agent_module/sentiment_analyzer.py:
<code>
# FILE: kamikaze_komodo/ai_news_analysis_agent_module/sentiment_analyzer.py
from typing import List, Dict, Optional, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.core.models import NewsArticle
from kamikaze_komodo.config.settings import settings # Import global settings

logger = get_logger(__name__)

# Pydantic model for structured sentiment output
class SentimentAnalysisOutput(BaseModel):
    sentiment_label: str = Field(description="The overall sentiment (e.g., 'very bullish', 'bullish', 'neutral', 'bearish', 'very bearish', 'mixed').")
    sentiment_score: float = Field(description="A numerical score from -1.0 (very negative) to 1.0 (very positive). Neutral is 0.0.")
    key_themes: Optional[List[str]] = Field(default_factory=list, description="List of key themes or topics identified in the text related to sentiment.")
    confidence: Optional[float] = Field(description="Confidence score of the sentiment analysis (0.0 to 1.0).")

class SentimentAnalyzer:
    """
    Analyzes text for sentiment using a configured LLM via Langchain.
    Supports Google Vertex AI.
    """
    def __init__(self):
        if not settings:
            logger.critical("Settings not loaded. SentimentAnalyzer cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.llm_provider = settings.sentiment_llm_provider
        self.llm: Any = None # Will be initialized in _initialize_llm

        self._initialize_llm()

        # Define a structured prompt for sentiment analysis
        self.prompt_template = ChatPromptTemplate.from_messages(
            [
                ("system",
                 "You are an expert financial sentiment analyst specializing in cryptocurrency markets. "
                 "Analyze the provided text for its sentiment towards the cryptocurrency or market mentioned. "
                 "Consider factors like news events, market reactions, technological developments, and regulatory news. "
                 "Your output MUST be in JSON format, adhering to the following Pydantic model structure (SentimentAnalysisOutput): "
                 "```json\n"
                 "{\n"
                 "  \"sentiment_label\": \"<label: 'very bullish'|'bullish'|'neutral'|'bearish'|'very bearish'|'mixed'>\",\n"
                 "  \"sentiment_score\": <score_float: -1.0 to 1.0>,\n"
                 "  \"key_themes\": [\"<theme1>\", \"<theme2>\"],\n" # Optional
                 "  \"confidence\": <confidence_float: 0.0 to 1.0>\n" # Optional
                 "}\n"
                 "```"
                 "sentiment_score should range from -1.0 (very bearish/negative) to 1.0 (very bullish/positive). Neutral is 0.0. "
                 "key_themes should highlight important topics influencing the sentiment. confidence is your perceived accuracy of this analysis (0.0 to 1.0)."
                 ),
                ("human", "Please analyze the sentiment of the following text regarding {asset_context}:\n\n---\n{text_to_analyze}\n---"),
            ]
        )
        self.output_parser = JsonOutputParser(pydantic_object=SentimentAnalysisOutput)
        self.chain = self.prompt_template | self.llm | self.output_parser

    def _initialize_llm(self):
        logger.info(f"Initializing LLM for SentimentAnalyzer with provider: {self.llm_provider}")
        if self.llm_provider == "VertexAI":
            if not settings.vertex_ai_project_id or not settings.vertex_ai_location:
                logger.error("Vertex AI project ID or location is not configured in settings.py. Sentiment analysis will not work.")
                raise ValueError("Vertex AI project ID or location missing.")
            try:
                from langchain_google_vertexai import ChatVertexAI # Correct import
                self.llm = ChatVertexAI(
                    project=settings.vertex_ai_project_id,
                    location=settings.vertex_ai_location,
                    model_name=settings.vertex_ai_sentiment_model_name,
                    temperature=0.1, # Low temperature for more factual/consistent sentiment
                    # max_output_tokens=1024, # Optional: if needed for longer summaries/themes
                )
                logger.info(f"SentimentAnalyzer initialized with Vertex AI model: {settings.vertex_ai_sentiment_model_name}")
                logger.info("Ensure Google Cloud credentials (GOOGLE_APPLICATION_CREDENTIALS) are set in your environment.")
            except ImportError:
                logger.error("langchain-google-vertexai is not installed. Please install it: pip install langchain-google-vertexai")
                raise
            except Exception as e:
                logger.error(f"Failed to initialize Vertex AI LLM ({settings.vertex_ai_sentiment_model_name}): {e}", exc_info=True)
                raise
        else:
            logger.error(f"Unsupported LLM provider: {self.llm_provider}")
            raise ValueError(f"Unsupported LLM provider: {self.llm_provider}")

        if self.llm is None:
            logger.error("LLM could not be initialized.")
            raise ValueError("LLM initialization failed.")

    async def analyze_sentiment_structured(self, text: str, asset_context: str = "the market") -> Optional[SentimentAnalysisOutput]:
        """
        Analyzes text and returns a structured sentiment analysis including score and label.
        """
        if not text or not text.strip():
            logger.warning("No text provided for sentiment analysis.")
            return None

        if self.llm is None:
            logger.error("LLM not initialized. Cannot analyze sentiment.")
            return None

        logger.debug(f"Analyzing sentiment for text (context: {asset_context}): '{text[:200]}...'")
        try:
            # Max input tokens for gemini-2.5-flash-preview is high, but let's be reasonable.
            # Prompt itself consumes tokens. Max 30k chars ~ 7.5k tokens for text.
            max_chars = 30000
            if len(text) > max_chars:
                logger.warning(f"Text too long ({len(text)} chars), truncating to {max_chars} for sentiment analysis.")
                text = text[:max_chars]

            response_dict = await self.chain.ainvoke({"text_to_analyze": text, "asset_context": asset_context})

            # The output_parser should already return a SentimentAnalysisOutput object if successful
            # If it's a dict, it means JsonOutputParser might not have directly instantiated the Pydantic model
            # or the LLM didn't return perfect JSON matching the Pydantic model structure.
            if isinstance(response_dict, dict):
                try:
                    # Attempt to create Pydantic model from dict for validation and type safety
                    validated_output = SentimentAnalysisOutput(**response_dict)
                    logger.info(f"Structured sentiment for '{asset_context}': Label: {validated_output.sentiment_label}, Score: {validated_output.sentiment_score:.2f}, Confidence: {validated_output.confidence}")
                    return validated_output
                except Exception as p_exc:
                    logger.error(f"Pydantic validation failed for LLM JSON output: {response_dict}. Error: {p_exc}", exc_info=True)
                    return None
            elif isinstance(response_dict, SentimentAnalysisOutput): # Already a Pydantic object
                   logger.info(f"Structured sentiment for '{asset_context}': Label: {response_dict.sentiment_label}, Score: {response_dict.sentiment_score:.2f}, Confidence: {response_dict.confidence}")
                   return response_dict
            else:
                logger.error(f"Unexpected structured sentiment analysis output type: {type(response_dict)}. Content: {str(response_dict)[:500]}")
                return None
        except Exception as e:
            logger.error(f"Error during structured sentiment analysis with {self.llm_provider} model: {e}", exc_info=True)
            return None

    async def get_sentiment_for_article(self, article: NewsArticle, asset_context: Optional[str] = None) -> NewsArticle:
        """
        Analyzes sentiment for a NewsArticle object and updates its sentiment fields.
        Uses article title and summary/content.
        """
        if not asset_context and article.related_symbols:
            asset_context = ", ".join(article.related_symbols)
        elif not asset_context:
            # Try to infer from title if no symbols
            if "bitcoin" in article.title.lower() or "btc" in article.title.lower():
                asset_context = "Bitcoin"
            elif "ethereum" in article.title.lower() or "eth" in article.title.lower():
                asset_context = "Ethereum"
            else:
                asset_context = "the cryptocurrency market"

        text_to_analyze = article.title
        if article.summary:
            text_to_analyze += "\n\n" + article.summary
        elif article.content: # Fallback to content if no summary
            text_to_analyze += "\n\n" + article.content

        if not text_to_analyze.strip():
            logger.warning(f"No text content found in article {article.id} to analyze.")
            return article # Return original article if no text

        sentiment_result = await self.analyze_sentiment_structured(text_to_analyze, asset_context=asset_context)

        if sentiment_result:
            article.sentiment_label = sentiment_result.sentiment_label
            article.sentiment_score = sentiment_result.sentiment_score
            article.key_themes = sentiment_result.key_themes
            article.sentiment_confidence = sentiment_result.confidence
            # article.raw_llm_response can store the full dict if needed for debugging
            # article.raw_llm_response = sentiment_result.model_dump()
        return article

# Example Usage
async def main_sentiment_example():
    """ Example of using the SentimentAnalyzer """
    if not settings or not settings.vertex_ai_project_id:
        logger.error("Vertex AI settings (Project ID) not loaded for sentiment example. Set GOOGLE_APPLICATION_CREDENTIALS env var.")
        return

    try:
        analyzer = SentimentAnalyzer()
    except Exception as e:
        logger.error(f"Could not start SentimentAnalyzer: {e}")
        return

    example_texts = [
        ("Bitcoin surges past $70,000, analysts predict further upside due to ETF inflows and positive market structure.", "Bitcoin"),
        ("Regulatory crackdown imminent? SEC chair issues stark warning on crypto staking, leading to market jitters.", "Cryptocurrency Regulation"),
        ("Ethereum's Dencun upgrade successfully goes live on mainnet, promising significantly lower fees for Layer 2 solutions and boosting scalability.", "Ethereum"),
        ("The crypto market remains flat this week with low volatility and trading volume, investors seem hesitant.", "the crypto market"),
        ("Solana's network outage causes temporary panic, but recovery was swift. Developers are addressing the root cause.", "Solana")
    ]

    for text, context in example_texts:
        logger.info(f"\n--- Analyzing text for '{context}' ---")
        logger.info(f"Text: {text}")
        result = await analyzer.analyze_sentiment_structured(text, asset_context=context)
        if result:
            logger.info(f"  Sentiment Label: {result.sentiment_label}")
            logger.info(f"  Sentiment Score: {result.sentiment_score:.3f}")
            logger.info(f"  Key Themes: {result.key_themes}")
            logger.info(f"  Confidence: {result.confidence}")
        else:
            logger.warning("  Failed to get structured sentiment analysis.")

    sample_article = NewsArticle(
        id="test_article_sol_123",
        url="http://example.com/news_sol_1",
        title="Solana Ecosystem Sees Major Investment for DeFi Growth",
        summary="The Solana Foundation has announced a new $100 million fund dedicated to fostering DeFi projects on its blockchain. This move is expected to attract more developers and users, with SOL token price reacting positively.",
        source="Crypto News Daily",
        related_symbols=["SOL", "Solana"]
    )
    logger.info(f"\n--- Analyzing NewsArticle: {sample_article.title} ---")
    updated_article = await analyzer.get_sentiment_for_article(sample_article)
    logger.info(f"  Analyzed Article Sentiment: Label='{updated_article.sentiment_label}', Score={updated_article.sentiment_score}, Themes: {updated_article.key_themes}")

if __name__ == "__main__":
    import asyncio
    # Ensure GOOGLE_APPLICATION_CREDENTIALS is set in your environment
    # e.g., export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-file.json"
    asyncio.run(main_sentiment_example())
</code>

kamikaze_komodo/ai_news_analysis_agent_module/__init__.py:
<code>
# kamikaze_komodo/ai_news_analysis_agent_module/__init__.py
# This file makes the 'ai_news_analysis_agent_module' directory a Python package.
</code>

kamikaze_komodo/backtesting_engine/engine.py:
<code>
# FILE: kamikaze_komodo/backtesting_engine/engine.py
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple, Union
from kamikaze_komodo.core.models import BarData, Trade, Order
from kamikaze_komodo.core.enums import SignalType, OrderSide, TradeResult
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone

from kamikaze_komodo.risk_control_module.position_sizer import BasePositionSizer, FixedFractionalPositionSizer
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager, PercentageStopManager

logger = get_logger(__name__)

class BacktestingEngine:
    def __init__(
        self,
        data_feed_df: pd.DataFrame,
        strategy: BaseStrategy,
        initial_capital: float = 10000.0,
        commission_bps: float = 0.0,
        position_sizer: Optional[BasePositionSizer] = None,
        stop_manager: Optional[BaseStopManager] = None,
        sentiment_data_df: Optional[pd.DataFrame] = None,
        slippage_bps: float = 0.0,
        funding_rate_annualized: float = 0.0,
        data_feed_df_pair_asset2: Optional[pd.DataFrame] = None
    ):
        if data_feed_df.empty: raise ValueError("Data feed DataFrame cannot be empty.")
        if not isinstance(data_feed_df.index, pd.DatetimeIndex):
            raise ValueError("Data feed DataFrame must be indexed by pd.DatetimeIndex.")

        self.data_feed_df = data_feed_df.sort_index()
        self.data_feed_df_pair_asset2 = data_feed_df_pair_asset2.sort_index() if data_feed_df_pair_asset2 is not None else None
        self.strategy = strategy
        self.initial_capital = initial_capital
        self.commission_rate = commission_bps / 10000.0
        self.slippage_rate = slippage_bps / 10000.0
        self.funding_rate_annualized = funding_rate_annualized

        self.position_sizer = position_sizer if position_sizer else FixedFractionalPositionSizer()
        self.stop_manager = stop_manager if stop_manager else PercentageStopManager()

        self.sentiment_data_df = sentiment_data_df
        if self.sentiment_data_df is not None and not self.sentiment_data_df.empty:
            if not isinstance(self.sentiment_data_df.index, pd.DatetimeIndex):
                logger.warning("Sentiment data DataFrame must be indexed by pd.DatetimeIndex. Sentiment will not be used.")
                self.sentiment_data_df = None
            else:
                if self.sentiment_data_df.index.tz is None:
                    self.sentiment_data_df.index = self.sentiment_data_df.index.tz_localize('UTC')
                else:
                    self.sentiment_data_df.index = self.sentiment_data_df.index.tz_convert('UTC')
                logger.info(f"Sentiment data loaded with {len(self.sentiment_data_df)} entries.")

        self.portfolio_history: List[Dict[str, Any]] = []
        self.trades_log: List[Trade] = []

        self.current_cash = initial_capital
        self.current_portfolio_value = initial_capital
        self.active_trades: Dict[str, Trade] = {}
        self.trade_id_counter = 0

        logger.info(
            f"BacktestingEngine initialized for strategy '{strategy.name}'. "
            f"Initial Capital: ${initial_capital:,.2f}, Commission: {commission_bps} bps, Slippage: {slippage_bps} bps, Annual Funding: {funding_rate_annualized*100:.2f}%."
        )

    def _get_next_trade_id(self) -> str:
        self.trade_id_counter += 1
        return f"trade_{self.trade_id_counter:05d}"

    def _apply_slippage(self, price: float, side: OrderSide) -> float:
        if self.slippage_rate == 0.0: return price
        if side == OrderSide.BUY: return price * (1 + self.slippage_rate)
        elif side == OrderSide.SELL: return price * (1 - self.slippage_rate)
        return price

    def _apply_funding(self, position_value: float, side: OrderSide, funding_rate_per_bar: float):
        if funding_rate_per_bar == 0.0: return 0.0
        funding_cost = position_value * funding_rate_per_bar
        if side == OrderSide.BUY:
            self.current_cash -= funding_cost
            return funding_cost
        elif side == OrderSide.SELL:
            self.current_cash += funding_cost
            return -funding_cost
        return 0.0

    def _execute_trade_command(self, command: SignalCommand, current_bar_data_for_command_symbol: BarData, bar_index: int):
        signal_type = command.signal_type
        trade_symbol = command.symbol
        execution_price_ideal = command.price if command.price else current_bar_data_for_command_symbol.close
        timestamp = current_bar_data_for_command_symbol.timestamp
        bar_for_atr_calc = command.related_bar_data if command.related_bar_data else current_bar_data_for_command_symbol

        active_trade_for_symbol = self.active_trades.get(trade_symbol)

        # --- ENTRY LOGIC ---
        if signal_type in [SignalType.LONG, SignalType.SHORT] and active_trade_for_symbol is None:
            side = OrderSide.BUY if signal_type == SignalType.LONG else OrderSide.SELL
            execution_price = self._apply_slippage(execution_price_ideal, side)
            
            position_size_units = self.position_sizer.calculate_size(
                symbol=trade_symbol, current_price=execution_price, available_capital=self.current_cash,
                current_portfolio_value=self.current_portfolio_value, latest_bar=bar_for_atr_calc,
                atr_value=bar_for_atr_calc.atr
            )
            if position_size_units is None or position_size_units <= 1e-8: return

            cost_of_assets = position_size_units * execution_price
            commission_cost = cost_of_assets * self.commission_rate

            if side == OrderSide.BUY and cost_of_assets + commission_cost > self.current_cash:
                logger.warning(f"{timestamp} - Insufficient cash for LONG on {trade_symbol}. Skipping.")
                return

            self.current_cash -= commission_cost
            if side == OrderSide.BUY:
                self.current_cash -= cost_of_assets

            custom_fields = {"atr_at_entry": bar_for_atr_calc.atr, "entry_bar_index": bar_index}
            self.active_trades[trade_symbol] = Trade(
                id=self._get_next_trade_id(), symbol=trade_symbol, entry_order_id=f"entry_{self.trade_id_counter}",
                side=side, entry_price=execution_price, amount=position_size_units, entry_timestamp=timestamp,
                commission=commission_cost, custom_fields=custom_fields
            )
            logger.info(f"{timestamp} - EXECUTE CMD {side.name}: {position_size_units:.6f} {trade_symbol} @ ${execution_price:.4f}")

        # --- EXIT LOGIC ---
        elif signal_type in [SignalType.CLOSE_LONG, SignalType.CLOSE_SHORT] and active_trade_for_symbol:
            expected_side = OrderSide.BUY if signal_type == SignalType.CLOSE_LONG else OrderSide.SELL
            if active_trade_for_symbol.side != expected_side: return

            exit_side = OrderSide.SELL if signal_type == SignalType.CLOSE_LONG else OrderSide.BUY
            execution_price = self._apply_slippage(execution_price_ideal, exit_side)
            exit_value = active_trade_for_symbol.amount * execution_price
            commission_cost = exit_value * self.commission_rate
            
            pnl = 0.0
            if active_trade_for_symbol.side == OrderSide.BUY:
                pnl = (execution_price - active_trade_for_symbol.entry_price) * active_trade_for_symbol.amount
                self.current_cash += exit_value
            else: # SELL
                pnl = (active_trade_for_symbol.entry_price - execution_price) * active_trade_for_symbol.amount
                self.current_cash += (active_trade_for_symbol.entry_price * active_trade_for_symbol.amount) + pnl
            
            pnl -= (active_trade_for_symbol.commission + commission_cost)
            self.current_cash -= commission_cost

            self._log_and_clear_active_trade(trade_symbol, timestamp, execution_price, pnl, commission_cost, "SignalClose")


    def _log_and_clear_active_trade(self, symbol_key: str, timestamp: datetime, exit_price: float, pnl: float, exit_commission: float, exit_reason: str):
        active_trade = self.active_trades.get(symbol_key)
        if not active_trade: return

        active_trade.exit_price = exit_price
        active_trade.exit_timestamp = timestamp
        active_trade.pnl = pnl
        active_trade.commission += exit_commission
        active_trade.result = TradeResult.WIN if pnl > 0 else (TradeResult.LOSS if pnl < 0 else TradeResult.BREAKEVEN)
        active_trade.notes = exit_reason

        self.trades_log.append(active_trade.model_copy(deep=True))
        logger.info(
            f"{timestamp} - EXECUTE CLOSE {active_trade.side.name} ({exit_reason}) for {symbol_key}: {active_trade.amount:.6f} @ ${exit_price:.4f}. "
            f"PnL: ${pnl:.2f}. Total Comm: ${active_trade.commission:.2f}. Cash Now: ${self.current_cash:.2f}."
        )
        del self.active_trades[symbol_key]

    def _handle_stop_take_profit(self, current_bar_data_for_symbol: BarData, trade_symbol: str, bar_index: int):
        active_trade = self.active_trades.get(trade_symbol)
        if not active_trade or not self.stop_manager:
            return

        # Pass bar_index to stop loss checker for time-based stops
        stop_loss_price = self.stop_manager.check_stop_loss(active_trade, current_bar_data_for_symbol, bar_index)
        if stop_loss_price is not None:
            exit_side = OrderSide.SELL if active_trade.side == OrderSide.BUY else OrderSide.BUY
            self._execute_trade_command(SignalCommand(
                signal_type=SignalType.CLOSE_LONG if active_trade.side == OrderSide.BUY else SignalType.CLOSE_SHORT,
                symbol=trade_symbol, price=stop_loss_price
            ), current_bar_data_for_symbol, bar_index)
            return # Exit as trade is closed

        active_trade = self.active_trades.get(trade_symbol) # Re-check as SL might have closed it
        if active_trade:
            take_profit_price = self.stop_manager.check_take_profit(active_trade, current_bar_data_for_symbol)
            if take_profit_price is not None:
                self._execute_trade_command(SignalCommand(
                    signal_type=SignalType.CLOSE_LONG if active_trade.side == OrderSide.BUY else SignalType.CLOSE_SHORT,
                    symbol=trade_symbol, price=take_profit_price
                ), current_bar_data_for_symbol, bar_index)

    def run(self) -> tuple[List[Trade], Dict[str, Any], pd.DataFrame]:
        logger.info(f"Starting backtest run for strategy '{self.strategy.name}'...")
        self.portfolio_history.append({
            "timestamp": self.data_feed_df.index[0] - pd.Timedelta(seconds=1) if not self.data_feed_df.empty else datetime.now(timezone.utc),
            "cash": self.initial_capital, "asset_value": 0.0, "total_value": self.initial_capital
        })

        for i, (timestamp, row_asset1) in enumerate(self.data_feed_df.iterrows()):
            ts_aware = timestamp.tz_localize('UTC') if timestamp.tzinfo is None else timestamp.tz_convert('UTC')

            current_bar_asset1 = BarData(
                timestamp=ts_aware, open=row_asset1['open'], high=row_asset1['high'], low=row_asset1['low'],
                close=row_asset1['close'], volume=row_asset1['volume'], symbol=self.strategy.symbol,
                timeframe=self.strategy.timeframe, atr=row_asset1.get('atr'),
                prediction_value=row_asset1.get('prediction_value'), prediction_confidence=row_asset1.get('prediction_confidence'),
                market_regime=row_asset1.get('market_regime')
            )
            self.data_feed_df.name = self.strategy.symbol

            current_bar_asset2 = None
            if self.data_feed_df_pair_asset2 is not None and ts_aware in self.data_feed_df_pair_asset2.index:
                row_asset2 = self.data_feed_df_pair_asset2.loc[ts_aware]
                current_bar_asset2 = BarData(
                    timestamp=ts_aware, open=row_asset2['open'], high=row_asset2['high'], low=row_asset2['low'],
                    close=row_asset2['close'], volume=row_asset2['volume'], symbol=self.strategy.params.get('asset2_symbol', 'PAIR_ASSET2'),
                    timeframe=self.strategy.timeframe, atr=row_asset2.get('atr')
                )
                self.data_feed_df_pair_asset2.name = self.strategy.params.get('asset2_symbol', 'PAIR_ASSET2')

            sentiment_val = self.sentiment_data_df['sentiment_score'].asof(ts_aware) if self.sentiment_data_df is not None else None
            current_bar_asset1.sentiment_score = sentiment_val if pd.notna(sentiment_val) else None

            # Handle stops for all active trades
            for sym_key in list(self.active_trades.keys()):
                bar_for_stop_check = current_bar_asset1 if sym_key == self.strategy.symbol else current_bar_asset2
                if bar_for_stop_check:
                    self._handle_stop_take_profit(bar_for_stop_check, sym_key, i)

            # Generate signals
            strategy_output: Union[Optional[SignalType], List[SignalCommand]] = self.strategy.on_bar_data(current_bar_asset1)
            
            if strategy_output:
                commands = strategy_output if isinstance(strategy_output, list) else [SignalCommand(signal_type=strategy_output, symbol=self.strategy.symbol, price=current_bar_asset1.close, related_bar_data=current_bar_asset1)]
                for command in commands:
                    if command.signal_type != SignalType.HOLD:
                        bar_for_cmd = current_bar_asset1 if command.symbol == self.strategy.symbol else current_bar_asset2
                        if bar_for_cmd:
                            self._execute_trade_command(command, bar_for_cmd, i)

            # Update portfolio MTM and history
            mtm_value = self.current_cash
            for trade in self.active_trades.values():
                price = current_bar_asset1.close if trade.symbol == current_bar_asset1.symbol else (current_bar_asset2.close if current_bar_asset2 else trade.entry_price)
                if trade.side == OrderSide.BUY:
                    mtm_value += trade.amount * price
                else: # Short
                    mtm_value += (trade.entry_price - price) * trade.amount + (trade.amount * trade.entry_price)
            
            self.portfolio_history.append({"timestamp": ts_aware, "cash": self.current_cash, "total_value": mtm_value})

        # Close any open positions at the end of the backtest
        if self.active_trades:
            last_bar = self.data_feed_df.iloc[-1]
            last_bar_data = BarData(timestamp=last_bar.name, open=last_bar['open'], high=last_bar['high'], low=last_bar['low'], close=last_bar['close'], volume=last_bar['volume'], symbol=self.strategy.symbol)
            for sym_key in list(self.active_trades.keys()):
                active_trade = self.active_trades[sym_key]
                self._execute_trade_command(SignalCommand(
                    signal_type=SignalType.CLOSE_LONG if active_trade.side == OrderSide.BUY else SignalType.CLOSE_SHORT,
                    symbol=sym_key, price=last_bar_data.close), last_bar_data, len(self.data_feed_df) - 1)

        final_portfolio_state = {"initial_capital": self.initial_capital, "final_portfolio_value": self.current_cash}
        logger.info(f"Backtest run completed. Final Portfolio Value: ${final_portfolio_state['final_portfolio_value']:.2f}")
        equity_curve_df = pd.DataFrame(self.portfolio_history).set_index('timestamp')
        return self.trades_log, final_portfolio_state, equity_curve_df
</code>

kamikaze_komodo/backtesting_engine/optimizer.py:
<code>
# kamikaze_komodo/backtesting_engine/optimizer.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Callable, Tuple, Optional
import itertools
import optuna # For more advanced optimization like TPE

from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.backtesting_engine.engine import BacktestingEngine
from kamikaze_komodo.backtesting_engine.performance_analyzer import PerformanceAnalyzer
from kamikaze_komodo.config.settings import settings as app_settings # Renamed to avoid conflict
from kamikaze_komodo.risk_control_module.position_sizer import BasePositionSizer, FixedFractionalPositionSizer, ATRBasedPositionSizer # Add more as needed
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager, PercentageStopManager, ATRStopManager # Add more as needed
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class StrategyOptimizer:
    """
    Optimizes strategy parameters using various methods like Grid Search or Optuna.
    Supports walk-forward optimization.
    """
    def __init__(
        self,
        strategy_class: type, # The class of the strategy to optimize (e.g., EWMACStrategy)
        data_feed_df: pd.DataFrame,
        param_grid: Dict[str, List[Any]], # e.g., {'short_window': [10, 12, 15], 'long_window': [20, 26, 30]}
        optimization_metric: str = 'total_net_profit', # Metric to optimize (from PerformanceAnalyzer)
        initial_capital: float = 10000.0,
        commission_bps: float = 0.0,
        slippage_bps: float = 0.0,
        position_sizer_class_name: Optional[str] = None, # e.g., "FixedFractionalPositionSizer"
        position_sizer_params: Optional[Dict[str, Any]] = None,
        stop_manager_class_name: Optional[str] = None, # e.g., "PercentageStopManager"
        stop_manager_params: Optional[Dict[str, Any]] = None,
        sentiment_data_df: Optional[pd.DataFrame] = None,
        symbol: Optional[str] = None, # Pass symbol if not in strategy params
        timeframe: Optional[str] = None # Pass timeframe if not in strategy params
    ):
        self.strategy_class = strategy_class
        self.data_feed_df = data_feed_df
        self.param_grid = param_grid
        self.optimization_metric = optimization_metric
        self.initial_capital = initial_capital
        self.commission_bps = commission_bps
        self.slippage_bps = slippage_bps

        self.position_sizer_class_name = position_sizer_class_name
        self.position_sizer_params = position_sizer_params if position_sizer_params else {}
        self.stop_manager_class_name = stop_manager_class_name
        self.stop_manager_params = stop_manager_params if stop_manager_params else {}

        self.sentiment_data_df = sentiment_data_df
        self.symbol = symbol if symbol else (app_settings.default_symbol if app_settings else "OPTIMIZE_SYMBOL")
        self.timeframe = timeframe if timeframe else (app_settings.default_timeframe if app_settings else "OPTIMIZE_TF")

        logger.info(f"StrategyOptimizer initialized for {strategy_class.__name__} on {self.symbol} ({self.timeframe}). Optimizing for: {optimization_metric}")

    def _get_risk_module_instance(self, class_name_str: Optional[str], base_module, params_to_pass: Dict):
        if not class_name_str:
            return None
        try:
            ClassReference = getattr(base_module, class_name_str)
            return ClassReference(params=params_to_pass) # Assuming constructors accept 'params' dict
        except AttributeError:
            logger.error(f"Could not find class {class_name_str} in {base_module.__name__}")
        except Exception as e:
            logger.error(f"Error instantiating {class_name_str}: {e}")
        return None


    def _run_backtest_for_params(self, params_set: Dict[str, Any], current_data_feed: pd.DataFrame) -> float:
        """Runs a single backtest for a given set of parameters."""
        try:
            # Ensure strategy parameters are correctly passed, including any being optimized
            strategy_instance = self.strategy_class(
                symbol=self.symbol,
                timeframe=self.timeframe,
                params=params_set # Pass the current combination of parameters being tested
            )
    
            # Instantiate PositionSizer and StopManager if class names are provided
            # Merging fixed params with optimized params (if any sizer/stop params are in param_grid)
            combined_sizer_params = {**self.position_sizer_params, **params_set}
            combined_stop_params = {**self.stop_manager_params, **params_set}

            sizer_instance = self._get_risk_module_instance(self.position_sizer_class_name, pdm, combined_sizer_params) if self.position_sizer_class_name else FixedFractionalPositionSizer()
            stop_instance = self._get_risk_module_instance(self.stop_manager_class_name, smm, combined_stop_params) if self.stop_manager_class_name else PercentageStopManager()
    
            # Dynamically import position_sizer_module and stop_manager_module
            import kamikaze_komodo.risk_control_module.position_sizer as pdm
            import kamikaze_komodo.risk_control_module.stop_manager as smm
            from kamikaze_komodo.risk_control_module.volatility_band_stop_manager import VolatilityBandStopManager # if used by name

            if self.position_sizer_class_name:
                sizer_instance = self._get_risk_module_instance(self.position_sizer_class_name, pdm, combined_sizer_params)
            else: # Default sizer if none specified for optimization run
                sizer_instance = FixedFractionalPositionSizer(params=combined_sizer_params)


            if self.stop_manager_class_name:
                if self.stop_manager_class_name == "VolatilityBandStopManager":
                    # VolatilityBandStopManager is in a separate file
                    from kamikaze_komodo.risk_control_module import volatility_band_stop_manager as vbsm
                    stop_instance = self._get_risk_module_instance(self.stop_manager_class_name, vbsm, combined_stop_params)
                else:
                    stop_instance = self._get_risk_module_instance(self.stop_manager_class_name, smm, combined_stop_params)
            else: # Default stop manager
                stop_instance = PercentageStopManager(params=combined_stop_params)


            engine = BacktestingEngine(
                data_feed_df=current_data_feed,
                strategy=strategy_instance,
                initial_capital=self.initial_capital,
                commission_bps=self.commission_bps,
                slippage_bps=self.slippage_bps,
                position_sizer=sizer_instance,
                stop_manager=stop_instance,
                sentiment_data_df=self.sentiment_data_df # Assumed to cover the current_data_feed period
            )
            trades_log, final_portfolio, equity_curve = engine.run()
    
            # Metrics calculation
            risk_free_rate = float(app_settings.config.get('BacktestingPerformance', 'RiskFreeRateAnnual', fallback=0.02)) if app_settings else 0.02
            annual_factor = int(app_settings.config.get('BacktestingPerformance', 'AnnualizationFactor', fallback=252)) if app_settings else 252

            analyzer = PerformanceAnalyzer(
                trades=trades_log,
                initial_capital=self.initial_capital,
                final_capital=final_portfolio['final_portfolio_value'],
                equity_curve_df=equity_curve,
                risk_free_rate_annual=risk_free_rate,
                annualization_factor=annual_factor
            )
            metrics = analyzer.calculate_metrics()
            metric_value = metrics.get(self.optimization_metric, -float('inf'))

            if pd.isna(metric_value):
                metric_value = -float('inf') if self.optimization_metric != 'max_drawdown_pct' else float('inf')

            # For Optuna, which maximizes by default. If metric is drawdown, we want to minimize, so return negative.
            # However, Optuna's direction is set in create_study. So, return the actual metric value.
            return float(metric_value)

        except Exception as e:
            logger.error(f"Error during backtest for params {params_set}: {e}", exc_info=True)
            return -float('inf') if self.optimization_metric != 'max_drawdown_pct' else float('inf')


    def grid_search(self) -> Tuple[Optional[Dict[str, Any]], float, pd.DataFrame]:
        param_names = list(self.param_grid.keys())
        param_value_combinations = list(itertools.product(*self.param_grid.values()))

        results = []
        best_metric = -float('inf')
        if self.optimization_metric == 'max_drawdown_pct':
            best_metric = float('inf')
    
        best_params = None
        logger.info(f"Starting Grid Search with {len(param_value_combinations)} combinations.")

        for i, combo in enumerate(param_value_combinations):
            current_params = dict(zip(param_names, combo))
            logger.debug(f"Grid Search - Combo {i+1}/{len(param_value_combinations)}: {current_params}")
            metric_value = self._run_backtest_for_params(current_params, self.data_feed_df)
            results.append({**current_params, 'metric_value': metric_value})
    
            if self.optimization_metric == 'max_drawdown_pct':
                if metric_value < best_metric:
                    best_metric = metric_value
                    best_params = current_params
            elif metric_value > best_metric:
                best_metric = metric_value
                best_params = current_params

        results_df = pd.DataFrame(results)
        if best_params:
            logger.info(f"Grid Search completed. Best params: {best_params}, Best {self.optimization_metric}: {best_metric:.4f}")
        else:
            logger.warning("Grid Search completed but no best parameters found (all trials might have failed or yielded non-comparable results).")
        return best_params, best_metric, results_df.sort_values(by='metric_value', ascending=(self.optimization_metric == 'max_drawdown_pct'))


    def optuna_optimize(self, n_trials: int = 100, study_name: Optional[str] = None, storage_url: Optional[str] = None) -> Tuple[Optional[Dict[str, Any]], float, optuna.study.Study]:
        if not study_name:
            study_name = f"{self.strategy_class.__name__}_{self.symbol.replace('/', '')}_{self.timeframe}_Optimization"

        direction = 'minimize' if self.optimization_metric == 'max_drawdown_pct' else 'maximize'
        study = optuna.create_study(study_name=study_name, storage=storage_url, load_if_exists=True, direction=direction)

        def objective(trial: optuna.trial.Trial) -> float:
            params_set = {}
            for param_name, values in self.param_grid.items():
                if not values: # Skip if param list is empty
                    logger.warning(f"Parameter '{param_name}' in grid has empty values. Skipping for Optuna.")
                    continue
                if isinstance(values[0], bool): # Categorical for bool
                    params_set[param_name] = trial.suggest_categorical(param_name, [True, False])
                elif isinstance(values[0], int) and len(values) >= 2: # Treat as int range [min, max, step(optional)]
                    step = values[2] if len(values) > 2 else 1
                    params_set[param_name] = trial.suggest_int(param_name, values[0], values[1], step=step)
                elif isinstance(values[0], float) and len(values) >= 2: # Treat as float range [min, max, step(optional for log)]
                    # Optuna's suggest_float doesn't have a direct step like suggest_int.
                    # If discrete floats are needed, use suggest_categorical or round after suggestion.
                    params_set[param_name] = trial.suggest_float(param_name, values[0], values[1])
                elif isinstance(values, list): # Categorical for other types
                    params_set[param_name] = trial.suggest_categorical(param_name, values)
                else:
                    logger.warning(f"Parameter '{param_name}' in grid has unsupported format for Optuna. Values: {values}. Skipping.")
    
            metric_value = self._run_backtest_for_params(params_set, self.data_feed_df)
            return metric_value

        logger.info(f"Starting Optuna optimization with {n_trials} trials. Optimizing for {self.optimization_metric} ({direction}).")
        study.optimize(objective, n_trials=n_trials, timeout=None) # Add timeout if needed

        best_params = None
        best_metric_value = study.best_value if study.best_trial else (-float('inf') if direction == 'maximize' else float('inf'))
        if study.best_trial:
            best_params = study.best_params
            logger.info(f"Optuna optimization completed. Best params: {best_params}, Best {self.optimization_metric}: {best_metric_value:.4f}")
        else:
            logger.warning("Optuna optimization completed but no best trial found (all trials might have failed or yielded non-comparable results).")
        return best_params, best_metric_value, study


    def walk_forward_optimization(
        self,
        training_period_bars: int,
        testing_period_bars: int,
        step_size_bars: int,
        optimization_method: str = 'grid_search',
        optuna_n_trials_per_step: int = 50
    ) -> List[Dict[str, Any]]:
        if not isinstance(self.data_feed_df.index, pd.DatetimeIndex):
            raise ValueError("data_feed_df must have a DatetimeIndex for walk-forward optimization.")

        full_data = self.data_feed_df.copy()
        n_total_bars = len(full_data)

        if training_period_bars + testing_period_bars > n_total_bars:
            logger.error("Not enough data for even one training/testing period in WFO.")
            return []

        results_over_time = []
        start_idx = 0

        logger.info(f"Starting Walk-Forward Optimization. Train: {training_period_bars}, Test: {testing_period_bars}, Step: {step_size_bars}")

        original_full_data_feed = self.data_feed_df # Store the original full data feed reference

        while start_idx + training_period_bars <= n_total_bars: # Ensure training period is within bounds
            train_end_idx = start_idx + training_period_bars
            test_start_idx = train_end_idx
            test_end_idx = min(test_start_idx + testing_period_bars, n_total_bars) # Don't go beyond total bars

            if test_start_idx >= test_end_idx : # No testing data left
                break

            training_data = full_data.iloc[start_idx:train_end_idx]
            testing_data = full_data.iloc[test_start_idx:test_end_idx]
    
            logger.info(f"WFO Step: Training from {training_data.index[0]} to {training_data.index[-1]} ({len(training_data)} bars)")
    
            self.data_feed_df = training_data # Temporarily set data_feed_df for optimization methods

            best_params_for_step: Optional[Dict[str, Any]] = None
            train_metric_value = -float('inf')

            if optimization_method == 'grid_search':
                best_params_for_step, train_metric_value, _ = self.grid_search()
            elif optimization_method == 'optuna':
                study_name_wfo = f"{self.strategy_class.__name__}_WFO_Step_{start_idx}"
                best_params_for_step, train_metric_value, _ = self.optuna_optimize(n_trials=optuna_n_trials_per_step, study_name=study_name_wfo)
            else:
                logger.error(f"Unsupported optimization_method: {optimization_method}")
                self.data_feed_df = original_full_data_feed # Restore
                return results_over_time

            if best_params_for_step:
                logger.info(f"WFO Step: Best params from training: {best_params_for_step} (Metric: {train_metric_value:.4f})")
                logger.info(f"WFO Step: Testing from {testing_data.index[0]} to {testing_data.index[-1]} ({len(testing_data)} bars) with best params.")
        
                test_metric_value = self._run_backtest_for_params(best_params_for_step, testing_data)
        
                results_over_time.append({
                    'train_start_date': training_data.index[0],
                    'train_end_date': training_data.index[-1],
                    'test_start_date': testing_data.index[0],
                    'test_end_date': testing_data.index[-1],
                    'best_params': best_params_for_step,
                    f'train_{self.optimization_metric}': train_metric_value,
                    f'test_{self.optimization_metric}': test_metric_value
                })
                logger.info(f"WFO Step: Test period {self.optimization_metric}: {test_metric_value:.4f}")
            else:
                logger.warning("WFO Step: No best parameters found in training phase. Skipping test for this step.")

            start_idx += step_size_bars

        self.data_feed_df = original_full_data_feed # Restore original full data feed
        logger.info("Walk-Forward Optimization completed.")
        return results_over_time
</code>

kamikaze_komodo/backtesting_engine/performance_analyzer.py:
<code>
# kamikaze_komodo/backtesting_engine/performance_analyzer.py
# Phase 6: Added Calmar, Sortino, Avg Holding Period, Win/Loss Streaks.
# Configurable risk-free rate and annualization factor.

from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from kamikaze_komodo.core.models import Trade
from kamikaze_komodo.core.enums import OrderSide, TradeResult
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class PerformanceAnalyzer:
    def __init__(
        self,
        trades: List[Trade],
        initial_capital: float,
        final_capital: float,
        equity_curve_df: Optional[pd.DataFrame] = None, # Timestamp-indexed 'total_value'
        risk_free_rate_annual: float = 0.02, # Annual risk-free rate (e.g., 2%)
        annualization_factor: int = 252 # Trading days in a year for Sharpe/Sortino
    ):
        if not trades:
            logger.warning("PerformanceAnalyzer initialized with no trades. Some metrics might be zero or NaN.")
        self.trades_df = pd.DataFrame([trade.model_dump() for trade in trades])
        if not self.trades_df.empty:
            self.trades_df['entry_timestamp'] = pd.to_datetime(self.trades_df['entry_timestamp'])
            self.trades_df['exit_timestamp'] = pd.to_datetime(self.trades_df['exit_timestamp'])
    
        self.initial_capital = initial_capital
        self.final_capital = final_capital
        self.equity_curve_df = equity_curve_df
        self.risk_free_rate_annual = risk_free_rate_annual
        self.annualization_factor = annualization_factor
    
        logger.info(f"PerformanceAnalyzer initialized. Trades: {len(trades)}, Initial: ${initial_capital:,.2f}, Final: ${final_capital:,.2f}")
        logger.info(f"Using Annual Risk-Free Rate: {self.risk_free_rate_annual*100:.2f}%, Annualization Factor: {self.annualization_factor}")


    def _calculate_periodic_returns(self) -> Optional[pd.Series]:
        if self.equity_curve_df is None or self.equity_curve_df.empty or 'total_value' not in self.equity_curve_df.columns:
            logger.warning("Equity curve data is missing or invalid. Cannot calculate periodic returns for Sharpe/Sortino.")
            return None
        # Resample to daily returns for annualization, handling potential non-unique index if multiple records per day
        daily_equity = self.equity_curve_df['total_value'].resample('D').last().ffill()
        periodic_returns = daily_equity.pct_change().dropna()
        return periodic_returns

    def calculate_metrics(self) -> Dict[str, Any]:
        metrics: Dict[str, Any] = {
            "initial_capital": self.initial_capital,
            "final_capital": self.final_capital,
            "total_net_profit": 0.0,
            "total_return_pct": 0.0,
            "total_trades": 0,
            "winning_trades": 0,
            "losing_trades": 0,
            "breakeven_trades": 0,
            "win_rate_pct": 0.0,
            "loss_rate_pct": 0.0,
            "average_pnl_per_trade": 0.0,
            "average_win_pnl": 0.0,
            "average_loss_pnl": 0.0,
            "profit_factor": np.nan,
            "max_drawdown_pct": 0.0,
            "sharpe_ratio": np.nan,
            "sortino_ratio": np.nan,
            "calmar_ratio": np.nan,
            "total_fees_paid": 0.0,
            "average_holding_period_hours": 0.0,
            "longest_win_streak": 0,
            "longest_loss_streak": 0,
        }

        if self.trades_df.empty:
            metrics["total_net_profit"] = self.final_capital - self.initial_capital
            if self.initial_capital > 0:
                metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
            logger.warning("No trades to analyze. Returning basic capital metrics.")
            return metrics

        pnl_series = self.trades_df['pnl'].dropna()
        if pnl_series.empty:
            metrics["total_net_profit"] = self.final_capital - self.initial_capital # If PnL couldn't be calculated for trades
            if self.initial_capital > 0: metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
            metrics["total_trades"] = len(self.trades_df)
            metrics["total_fees_paid"] = self.trades_df['commission'].sum()
            return metrics
    
        metrics["total_net_profit"] = pnl_series.sum()
        if self.initial_capital > 0: metrics["total_return_pct"] = (metrics["total_net_profit"] / self.initial_capital) * 100
        metrics["total_trades"] = len(pnl_series)

        wins = pnl_series[pnl_series > 0]
        losses = pnl_series[pnl_series < 0]
        breakevens = pnl_series[pnl_series == 0]
        metrics["winning_trades"] = len(wins)
        metrics["losing_trades"] = len(losses)
        metrics["breakeven_trades"] = len(breakevens)

        if metrics["total_trades"] > 0:
            metrics["win_rate_pct"] = (metrics["winning_trades"] / metrics["total_trades"]) * 100
            metrics["loss_rate_pct"] = (metrics["losing_trades"] / metrics["total_trades"]) * 100
            metrics["average_pnl_per_trade"] = pnl_series.mean()
        if not wins.empty: metrics["average_win_pnl"] = wins.mean()
        if not losses.empty: metrics["average_loss_pnl"] = losses.mean()

        gross_profit = wins.sum()
        gross_loss = abs(losses.sum())
        if gross_loss > 0: metrics["profit_factor"] = gross_profit / gross_loss
        elif gross_profit > 0: metrics["profit_factor"] = np.inf
    
        metrics["total_fees_paid"] = self.trades_df['commission'].sum()

        # Max Drawdown (from equity curve)
        if self.equity_curve_df is not None and not self.equity_curve_df.empty and 'total_value' in self.equity_curve_df.columns:
            equity_values = self.equity_curve_df['total_value']
            if len(equity_values) > 1:
                peak = equity_values.expanding(min_periods=1).max()
                drawdown = (equity_values - peak) / peak
                metrics["max_drawdown_pct"] = abs(drawdown.min()) * 100 if not drawdown.empty else 0.0
    
        # Sharpe and Sortino Ratios
        periodic_returns = self._calculate_periodic_returns()
        if periodic_returns is not None and len(periodic_returns) > 1:
            risk_free_rate_periodic = self.risk_free_rate_annual / self.annualization_factor
            excess_returns = periodic_returns - risk_free_rate_periodic
        
            # Sharpe Ratio
            sharpe_avg_excess_return = excess_returns.mean()
            sharpe_std_excess_return = excess_returns.std()
            if sharpe_std_excess_return is not None and sharpe_std_excess_return != 0:
                metrics["sharpe_ratio"] = (sharpe_avg_excess_return / sharpe_std_excess_return) * np.sqrt(self.annualization_factor)
        
            # Sortino Ratio
            downside_returns = excess_returns[excess_returns < 0]
            if not downside_returns.empty:
                downside_deviation = downside_returns.std()
                if downside_deviation is not None and downside_deviation != 0:
                    metrics["sortino_ratio"] = (sharpe_avg_excess_return / downside_deviation) * np.sqrt(self.annualization_factor)
    
        # Calmar Ratio
        if metrics["max_drawdown_pct"] is not None and metrics["max_drawdown_pct"] > 0:
            # Annualized total return
            if self.equity_curve_df is not None and not self.equity_curve_df.empty:
                start_date = self.equity_curve_df.index.min()
                end_date = self.equity_curve_df.index.max()
                duration_years = (end_date - start_date).days / 365.25 if (end_date - start_date).days > 0 else 1.0/365.25 # Avoid zero division
                total_return = (self.final_capital / self.initial_capital) - 1 if self.initial_capital > 0 else 0
                annualized_return = ((1 + total_return) ** (1 / duration_years)) - 1 if duration_years > 0 else total_return
                metrics["calmar_ratio"] = (annualized_return * 100) / metrics["max_drawdown_pct"]

        # Average Holding Period
        if not self.trades_df.empty and 'exit_timestamp' in self.trades_df.columns and 'entry_timestamp' in self.trades_df.columns:
            valid_trades_for_duration = self.trades_df.dropna(subset=['entry_timestamp', 'exit_timestamp'])
            if not valid_trades_for_duration.empty:
                holding_periods = (valid_trades_for_duration['exit_timestamp'] - valid_trades_for_duration['entry_timestamp'])
                metrics["average_holding_period_hours"] = holding_periods.mean().total_seconds() / 3600 if not holding_periods.empty else 0.0

        # Win/Loss Streaks
        if not pnl_series.empty:
            win_streak, loss_streak = 0, 0
            current_win_streak, current_loss_streak = 0, 0
            for pnl_val in pnl_series:
                if pnl_val > 0:
                    current_win_streak += 1
                    current_loss_streak = 0
                elif pnl_val < 0:
                    current_loss_streak += 1
                    current_win_streak = 0
                else: # Breakeven
                    current_win_streak = 0
                    current_loss_streak = 0
                win_streak = max(win_streak, current_win_streak)
                loss_streak = max(loss_streak, current_loss_streak)
            metrics["longest_win_streak"] = win_streak
            metrics["longest_loss_streak"] = loss_streak

        return metrics

    def print_summary(self, metrics: Optional[Dict[str, Any]] = None):
        if metrics is None:
            metrics = self.calculate_metrics()

        summary = f"""
        --------------------------------------------------
        |         Backtest Performance Summary         |
        --------------------------------------------------
        | Metric                        | Value          |
        --------------------------------------------------
        | Initial Capital               | ${metrics.get("initial_capital", 0):<15,.2f} |
        | Final Capital                 | ${metrics.get("final_capital", 0):<15,.2f} |
        | Total Net Profit              | ${metrics.get("total_net_profit", 0):<15,.2f} |
        | Total Return                  | {metrics.get("total_return_pct", 0):<15.2f}% |
        | Total Trades                  | {metrics.get("total_trades", 0):<16} |
        | Winning Trades                | {metrics.get("winning_trades", 0):<16} |
        | Losing Trades                 | {metrics.get("losing_trades", 0):<16} |
        | Breakeven Trades              | {metrics.get("breakeven_trades", 0):<16} |
        | Win Rate                      | {metrics.get("win_rate_pct", 0):<15.2f}% |
        | Loss Rate                     | {metrics.get("loss_rate_pct", 0):<15.2f}% |
        | Average PnL per Trade         | ${metrics.get("average_pnl_per_trade", 0):<15,.2f} |
        | Average Win PnL               | ${metrics.get("average_win_pnl", 0):<15,.2f} |
        | Average Loss PnL              | ${metrics.get("average_loss_pnl", 0):<15,.2f} |
        | Profit Factor                 | {metrics.get("profit_factor", float('nan')):<16.2f} |
        | Max Drawdown                  | {metrics.get("max_drawdown_pct", 0):<15.2f}% |
        | Sharpe Ratio                  | {metrics.get("sharpe_ratio", float('nan')):<16.2f} |
        | Sortino Ratio                 | {metrics.get("sortino_ratio", float('nan')):<16.2f} |
        | Calmar Ratio                  | {metrics.get("calmar_ratio", float('nan')):<16.2f} |
        | Avg Holding Period (hours)    | {metrics.get("average_holding_period_hours", 0):<16.2f} |
        | Longest Win Streak            | {metrics.get("longest_win_streak", 0):<16} |
        | Longest Loss Streak           | {metrics.get("longest_loss_streak", 0):<16} |
        | Total Fees Paid               | ${metrics.get("total_fees_paid", 0):<15,.2f} |
        --------------------------------------------------
        """
        print(summary)
        logger.info("Performance summary generated." + summary.replace("\n      |", "\n"))
</code>

kamikaze_komodo/backtesting_engine/__init__.py:
<code>
# kamikaze_komodo/backtesting_engine/__init__.py
# This file makes the 'backtesting_engine' directory a Python package.
</code>

kamikaze_komodo/config/config.ini:
<code>
# kamikaze_komodo/config/config.ini

[General]
LogLevel = INFO
LogFilePath = logs/kamikaze_komodo.log

[API]
ExchangeID = krakenfutures
KrakenTestnet = True

[DataFetching]
DefaultSymbol = PF_XBTUSD
DefaultTimeframe = 4h
HistoricalDataDays = 365
DataFetchLimitPerCall = 500

[Trading]
MaxPortfolioRisk = 0.02
DefaultLeverage = 1.0
CommissionBPS = 10
SlippageBPS = 2
FundingRateAnnualized = 0.00

[EWMAC_Strategy]
ShortWindow = 12
LongWindow = 26
SignalWindow = 9
atr_period = 14
SentimentFilter_Long_Threshold = 0.05
SentimentFilter_Short_Threshold = -0.05
EnableShorting = True

[RiskManagement]
PositionSizer = ATRBased
FixedFractional_AllocationFraction = 0.10
ATRBased_RiskPerTradeFraction = 0.01
ATRBased_ATRMultipleForStop = 2.0
PairTradingPositionSizer_DollarNeutral = True
StopManager_Default = ATRBased
PercentageStop_LossPct = 0.02
PercentageStop_TakeProfitPct = 0.05
ATRStop_ATRMultiple = 2.0
VolatilityBandStop_BB_Period = 20
VolatilityBandStop_BB_StdDev = 2.0
VolatilityBandStop_KC_Period = 20
VolatilityBandStop_KC_ATR_Period = 10
VolatilityBandStop_KC_ATR_Multiplier = 1.5
VolatilityBandStop_TrailType = none

[PortfolioConstructor]
AssetAllocator = FixedWeight
DefaultAllocation_BTCUSD = 1.0
Rebalancer_DeviationThreshold = 0.05

[AI_NewsAnalysis]
EnableSentimentAnalysis = True
SentimentLLMProvider = VertexAI
SentimentFilter_Threshold_Long = 0.1
SentimentFilter_Threshold_Short = -0.1
SimulatedSentimentDataPath = kamikaze_komodo/data/simulated_sentiment_data.csv
NewsScraper_Enable = True
NotificationListener_Enable = False
BrowserAgent_Enable = False
BrowserAgent_LLMProvider = VertexAI
BrowserAgent_Max_Steps = 20
; RSS Feeds are now correctly placed within this section
RSSFeed_Coindesk = https://www.coindesk.com/arc/outboundfeeds/rss/?outputType=xml
RSSFeed_Cointelegraph = https://cointelegraph.com/rss
RSSFeed_Decrypt = https://decrypt.co/feed/
RSSFeed_BitcoinComNews = https://news.bitcoin.com/feed/
RSSFeed_Bitcoinist = https://bitcoinist.com/feed/
RSSFeed_UToday = https://u.today/feed/
RSSFeed_CCNNews = https://www.ccn.com/news/crypto-news/feeds/
RSSFeed_CryptoPotato = https://cryptopotato.com/feed/
RSSFeed_CryptoSlate = https://cryptoslate.com/feed/
RSSFeed_TheDefiant = https://thedefiant.io/feed/
RSSFeed_ConsensysNews = https://consensys.io/category/news/feed/

[VertexAI]
ProjectID = kamikazekomodo
Location = us-central1
SentimentModelName = gemini-2.5-flash-preview-05-20
BrowserAgentModelName = gemini-2.5-flash-preview-05-20

[LightGBM_Forecaster]
ModelSavePath = ml_models/trained_models
TargetColumnName = close_change_lag_1_future
TrainingDaysHistory = 730
MinBarsForTraining = 200

[MLForecaster_Strategy]
ForecasterType = lightgbm
ModelConfigSection = LightGBM_Forecaster
LongThreshold = 0.0005
ShortThreshold = -0.0005
ExitLongThreshold = -0.0001
ExitShortThreshold = 0.0001
MinBarsForPrediction = 50
atr_period = 14
EnableShorting = True

[EhlersInstantaneousTrendline_Strategy]
IT_Lag_Trigger = 1
atr_period = 14
EnableShorting = True

[BollingerBandBreakout_Strategy]
bb_period = 20
bb_std_dev = 2.0
atr_period = 14
volume_filter_enabled = True
volume_sma_period = 20
volume_factor_above_sma = 1.5
min_breakout_atr_multiple = 0.5
EnableShorting = True

[PairTrading_Strategy]
Asset1_Symbol = PF_XBTUSD
Asset2_Symbol = PF_ETHUSD
Cointegration_Lookback_Days = 90
Cointegration_Test_PValue_Threshold = 0.05
Spread_ZScore_Entry_Threshold = 2.0
Spread_ZScore_Exit_Threshold = 0.5
Spread_Calculation_Window = 20
EnableShorting = True

[XGBoost_Classifier_Forecaster]
ModelSavePath = ml_models/trained_models
TargetDefinition = next_bar_direction
NumClasses = 3
ReturnThresholds_Percent = -0.001, 0.001
TrainingDaysHistory = 730
MinBarsForTraining = 200

[KMeans_Regime_Model]
ModelSavePath = ml_models/trained_models
NumClusters = 3
FeaturesForClustering = volatility_20d,atr_14d_percentage
TrainingDaysHistory = 730

[BacktestingPerformance]
RiskFreeRateAnnual = 0.02
AnnualizationFactor = 252
</code>

kamikaze_komodo/config/secrets.ini:
<code>
; kamikaze_komodo/config/secrets.ini
; This file should be in .gitignore and contain sensitive information.
[KRAKEN_API]
API_KEY = 'd27PYGi95tlsV4gVotVNXinHOTAxXY2usUta7kw3IogO9/9kpLHCHgcv'
SECRET_KEY = 'kB+i8be+l7J6Lr+RyjodrqNyQXrIn6reFeNfDsmMs01zsQg3KPGSSshd9l4KwvY92LQyYamDc1lMrHsnZ6+LaWQP'

[DATABASE]
User = db_user
Password = db_password
</code>

kamikaze_komodo/config/settings.py:
<code>
# FILE: kamikaze_komodo/config/settings.py
import configparser
import os
from kamikaze_komodo.app_logger import get_logger
from typing import Dict, List, Optional, Any

logger = get_logger(__name__)

# Define a single, reliable project root.
# This file is in .../kamikaze_komodo/config, so two levels up is the project root.
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


class Config:
    """
    Manages application configuration using config.ini and secrets.ini.
    """
    def __init__(self, config_file_rel_path='config/config.ini', secrets_file_rel_path='config/secrets.ini'):
        self.config = configparser.ConfigParser()
        self.secrets = configparser.ConfigParser()

        self.config_file_path = os.path.join(PROJECT_ROOT, config_file_rel_path)
        self.secrets_file_path = os.path.join(PROJECT_ROOT, secrets_file_rel_path)

        if not os.path.exists(self.config_file_path):
            logger.error(f"Config file not found: {self.config_file_path}")
            raise FileNotFoundError(f"Config file not found: {self.config_file_path}")
        if not os.path.exists(self.secrets_file_path):
            logger.warning(f"Secrets file not found: {self.secrets_file_path}. Some features might be unavailable.")

        self.config.read(self.config_file_path)
        self.secrets.read(self.secrets_file_path)

        # General Settings
        self.log_level: str = self.config.get('General', 'LogLevel', fallback='INFO')
        self.log_file_path: str = self.config.get('General', 'LogFilePath', fallback='logs/kamikaze_komodo.log')

        # API Settings
        self.exchange_id_to_use: str = self.config.get('API', 'ExchangeID', fallback='krakenfutures')
        self.kraken_api_key: Optional[str] = self.secrets.get('KRAKEN_API', 'API_KEY', fallback=None)
        self.kraken_secret_key: Optional[str] = self.secrets.get('KRAKEN_API', 'SECRET_KEY', fallback=None)
        self.kraken_testnet: bool = self.config.getboolean('API', 'KrakenTestnet', fallback=True)

        # Data Fetching Settings
        self.default_symbol: str = self.config.get('DataFetching', 'DefaultSymbol', fallback='PF_XBTUSD')
        self.default_timeframe: str = self.config.get('DataFetching', 'DefaultTimeframe', fallback='4h')
        self.historical_data_days: int = self.config.getint('DataFetching', 'HistoricalDataDays', fallback=365)
        self.data_fetch_limit_per_call: int = self.config.getint('DataFetching', 'DataFetchLimitPerCall', fallback=500)

        # Trading Settings
        self.max_portfolio_risk: float = self.config.getfloat('Trading', 'MaxPortfolioRisk', fallback=0.02)
        self.default_leverage: float = self.config.getfloat('Trading', 'DefaultLeverage', fallback=1.0)
        self.commission_bps: float = self.config.getfloat('Trading', 'CommissionBPS', fallback=10.0)

        # EWMAC Strategy Settings (Example, specific strategies below)
        self.ewmac_short_window: int = self.config.getint('EWMAC_Strategy', 'ShortWindow', fallback=12)
        self.ewmac_long_window: int = self.config.getint('EWMAC_Strategy', 'LongWindow', fallback=26)
        self.ewmac_signal_window: int = self.config.getint('EWMAC_Strategy', 'SignalWindow', fallback=9)
        self.ewmac_atr_period: int = self.config.getint('EWMAC_Strategy', 'atr_period', fallback=14)


        # --- Phase 3: Risk Management Settings ---
        self.position_sizer_type: str = self.config.get('RiskManagement', 'PositionSizer', fallback='FixedFractional')
        self.fixed_fractional_allocation_fraction: float = self.config.getfloat('RiskManagement', 'FixedFractional_AllocationFraction', fallback=0.10)
        self.atr_based_risk_per_trade_fraction: float = self.config.getfloat('RiskManagement', 'ATRBased_RiskPerTradeFraction', fallback=0.01)
        self.atr_based_atr_multiple_for_stop: float = self.config.getfloat('RiskManagement', 'ATRBased_ATRMultipleForStop', fallback=2.0)

        self.stop_manager_type: str = self.config.get('RiskManagement', 'StopManager_Default', fallback='PercentageBased')
        _sl_pct_str = self.config.get('RiskManagement', 'PercentageStop_LossPct', fallback='0.02')
        self.percentage_stop_loss_pct: Optional[float] = float(_sl_pct_str) if _sl_pct_str and _sl_pct_str.lower() not in ['none', '0', '0.0'] else None
        _tp_pct_str = self.config.get('RiskManagement', 'PercentageStop_TakeProfitPct', fallback='0.05')
        self.percentage_stop_take_profit_pct: Optional[float] = float(_tp_pct_str) if _tp_pct_str and _tp_pct_str.lower() not in ['none', '0', '0.0'] else None
        self.atr_stop_atr_multiple: float = self.config.getfloat('RiskManagement', 'ATRStop_ATRMultiple', fallback=2.0)

        # --- Phase 3: Portfolio Constructor Settings ---
        self.asset_allocator_type: str = self.config.get('PortfolioConstructor', 'AssetAllocator', fallback='FixedWeight')
        default_symbol_config_key = f'DefaultAllocation_{self.default_symbol.replace("/", "").replace(":", "")}'
        self.default_allocation_for_symbol: float = self.config.getfloat('PortfolioConstructor', default_symbol_config_key, fallback=1.0)
        self.rebalancer_deviation_threshold: float = self.config.getfloat('PortfolioConstructor', 'Rebalancer_DeviationThreshold', fallback=0.05)

        # --- Phase 4: AI News Analysis Settings ---
        self.enable_sentiment_analysis: bool = self.config.getboolean('AI_NewsAnalysis', 'EnableSentimentAnalysis', fallback=True)
        self.sentiment_llm_provider: str = self.config.get('AI_NewsAnalysis', 'SentimentLLMProvider', fallback='VertexAI')
        self.browser_agent_llm_provider: str = self.config.get('AI_NewsAnalysis', 'BrowserAgent_LLMProvider', fallback='VertexAI')
        self.browser_agent_max_steps: int = self.config.getint('AI_NewsAnalysis', 'BrowserAgent_Max_Steps', fallback=20)


        self.sentiment_filter_threshold_long: float = self.config.getfloat('AI_NewsAnalysis', 'SentimentFilter_Threshold_Long', fallback=0.1)
        self.sentiment_filter_threshold_short: float = self.config.getfloat('AI_NewsAnalysis', 'SentimentFilter_Threshold_Short', fallback=-0.1)
        
        self.simulated_sentiment_data_path: Optional[str] = self.config.get('AI_NewsAnalysis', 'SimulatedSentimentDataPath', fallback=None)
        if self.simulated_sentiment_data_path and self.simulated_sentiment_data_path.lower() in ['none', '']:
             self.simulated_sentiment_data_path = None
        if self.simulated_sentiment_data_path and not os.path.isabs(self.simulated_sentiment_data_path):
            path_parts = self.simulated_sentiment_data_path.split(os.sep)
            if path_parts[0] == 'kamikaze_komodo':
                correct_relative_path = os.path.join(*path_parts[1:])
            else:
                correct_relative_path = self.simulated_sentiment_data_path
            self.simulated_sentiment_data_path = os.path.join(PROJECT_ROOT, correct_relative_path)


        self.news_scraper_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'NewsScraper_Enable', fallback=True)
        self.notification_listener_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'NotificationListener_Enable', fallback=False)
        self.browser_agent_enable: bool = self.config.getboolean('AI_NewsAnalysis', 'BrowserAgent_Enable', fallback=False)
        
        # VertexAI Settings
        self.vertex_ai_project_id: Optional[str] = self.config.get('VertexAI', 'ProjectID', fallback=None)
        self.vertex_ai_location: Optional[str] = self.config.get('VertexAI', 'Location', fallback=None)
        self.vertex_ai_sentiment_model_name: str = self.config.get('VertexAI', 'SentimentModelName', fallback='gemini-2.5-flash-preview-05-20')
        self.vertex_ai_browser_agent_model_name: str = self.config.get('VertexAI', 'BrowserAgentModelName', fallback='gemini-2.5-flash-preview-05-20')

        if self.vertex_ai_project_id and self.vertex_ai_project_id.lower() == 'your-gcp-project-id':
            logger.warning("Vertex AI ProjectID is set to 'your-gcp-project-id'. Please update it in config.ini.")
            self.vertex_ai_project_id = None

        self.rss_feeds: List[Dict[str, str]] = []
        if self.config.has_section('AI_NewsAnalysis'):
            for key, value in self.config.items('AI_NewsAnalysis'):
                clean_key = key.strip().lower()
                if clean_key.startswith("rssfeed_"):
                    feed_name_part = clean_key.replace("rssfeed_", "")
                    feed_name = feed_name_part.replace("_", " ").title()
                    self.rss_feeds.append({"name": feed_name, "url": value})
        if not self.rss_feeds:
            logger.warning("No RSS feeds configured in config.ini under [AI_NewsAnalysis] with 'RSSFeed_' prefix.")


    def get_strategy_params(self, strategy_or_component_name: str) -> dict:
        """
        Retrieves parameters for a given strategy or component section name.
        Example section names: EWMAC_Strategy, LightGBM_Forecaster, MLForecaster_Strategy
        """
        params = {}
        found_section = None
        for section in self.config.sections():
            if section.lower() == strategy_or_component_name.lower():
                found_section = section
                break
        
        if found_section and self.config.has_section(found_section):
            params = dict(self.config.items(found_section))
            for key, value in params.items():
                original_value = value
                try:
                    if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                        params[key] = int(value)
                    else:
                        try:
                            params[key] = float(value)
                        except ValueError:
                            if value.lower() == 'true': params[key] = True
                            elif value.lower() == 'false': params[key] = False
                            elif value.lower() in ['none', '']: params[key] = None
                            else:
                                params[key] = original_value
                except Exception as e:
                    logger.debug(f"Could not auto-convert param '{key}' with value '{original_value}' in section '{found_section}'. Kept as string. Error: {e}")
                    params[key] = original_value
        else:
            logger.warning(f"No specific configuration section found for: {strategy_or_component_name}. Using defaults or globally passed params.")
        
        # FIX: Use the correct attribute names as defined in __init__
        if 'sentimentfilter_long_threshold' not in params:
            params['sentimentfilter_long_threshold'] = self.sentiment_filter_threshold_long
        if 'sentimentfilter_short_threshold' not in params:
            params['sentimentfilter_short_threshold'] = self.sentiment_filter_threshold_short
            
        return params

    def get_news_scraper_config(self) -> Dict[str, Any]:
        cfg = {"rss_feeds": self.rss_feeds, "websites": []}
        return cfg


try:
    settings = Config()
except FileNotFoundError as e:
    logger.critical(f"Could not initialize settings due to missing configuration file: {e}")
    settings = None # type: ignore
except Exception as e_global:
    logger.critical(f"Failed to initialize Config object: {e_global}", exc_info=True)
    settings = None # type: ignore

if settings and (not settings.kraken_api_key or "YOUR_API_KEY" in str(settings.kraken_api_key).upper() or "D27PYGI95TLS" in str(settings.kraken_api_key).upper()):
    logger.warning(f"API Key for '{settings.exchange_id_to_use}' appears to be a placeholder or is not configured in secrets.ini. Authenticated interaction will be limited/simulated.")
</code>

kamikaze_komodo/config/__init__.py:
<code>
# kamikaze_komodo/config/__init__.py
# This file makes the 'config' directory a Python package.
</code>

kamikaze_komodo/core/enums.py:
<code>
# kamikaze_komodo/core/enums.py

from enum import Enum


class OrderType(Enum):

    """

    Represents the type of an order.

    """

    MARKET = "market"

    LIMIT = "limit"

    STOP = "stop"

    STOP_LIMIT = "stop_limit"

    TAKE_PROFIT = "take_profit"

    TAKE_PROFIT_LIMIT = "take_profit_limit"


class OrderSide(Enum):

    """

    Represents the side of an order.

    """

    BUY = "buy"

    SELL = "sell"


class SignalType(Enum):

    """

    Represents the type of trading signal generated by a strategy.

    """

    LONG = "LONG"

    SHORT = "SHORT"

    HOLD = "HOLD"

    CLOSE_LONG = "CLOSE_LONG"

    CLOSE_SHORT = "CLOSE_SHORT"


class CandleInterval(Enum):

    """

    Represents common candle intervals for market data.

    Follows CCXT conventions where possible.

    """

    ONE_MINUTE = "1m"

    THREE_MINUTES = "3m"

    FIVE_MINUTES = "5m"

    FIFTEEN_MINUTES = "15m"

    THIRTY_MINUTES = "30m"

    ONE_HOUR = "1h"

    TWO_HOURS = "2h"

    FOUR_HOURS = "4h"

    SIX_HOURS = "6h"

    EIGHT_HOURS = "8h"

    TWELVE_HOURS = "12h"

    ONE_DAY = "1d"

    THREE_DAYS = "3d"

    ONE_WEEK = "1w"

    ONE_MONTH = "1M"


class TradeResult(Enum):

    """

    Represents the outcome of a trade.

    """

    WIN = "WIN"

    LOSS = "LOSS"

    BREAKEVEN = "BREAKEVEN"

</code>

kamikaze_komodo/core/models.py:
<code>
# kamikaze_komodo/core/models.py
# Added fields to NewsArticle and Trade as potentially needed by new modules.
# Added prediction_value and prediction_confidence to BarData for Phase 5 ML strategy.
# Phase 6: Added PairTrade model.
from typing import Optional, List, Dict, Any # Added Any
from pydantic import BaseModel, Field
from datetime import datetime, timezone
import pydantic # Added timezone
from kamikaze_komodo.core.enums import OrderType, OrderSide, SignalType, TradeResult

class BarData(BaseModel):
    """
    Represents OHLCV market data for a specific time interval.
    """
    timestamp: datetime = Field(..., description="The start time of the candle, expected to be timezone-aware (UTC)")
    open: float = Field(..., gt=0, description="Opening price")
    high: float = Field(..., gt=0, description="Highest price")
    low: float = Field(..., gt=0, description="Lowest price")
    close: float = Field(..., gt=0, description="Closing price")
    volume: float = Field(..., ge=0, description="Trading volume")
    symbol: Optional[str] = Field(None, description="Trading symbol, e.g., BTC/USD")
    timeframe: Optional[str] = Field(None, description="Candle timeframe, e.g., 1h")
    
    # Optional fields for indicators or sentiment
    atr: Optional[float] = Field(None, description="Average True Range at this bar")
    sentiment_score: Optional[float] = Field(None, description="Sentiment score associated with this bar's timestamp")
    
    # Fields for ML predictions (Phase 5)
    prediction_value: Optional[float] = Field(None, description="Predicted value by an ML model (e.g., future price, return)")
    prediction_confidence: Optional[float] = Field(None, description="Confidence of the ML prediction (0.0 to 1.0)")

    # Phase 6: Market Regime
    market_regime: Optional[int] = Field(None, description="Market regime identified by a model (e.g., 0, 1, 2)")


    class Config:
        frozen = False # Allow modification by strategies/engine (e.g. to add ATR or predictions)
        
class Order(BaseModel):
    # ... (no changes from existing)
    id: str = Field(..., description="Unique order identifier (from exchange or internal)")
    symbol: str = Field(..., description="Trading symbol, e.g., BTC/USD")
    type: OrderType = Field(..., description="Type of order (market, limit, etc.)")
    side: OrderSide = Field(..., description="Order side (buy or sell)")
    amount: float = Field(..., gt=0, description="Quantity of the asset to trade")
    price: Optional[float] = Field(None, gt=0, description="Price for limit or stop orders")
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description="Time the order was created")
    status: str = Field("open", description="Current status of the order (e.g., open, filled, canceled)")
    filled_amount: float = Field(0.0, ge=0, description="Amount of the order that has been filled")
    average_fill_price: Optional[float] = Field(None, description="Average price at which the order was filled")
    exchange_id: Optional[str] = Field(None, description="Order ID from the exchange")

class Trade(BaseModel):
    """
    Represents an executed trade.
    """
    id: str = Field(..., description="Unique trade identifier")
    symbol: str = Field(..., description="Trading symbol, e.g., BTC/USD")
    entry_order_id: str = Field(..., description="ID of the order that opened the trade")
    exit_order_id: Optional[str] = Field(None, description="ID of the order that closed the trade")
    side: OrderSide = Field(..., description="Trade side (buy/long or sell/short)")
    entry_price: float = Field(..., gt=0, description="Price at which the trade was entered")
    exit_price: Optional[float] = Field(None, description="Price at which the trade was exited (must be >0 if set)")
    amount: float = Field(..., gt=0, description="Quantity of the asset traded")
    entry_timestamp: datetime = Field(..., description="Time the trade was entered")
    exit_timestamp: Optional[datetime] = Field(None, description="Time the trade was exited")
    pnl: Optional[float] = Field(None, description="Profit or Loss for the trade")
    pnl_percentage: Optional[float] = Field(None, description="Profit or Loss percentage for the trade")
    commission: float = Field(0.0, ge=0, description="Trading commission paid")
    result: Optional[TradeResult] = Field(None, description="Outcome of the trade (Win/Loss/Breakeven)")
    notes: Optional[str] = Field(None, description="Any notes related to the trade")
    # Added for ATR based stops or other context
    custom_fields: Dict[str, Any] = Field(default_factory=dict, description="Custom fields for additional trade data, e.g., atr_at_entry")

    @pydantic.field_validator('exit_price')
    def exit_price_must_be_positive(cls, v):
        if v is not None and v <= 0:
            raise ValueError('exit_price must be positive if set')
        return v

class NewsArticle(BaseModel):
    """
    Represents a news article relevant to market analysis.
    """
    id: str = Field(..., description="Unique identifier for the news article (e.g., URL hash or URL itself)")
    url: str = Field(..., description="Source URL of the article")
    title: str = Field(..., description="Headline or title of the article")
    publication_date: Optional[datetime] = Field(None, description="Date the article was published (UTC)")
    retrieval_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description="Date the article was retrieved (UTC)")
    source: str = Field(..., description="Source of the news (e.g., CoinDesk, CoinTelegraph, RSS feed name)")
    content: Optional[str] = Field(None, description="Full text content of the article")
    summary: Optional[str] = Field(None, description="AI-generated or scraped summary")
    
    # Sentiment related fields
    sentiment_score: Optional[float] = Field(None, description="Overall sentiment score (-1.0 to 1.0)")
    sentiment_label: Optional[str] = Field(None, description="Sentiment label (e.g., positive, negative, neutral, bullish, bearish)")
    sentiment_confidence: Optional[float] = Field(None, description="Confidence of the sentiment analysis (0.0 to 1.0)")
    key_themes: Optional[List[str]] = Field(default_factory=list, description="Key themes identified by sentiment analysis")
    
    related_symbols: Optional[List[str]] = Field(default_factory=list, description="Cryptocurrencies mentioned or related")
    raw_llm_response: Optional[Dict[str, Any]] = Field(None, description="Raw response from LLM for sentiment if available")


class PortfolioSnapshot(BaseModel):
    # ... (no changes from existing)
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    total_value_usd: float = Field(..., description="Total portfolio value in USD")
    cash_balance_usd: float = Field(..., description="Available cash in USD")
    positions: Dict[str, float] = Field(default_factory=dict, description="Asset quantities, e.g., {'BTC': 0.5, 'ETH': 10}") # symbol: quantity
    open_pnl_usd: float = Field(0.0, description="Total open Profit/Loss in USD for current positions")


class PairTrade(BaseModel):
    """
    Represents a pair trade involving two assets.
    Phase 6 Model.
    """
    id: str = Field(..., description="Unique identifier for the pair trade")
    asset1_symbol: str = Field(..., description="Symbol of the first asset in the pair")
    asset2_symbol: str = Field(..., description="Symbol of the second asset in the pair")
    
    asset1_trade_id: Optional[str] = Field(None, description="Trade ID for the leg involving asset1")
    asset2_trade_id: Optional[str] = Field(None, description="Trade ID for the leg involving asset2")

    entry_timestamp: datetime = Field(..., description="Time the pair trade was initiated")
    exit_timestamp: Optional[datetime] = Field(None, description="Time the pair trade was closed")
    
    # Spread details at entry
    entry_spread: float = Field(..., description="Spread value at the time of entry")
    entry_zscore: Optional[float] = Field(None, description="Z-score of the spread at entry")
    
    # Spread details at exit
    exit_spread: Optional[float] = Field(None, description="Spread value at the time of exit")
    exit_zscore: Optional[float] = Field(None, description="Z-score of the spread at exit")

    # P&L for the combined pair trade
    pnl: Optional[float] = Field(None, description="Overall Profit or Loss for the pair trade")
    pnl_percentage: Optional[float] = Field(None, description="Overall Profit or Loss percentage for the pair trade")
    
    total_commission: float = Field(0.0, ge=0, description="Total commission for both legs of the pair trade")
    status: str = Field("open", description="Status of the pair trade (e.g., open, closed)")
    exit_reason: Optional[str] = Field(None, description="Reason for closing the pair trade (e.g., spread reversion, stop loss)")
    notes: Optional[str] = Field(None, description="Any notes related to the pair trade")
    custom_fields: Dict[str, Any] = Field(default_factory=dict, description="Custom fields for additional pair trade data")
</code>

kamikaze_komodo/core/utils.py:
<code>
from datetime import datetime, timezone
from kamikaze_komodo.core.models import BarData
def format_timestamp(ts: datetime, fmt: str = "%Y-%m-%d %H:%M:%S %Z") -> str:
    """
    Formats a datetime object into a string.
    Ensures timezone awareness, defaulting to UTC if naive.
    """
    if ts.tzinfo is None:
        ts = ts.replace(tzinfo=timezone.utc)
    return ts.strftime(fmt)
def current_timestamp_ms() -> int:
    """
    Returns the current UTC timestamp in milliseconds.
    """
    return int(datetime.now(timezone.utc).timestamp() * 1000)
def ohlcv_to_bardata(ohlcv: list, symbol: str, timeframe: str) -> BarData:
    """
    Converts a CCXT OHLCV list [timestamp_ms, open, high, low, close, volume]
    to a BarData object.
    """
    from kamikaze_komodo.core.models import BarData # Local import to avoid circular dependency
    
    if len(ohlcv) != 6:
        raise ValueError("OHLCV list must contain 6 elements: timestamp, open, high, low, close, volume")
    dt_object = datetime.fromtimestamp(ohlcv[0] / 1000, tz=timezone.utc)
    return BarData(
        timestamp=dt_object,
        open=float(ohlcv[1]),
        high=float(ohlcv[2]),
        low=float(ohlcv[3]),
        close=float(ohlcv[4]),
        volume=float(ohlcv[5]),
        symbol=symbol,
        timeframe=timeframe
    )
# Add other utility functions as needed, e.g.,
# - Mathematical helpers not in TA-Lib
# - Data validation functions
# - etc.
</code>

kamikaze_komodo/core/__init__.py:
<code>
# kamikaze_komodo/core/__init__.py
# This file makes the 'core' directory a Python package.
</code>

kamikaze_komodo/data_handling/database_manager.py:
<code>
# kamikaze_komodo/data_handling/database_manager.py
# Updated to include store/retrieve for NewsArticle
# Phase 6: Minor modification to bar_data table to include market_regime.
import sqlite3
from typing import List, Optional, Dict, Any # Added Dict, Any
from kamikaze_komodo.core.models import BarData, NewsArticle # Added NewsArticle
from kamikaze_komodo.app_logger import get_logger
from datetime import datetime, timezone, UTC 
import json # For storing dicts/lists like related_symbols or key_themes

logger = get_logger(__name__)

class DatabaseManager:
    """
    Manages local storage of data (initially SQLite).
    Timestamps are stored as ISO 8601 TEXT.
    Lists/Dicts are stored as JSON TEXT.
    """
    def __init__(self, db_name: str = "kamikaze_komodo_data.db"):
        self.db_name = db_name
        self.conn: Optional[sqlite3.Connection] = None
        self._connect()
        self._create_tables()

    def _connect(self):
        try:
            self.conn = sqlite3.connect(self.db_name, detect_types=sqlite3.PARSE_COLNAMES)
            self.conn.row_factory = sqlite3.Row 
            logger.info(f"Successfully connected to database: {self.db_name}")
        except sqlite3.Error as e:
            logger.error(f"Error connecting to database {self.db_name}: {e}")
            self.conn = None

    def _create_tables(self):
        if not self.conn:
            logger.error("Cannot create tables, no database connection.")
            return
        try:
            cursor = self.conn.cursor()
            # BarData Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS bar_data (
                    timestamp TEXT NOT NULL, 
                    symbol TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    open REAL NOT NULL,
                    high REAL NOT NULL,
                    low REAL NOT NULL,
                    close REAL NOT NULL,
                    volume REAL NOT NULL,
                    atr REAL, 
                    sentiment_score REAL,
                    prediction_value REAL, -- Phase 5
                    prediction_confidence REAL, -- Phase 5
                    market_regime INTEGER, -- Phase 6
                    PRIMARY KEY (timestamp, symbol, timeframe)
                )
            """)
            # NewsArticle Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS news_articles (
                    id TEXT PRIMARY KEY,
                    url TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    publication_date TEXT, 
                    retrieval_date TEXT NOT NULL, 
                    source TEXT NOT NULL,
                    content TEXT,
                    summary TEXT,
                    sentiment_score REAL,
                    sentiment_label TEXT,
                    sentiment_confidence REAL,
                    key_themes TEXT,
                    related_symbols TEXT,
                    raw_llm_response TEXT 
                )
            """)
            # Trades Table (Example for future use)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS trades (
                    id TEXT PRIMARY KEY, symbol TEXT NOT NULL, entry_order_id TEXT, exit_order_id TEXT,
                    side TEXT NOT NULL, entry_price REAL NOT NULL, exit_price REAL, amount REAL NOT NULL,
                    entry_timestamp TEXT NOT NULL, exit_timestamp TEXT, pnl REAL, pnl_percentage REAL,
                    commission REAL DEFAULT 0.0, result TEXT, notes TEXT, custom_fields TEXT
                )
            """)
            self.conn.commit()
            logger.info("Tables checked/created successfully (timestamps as TEXT, complex fields as JSON TEXT).")
        except sqlite3.Error as e:
            logger.error(f"Error creating tables: {e}")

    def _to_iso_format(self, dt: Optional[datetime]) -> Optional[str]:
        if dt is None: return None
        if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:
            dt = dt.replace(tzinfo=UTC)
        else:
            dt = dt.astimezone(UTC)
        return dt.isoformat()

    def _from_iso_format(self, iso_str: Optional[str]) -> Optional[datetime]:
        if iso_str is None: return None
        try:
            dt = datetime.fromisoformat(iso_str)
            if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:
                return dt.replace(tzinfo=UTC)
            return dt.astimezone(UTC)
        except ValueError:
            logger.warning(f"Could not parse ISO timestamp string: {iso_str}")
            return None

    def store_bar_data(self, bar_data_list: List[BarData]):
        if not self.conn: logger.error("No DB connection for bar data."); return False
        if not bar_data_list: logger.info("No bar data to store."); return True
        try:
            cursor = self.conn.cursor()
            data_to_insert = [
                (
                    self._to_iso_format(bd.timestamp), bd.symbol, bd.timeframe,
                    bd.open, bd.high, bd.low, bd.close, bd.volume,
                    bd.atr, bd.sentiment_score,
                    bd.prediction_value, bd.prediction_confidence, # Phase 5
                    bd.market_regime # Phase 6
                ) for bd in bar_data_list
            ]
            cursor.executemany("""
                INSERT OR REPLACE INTO bar_data 
                (timestamp, symbol, timeframe, open, high, low, close, volume, atr, sentiment_score,
                 prediction_value, prediction_confidence, market_regime)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?) 
            """, data_to_insert) 
            self.conn.commit()
            logger.info(f"Stored/Replaced {len(data_to_insert)} bar data entries. ({cursor.rowcount} affected)")
            return True
        except Exception as e:
            logger.error(f"Error storing bar data: {e}", exc_info=True); return False

    def retrieve_bar_data(self, symbol: str, timeframe: str, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> List[BarData]:
        if not self.conn: logger.error("No DB connection for bar data."); return []
        try:
            cursor = self.conn.cursor()
            query = "SELECT timestamp, open, high, low, close, volume, symbol, timeframe, atr, sentiment_score, prediction_value, prediction_confidence, market_regime FROM bar_data WHERE symbol = ? AND timeframe = ?"
            params = [symbol, timeframe]
            if start_date: query += " AND timestamp >= ?"; params.append(self._to_iso_format(start_date))
            if end_date: query += " AND timestamp <= ?"; params.append(self._to_iso_format(end_date))
            query += " ORDER BY timestamp ASC"

            cursor.execute(query, tuple(params))
            rows = cursor.fetchall()
            bar_data_list = []
            for row in rows:
                dt_object = self._from_iso_format(row['timestamp'])
                if not dt_object: continue
                bar_data_list.append(BarData(
                    timestamp=dt_object, open=row['open'], high=row['high'], low=row['low'],
                    close=row['close'], volume=row['volume'], symbol=row['symbol'], timeframe=row['timeframe'],
                    atr=row['atr'], sentiment_score=row['sentiment_score'],
                    prediction_value=row['prediction_value'], prediction_confidence=row['prediction_confidence'], # Phase 5
                    market_regime=row['market_regime'] # Phase 6
                ))
            logger.info(f"Retrieved {len(bar_data_list)} bar data entries for {symbol} ({timeframe}).")
            return bar_data_list
        except Exception as e:
            logger.error(f"Error retrieving bar data for {symbol} ({timeframe}): {e}", exc_info=True); return []

    def store_news_articles(self, articles: List[NewsArticle]):
        if not self.conn: logger.error("No DB connection for news articles."); return False
        if not articles: logger.info("No news articles to store."); return True
        try:
            cursor = self.conn.cursor()
            data_to_insert = []
            for article in articles:
                data_to_insert.append((
                    article.id, article.url, article.title,
                    self._to_iso_format(article.publication_date),
                    self._to_iso_format(article.retrieval_date),
                    article.source, article.content, article.summary,
                    article.sentiment_score, article.sentiment_label,
                    article.sentiment_confidence,
                    json.dumps(article.key_themes) if article.key_themes else None,
                    json.dumps(article.related_symbols) if article.related_symbols else None,
                    json.dumps(article.raw_llm_response) if article.raw_llm_response else None
                ))
            
            cursor.executemany("""
                INSERT OR REPLACE INTO news_articles
                (id, url, title, publication_date, retrieval_date, source, content, summary,
                 sentiment_score, sentiment_label, sentiment_confidence, key_themes, related_symbols, raw_llm_response)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, data_to_insert)
            self.conn.commit()
            logger.info(f"Stored/Replaced {len(data_to_insert)} news articles. ({cursor.rowcount} affected)")
            return True
        except Exception as e:
            logger.error(f"Error storing news articles: {e}", exc_info=True); return False

    def retrieve_news_articles(self, symbol: Optional[str] = None, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None, source: Optional[str] = None, limit: int = 100) -> List[NewsArticle]:
        if not self.conn: logger.error("No DB connection for news articles."); return []
        try:
            cursor = self.conn.cursor()
            query = "SELECT * FROM news_articles WHERE 1=1"
            params = []

            if symbol: # Search in related_symbols (requires LIKE or a better FTS setup)
                query += " AND related_symbols LIKE ?"
                params.append(f'%"{symbol}"%') # Simple JSON array search, not very efficient
            if start_date: # Based on publication_date
                query += " AND publication_date >= ?"
                params.append(self._to_iso_format(start_date))
            if end_date:
                query += " AND publication_date <= ?"
                params.append(self._to_iso_format(end_date))
            if source:
                query += " AND source = ?"
                params.append(source)
            
            query += " ORDER BY publication_date DESC, retrieval_date DESC LIMIT ?"
            params.append(limit)

            cursor.execute(query, tuple(params))
            rows = cursor.fetchall()
            articles_list = []
            for row in rows:
                articles_list.append(NewsArticle(
                    id=row['id'], url=row['url'], title=row['title'],
                    publication_date=self._from_iso_format(row['publication_date']),
                    retrieval_date=self._from_iso_format(row['retrieval_date']),
                    source=row['source'], content=row['content'], summary=row['summary'],
                    sentiment_score=row['sentiment_score'], sentiment_label=row['sentiment_label'],
                    sentiment_confidence=row['sentiment_confidence'],
                    key_themes=json.loads(row['key_themes']) if row['key_themes'] else [],
                    related_symbols=json.loads(row['related_symbols']) if row['related_symbols'] else [],
                    raw_llm_response=json.loads(row['raw_llm_response']) if row['raw_llm_response'] else None
                ))
            logger.info(f"Retrieved {len(articles_list)} news articles with given criteria.")
            return articles_list
        except Exception as e:
            logger.error(f"Error retrieving news articles: {e}", exc_info=True); return []

    def close(self):
        if self.conn: self.conn.close(); logger.info("Database connection closed."); self.conn = None

    def __del__(self): self.close()
</code>

kamikaze_komodo/data_handling/data_fetcher.py:
<code>
# kamikaze_komodo/data_handling/data_fetcher.py
import ccxt.async_support as ccxt # Use async version for future compatibility
import asyncio
from typing import List, Optional, Tuple
from datetime import datetime, timedelta, timezone
# Assuming these are correctly located relative to this file for your project structure
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.core.utils import ohlcv_to_bardata
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Ensure settings is loaded globally

logger = get_logger(__name__)

class DataFetcher:
    """
    Fetches historical and real-time market data using CCXT.
    Phase 6: Added fetch_historical_data_for_pair for pair trading strategies.
    """
    def __init__(self): # MODIFIED: No longer takes exchange_id as an argument
        if not settings:
            logger.critical("Settings not loaded. DataFetcher cannot be initialized.")
            raise ValueError("Settings not loaded. Ensure config files are present and correct.")

        self.exchange_id = settings.exchange_id_to_use # MODIFIED: Get from global settings
        exchange_class = getattr(ccxt, self.exchange_id, None)
        
        if not exchange_class:
            logger.error(f"Exchange '{self.exchange_id}' is not supported by CCXT.")
            raise ValueError(f"Exchange '{self.exchange_id}' is not supported by CCXT.")

        # API keys should be specific to the selected exchange_id 
        # (e.g., Kraken Spot keys for 'kraken', Kraken Futures Demo keys for 'krakenfutures')
        config = {
            'apiKey': settings.kraken_api_key, # This assumes kraken_api_key holds the relevant key
            'secret': settings.kraken_secret_key, # This assumes kraken_secret_key holds the relevant secret
            'enableRateLimit': True, # Recommended by CCXT
        }
        
        # Example: If your settings had distinct keys for different exchanges:
        # if self.exchange_id == 'krakenfutures':
        #     config['apiKey'] = settings.kraken_futures_api_key 
        #     config['secret'] = settings.kraken_futures_secret_key
        # elif self.exchange_id == 'kraken':
        #     config['apiKey'] = settings.kraken_spot_api_key
        #     config['secret'] = settings.kraken_spot_secret_key
        # For now, we use the general kraken_api_key/secret from settings.

        self.exchange = exchange_class(config)
        logger.info(f"Instantiated CCXT exchange class: {self.exchange_id}")

        if settings.kraken_testnet: # This flag now controls sandbox mode for the selected exchange
            if hasattr(self.exchange, 'set_sandbox_mode') and callable(self.exchange.set_sandbox_mode):
                try:
                    self.exchange.set_sandbox_mode(True)
                    logger.info(f"CCXT sandbox mode successfully enabled for {self.exchange_id}.")
                    # You can log the API URL to verify it changed, e.g.:
                    # logger.info(f"Using API URLs: {self.exchange.urls['api']}")
                except ccxt.NotSupported:
                    logger.warning(f"{self.exchange_id} does not support unified set_sandbox_mode via method. Testnet functionality might depend on specific API keys or default URLs for this exchange class.")
                except Exception as e:
                    logger.error(f"An error occurred while trying to set sandbox mode for {self.exchange_id}: {e}", exc_info=True)
            else:
                logger.warning(f"{self.exchange_id} CCXT class does not have a 'set_sandbox_mode' method. Testnet operation relies on correct API keys for the test environment and default URLs.")
        
        self.exchange.verbose = True # Essential for debugging API calls
        logger.info(f"Initialized DataFetcher for '{self.exchange_id}'. Configured Testnet (Sandbox) from settings: {settings.kraken_testnet}")

    async def fetch_historical_ohlcv(
        self,
        symbol: str,
        timeframe: str,
        since: Optional[datetime] = None,
        limit: Optional[int] = None,
        params: Optional[dict] = None
    ) -> List[BarData]:
        if not self.exchange.has['fetchOHLCV']:
            logger.error(f"{self.exchange_id} does not support fetchOHLCV.")
            # await self.close() # Closing here might be premature if other operations are pending
            return []

        since_timestamp_ms = None
        if since:
            if since.tzinfo is None: 
                since = since.replace(tzinfo=timezone.utc)
            since_timestamp_ms = int(since.timestamp() * 1000)

        ohlcv_data_list: List[BarData] = []
        try:
            logger.info(f"Fetching historical OHLCV for {symbol} ({timeframe}) from exchange {self.exchange_id} since {since} with limit {limit}")
            raw_ohlcv = await self.exchange.fetch_ohlcv(symbol, timeframe, since_timestamp_ms, limit, params or {})            
            # More robust check for raw_ohlcv
            if raw_ohlcv is not None and isinstance(raw_ohlcv, list):
                if not raw_ohlcv: # Empty list
                    logger.info(f"No OHLCV data returned (empty list) for {symbol} ({timeframe}) from {self.exchange_id} with the given parameters.")
                else:
                    for entry in raw_ohlcv:
                        try:
                            bar = ohlcv_to_bardata(entry, symbol, timeframe)
                            ohlcv_data_list.append(bar)
                        except ValueError as e_bar:
                            logger.warning(f"Skipping invalid OHLCV entry for {symbol} ({timeframe}): {entry}. Error: {e_bar}")
                    logger.info(f"Successfully fetched {len(ohlcv_data_list)} candles for {symbol} ({timeframe}) from {self.exchange_id}.")
            elif raw_ohlcv is None:
                logger.info(f"No OHLCV data returned (got None) for {symbol} ({timeframe}) from {self.exchange_id} with the given parameters.")
            else: # It's something else, not None and not a list
                logger.warning(f"Unexpected data type received for OHLCV for {symbol} ({timeframe}) from {self.exchange_id}: {type(raw_ohlcv)}. Data: {str(raw_ohlcv)[:200]}")
        except ccxt.NetworkError as e:
            logger.error(f"Network error fetching OHLCV for {symbol} from {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error fetching OHLCV for {symbol} from {self.exchange_id}: {e}", exc_info=True)
        except Exception as e: # Generic catch-all
            logger.error(f"An unexpected error occurred in fetch_historical_ohlcv for {symbol} from {self.exchange_id}: {e}", exc_info=True)
        
        return ohlcv_data_list

    async def fetch_historical_data_for_period(
        self,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime = datetime.now(timezone.utc)
    ) -> List[BarData]:
        all_bars: List[BarData] = []
        current_start_date = start_date
        
        try:
            timeframe_duration_seconds = self.exchange.parse_timeframe(timeframe)
        except Exception as e_tf:
            logger.error(f"Failed to parse timeframe '{timeframe}' using CCXT for {self.exchange_id}: {e_tf}")
            timeframe_duration_seconds = None # Fallback if parse_timeframe itself errors

        if timeframe_duration_seconds is None: # Still None after try-except
            logger.error(f"Could not parse timeframe: {timeframe} for {self.exchange_id}. Cannot paginate effectively. Attempting single fetch.")
            return await self.fetch_historical_ohlcv(symbol, timeframe, since=start_date, limit=1000) # Example limit

        logger.info(f"Fetching historical period data for {symbol} ({timeframe}) on {self.exchange_id} from {start_date} to {end_date}")

        while current_start_date < end_date:
            limit_per_call = 500 # Adjust as needed
            
            logger.debug(f"Fetching batch for {symbol} from {current_start_date} with limit {limit_per_call}")
            bars = await self.fetch_historical_ohlcv(symbol, timeframe, since=current_start_date, limit=limit_per_call)
            
            if not bars: # Includes None or empty list after fetch_historical_ohlcv's logging
                logger.info(f"No more data found for {symbol} ({timeframe}) starting {current_start_date}, or an error occurred during fetch.")
                break 
            
            # Filter bars that are strictly before the overall end_date
            # The timestamp from OHLCV is the start of the candle.
            # If a candle's start is >= end_date, we don't need it or subsequent ones.
            relevant_bars = [b for b in bars if b.timestamp < end_date]
            
            if not relevant_bars:
                if bars and bars[0].timestamp >= end_date: # First fetched bar is already past our period
                    logger.debug(f"First bar fetched ({bars[0].timestamp}) is already at or after end_date ({end_date}). Stopping pagination.")
                break # No relevant bars in this batch

            all_bars.extend(relevant_bars)
            
            # Move to the next period: start after the last fetched relevant candle
            last_fetched_timestamp = relevant_bars[-1].timestamp
            # To get the start of the *next* candle, add the timeframe duration
            current_start_date = last_fetched_timestamp + timedelta(seconds=timeframe_duration_seconds)
            
            if current_start_date >= end_date: # Optimization: if next fetch starts at or after end_date
                logger.debug("Next calculated start_date is at or after end_date. Concluding pagination.")
                break
            
            logger.debug(f"Fetched {len(relevant_bars)} relevant bars. Next fetch for {symbol} will start from {current_start_date}. Total collected: {len(all_bars)}")
            
            # Respect rate limits (ensure rateLimit is a number)
            if isinstance(self.exchange.rateLimit, (int, float)) and self.exchange.rateLimit > 0:
                await asyncio.sleep(self.exchange.rateLimit / 1000.0) 
            else:
                await asyncio.sleep(0.2) # Default small delay if rateLimit is not standard

        # Remove duplicates (if any from overlapping fetches, though logic above tries to avoid it) and sort
        if all_bars:
            unique_bars_dict = {bar.timestamp: bar for bar in all_bars}
            all_bars = sorted(list(unique_bars_dict.values()), key=lambda b: b.timestamp)
            logger.info(f"Total unique historical bars fetched for {symbol} ({timeframe}) in period: {len(all_bars)}")
        
        return all_bars

    async def fetch_historical_data_for_pair(
        self,
        symbol1: str,
        symbol2: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime = datetime.now(timezone.utc)
    ) -> Tuple[Optional[List[BarData]], Optional[List[BarData]]]:
        """
        Fetches historical data for two symbols forming a pair.
        Returns a tuple of (data_symbol1, data_symbol2).
        Data is attempted to be synchronized by timestamp, but perfect sync is not guaranteed
        if one asset has missing bars where the other doesn't.
        Further alignment might be needed in the strategy.
        """
        logger.info(f"Fetching historical data for pair: {symbol1} and {symbol2} ({timeframe}) from {start_date} to {end_date}")
        
        data_symbol1 = await self.fetch_historical_data_for_period(symbol1, timeframe, start_date, end_date)
        data_symbol2 = await self.fetch_historical_data_for_period(symbol2, timeframe, start_date, end_date)

        if not data_symbol1:
            logger.warning(f"No data fetched for {symbol1} in the pair.")
        if not data_symbol2:
            logger.warning(f"No data fetched for {symbol2} in the pair.")
        
        # Basic check for data presence
        if not data_symbol1 or not data_symbol2:
            logger.warning(f"Could not fetch data for one or both assets in the pair ({symbol1}, {symbol2}).")
            return None, None # Indicate failure to fetch for one or both

        # Strategies will need to handle potential misalignments or use pandas to merge/align.
        logger.info(f"Fetched {len(data_symbol1)} bars for {symbol1} and {len(data_symbol2)} bars for {symbol2} for pair trading.")
        return data_symbol1, data_symbol2

    async def subscribe_to_realtime_trades(self, symbol: str):
        # (Your existing placeholder code for this method)
        if not self.exchange.has['watchTrades']:
            logger.warning(f"{self.exchange_id} does not support real-time trade watching via WebSockets in CCXT.")
            # await self.close() # Consider if closing here is always appropriate
            return

        logger.info(f"Attempting to subscribe to real-time trades for {symbol} on {self.exchange_id}...")
        logger.warning("Real-time data subscription is a placeholder and not fully implemented.")
        pass

    async def close(self):
        """Closes the CCXT exchange connection."""
        try:
            if hasattr(self.exchange, 'close') and callable(self.exchange.close):
                await self.exchange.close()
            logger.info(f"CCXT exchange connection for {self.exchange_id} closed.")
        except Exception as e:
            logger.error(f"Error closing CCXT exchange connection for {self.exchange_id}: {e}", exc_info=True)


if __name__ == '__main__':
    # This is just for isolated testing of data_fetcher.
    # In the main application, this would be integrated differently.
    # Ensure settings are loaded before running this.
    # For example, by running from the root kamikaze_komodo directory or adjusting paths.
    # `python -m kamikaze_komodo.data_handling.data_fetcher`
    
    # Need to load settings explicitly if running standalone for testing
    # from kamikaze_komodo.config.settings import Config
    # settings_instance = Config(config_file='../config/config.ini', secrets_file='../config/secrets.ini')
    # global settings # make it available globally in this module for the example
    # settings = settings_instance

    # if settings:
    #     asyncio.run(main_data_fetcher_example())
    # else:
    #     print("Failed to load settings for standalone data_fetcher example.")
    pass
</code>

kamikaze_komodo/data_handling/__init__.py:
<code>
# kamikaze_komodo/data_handling/__init__.py
# This file makes the 'data_handling' directory a Python package.
</code>

kamikaze_komodo/exchange_interaction/exchange_api.py:
<code>
# kamikaze_komodo/exchange_interaction/exchange_api.py
import ccxt.async_support as ccxt
import asyncio
from typing import Dict, Optional, List
from kamikaze_komodo.core.enums import OrderType, OrderSide
from kamikaze_komodo.core.models import Order
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings # Import global settings
from datetime import datetime, timezone # Ensure timezone is imported

logger = get_logger(__name__)

class ExchangeAPI:
    """
    Handles interactions with the cryptocurrency exchange.
    Manages order placement, cancellation, and fetching account information.
    Phase 6: Added explicit check for short selling capability (though CCXT often handles this implicitly for derivative exchanges).
    """
    def __init__(self, exchange_id: Optional[str] = None): # exchange_id is now optional
        if not settings:
            logger.critical("Settings not loaded. ExchangeAPI cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.exchange_id = exchange_id if exchange_id else settings.exchange_id_to_use
        exchange_class = getattr(ccxt, self.exchange_id, None)

        if not exchange_class:
            logger.error(f"Exchange {self.exchange_id} is not supported by CCXT.")
            raise ValueError(f"Exchange {self.exchange_id} is not supported by CCXT.")

        # Determine API keys based on the exchange_id
        # This example assumes a single set of keys in settings (e.g., KRAKEN_API)
        # For a multi-exchange system, you'd fetch keys specific to self.exchange_id
        api_key = settings.kraken_api_key # Defaulting to Kraken keys for now
        secret_key = settings.kraken_secret_key # Defaulting to Kraken keys
        use_testnet = settings.kraken_testnet # Defaulting to Kraken testnet setting

        # Example for specific exchange key loading (if settings were structured differently)
        # if self.exchange_id == 'binance':
        #     api_key = settings.binance_api_key
        #     secret_key = settings.binance_secret_key
        #     use_testnet = settings.binance_testnet
        # elif self.exchange_id == 'krakenfutures':
        #     api_key = settings.kraken_futures_api_key # etc.

        config = {
            'apiKey': api_key,
            'secret': secret_key,
            'enableRateLimit': True,
        }
        self.exchange = exchange_class(config)
        logger.info(f"Initialized ExchangeAPI for {self.exchange_id}.")

        if use_testnet:
            if hasattr(self.exchange, 'set_sandbox_mode') and callable(self.exchange.set_sandbox_mode):
                try:
                    self.exchange.set_sandbox_mode(True)
                    logger.info(f"Sandbox mode enabled for {self.exchange_id}.")
                except ccxt.NotSupported:
                    logger.warning(f"{self.exchange_id} does not support unified set_sandbox_mode. Testnet functionality depends on API keys/URL.")
                except Exception as e_sandbox:
                    logger.error(f"Error setting sandbox mode for {self.exchange_id}: {e_sandbox}")
            else:
                logger.warning(f"{self.exchange_id} does not have set_sandbox_mode. Testnet relies on specific API keys or default URL pointing to sandbox.")
        else:
            logger.info(f"Running in live mode for {self.exchange_id}.")

        if not api_key or "YOUR_API_KEY" in str(api_key).upper() or (isinstance(api_key, str) and "D27PYGI95TLS" in api_key.upper()): # Check specific placeholder
            logger.warning(f"API key for {self.exchange_id} appears to be a placeholder or is not configured. Authenticated calls may fail.")

    async def fetch_balance(self) -> Optional[Dict]:
        if not self.exchange.has['fetchBalance']:
            logger.error(f"{self.exchange_id} does not support fetchBalance.")
            return None
        try:
            balance = await self.exchange.fetch_balance()
            logger.info(f"Successfully fetched balance from {self.exchange_id}.")
            return balance
        except ccxt.NetworkError as e:
            logger.error(f"Network error fetching balance: {e}", exc_info=True)
        except ccxt.AuthenticationError as e_auth:
            logger.error(f"Authentication error fetching balance from {self.exchange_id}. Check API keys and permissions: {e_auth}", exc_info=True)
            return None # Explicitly return None on auth error
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error fetching balance: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred fetching balance: {e}", exc_info=True)
        return None

    async def create_order(
        self,
        symbol: str,
        order_type: OrderType,
        side: OrderSide,
        amount: float,
        price: Optional[float] = None,
        params: Optional[Dict] = None
    ) -> Optional[Order]:
        if order_type == OrderType.LIMIT and price is None:
            logger.error("Price must be specified for a LIMIT order.")
            return None

        if not self.exchange.has['createOrder']:
            logger.error(f"{self.exchange_id} does not support createOrder.")
            return None

        order_type_str = order_type.value
        side_str = side.value

        # Phase 6: Check for short selling specific capabilities (conceptual for CCXT futures)
        if side == OrderSide.SELL: # This could be opening a short or closing a long
            # For many futures exchanges, 'sell' with no existing position implies short.
            # CCXT often handles this implicitly. Some exchanges might need specific params for short.
            # e.g., params = {'reduceOnly': False} if it was to ensure opening a new position.
            # We assume for now that a simple SELL order will open a short if no long position exists.
            # If the exchange has explicit shorting methods (less common in CCXT unified API), that'd be different.
            logger.info(f"Preparing to place a SELL order for {symbol}. This may open a short position.")

        try:
            logger.info(f"Attempting to place {side_str} {order_type_str} order for {amount} {symbol} at price {price if price else 'market'} on {self.exchange_id}")
            
            # Check for placeholder API keys again before actual call
            is_placeholder_key = not self.exchange.apiKey or "YOUR_API_KEY" in self.exchange.apiKey.upper() or "D27PYGI95TLS" in self.exchange.apiKey.upper()
            if settings.kraken_testnet and is_placeholder_key : # Use general testnet flag
                logger.warning(f"Simulating order creation for {self.exchange_id} due to testnet mode and placeholder API keys.")
                simulated_order_id = f"sim_{self.exchange_id}_{ccxt.Exchange.uuid()}"
                return Order(
                    id=simulated_order_id,
                    symbol=symbol,
                    type=order_type,
                    side=side,
                    amount=amount,
                    price=price if order_type == OrderType.LIMIT else None,
                    timestamp=datetime.now(timezone.utc), # Use timezone.utc
                    status="open", # Simulate as open
                    exchange_id=simulated_order_id
                )

            exchange_order_response = await self.exchange.create_order(symbol, order_type_str, side_str, amount, price, params or {})
            logger.info(f"Successfully placed order on {self.exchange_id}. Order ID: {exchange_order_response.get('id')}")
            
            created_order = Order(
                id=str(exchange_order_response.get('id')),
                symbol=exchange_order_response.get('symbol'),
                type=OrderType(exchange_order_response.get('type', order_type_str).lower()),
                side=OrderSide(exchange_order_response.get('side', side_str).lower()),
                amount=float(exchange_order_response.get('amount', amount)),
                price=float(exchange_order_response['price']) if exchange_order_response.get('price') else None,
                timestamp=datetime.fromtimestamp(exchange_order_response['timestamp'] / 1000, tz=timezone.utc) if exchange_order_response.get('timestamp') else datetime.now(timezone.utc),
                status=exchange_order_response.get('status', 'open'),
                filled_amount=float(exchange_order_response.get('filled', 0.0)),
                average_fill_price=float(exchange_order_response.get('average')) if exchange_order_response.get('average') else None,
                exchange_id=str(exchange_order_response.get('id'))
            )
            return created_order

        except ccxt.InsufficientFunds as e:
            logger.error(f"Insufficient funds to place order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.InvalidOrder as e:
            logger.error(f"Invalid order parameters for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.AuthenticationError as e_auth:
            logger.error(f"Authentication error placing order for {symbol} on {self.exchange_id}. Check API keys: {e_auth}", exc_info=True)
        except ccxt.NetworkError as e:
            logger.error(f"Network error placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e: # Catch specific exchange errors before generic Exception
            logger.error(f"Exchange error placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred placing order for {symbol} on {self.exchange_id}: {e}", exc_info=True)
        return None

    async def cancel_order(self, order_id: str, symbol: Optional[str] = None, params: Optional[Dict] = None) -> bool:
        if not self.exchange.has['cancelOrder']:
            logger.error(f"{self.exchange_id} does not support cancelOrder.")
            return False
        try:
            await self.exchange.cancel_order(order_id, symbol, params or {})
            logger.info(f"Successfully requested cancellation for order ID {order_id} on {self.exchange_id}.")
            return True
        except ccxt.OrderNotFound as e:
            logger.error(f"Order ID {order_id} not found for cancellation on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.NetworkError as e:
            logger.error(f"Network error canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        except ccxt.ExchangeError as e:
            logger.error(f"Exchange error canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred canceling order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        return False

    async def fetch_order(self, order_id: str, symbol: Optional[str] = None) -> Optional[Order]:
        if not self.exchange.has['fetchOrder']:
            logger.warning(f"{self.exchange_id} does not support fetching individual orders directly.")
            return None
        try:
            exchange_order_response = await self.exchange.fetch_order(order_id, symbol)
            fetched_order = Order(
                id=str(exchange_order_response.get('id')),
                symbol=exchange_order_response.get('symbol'),
                type=OrderType(exchange_order_response.get('type').lower()),
                side=OrderSide(exchange_order_response.get('side').lower()),
                amount=float(exchange_order_response.get('amount')),
                price=float(exchange_order_response['price']) if exchange_order_response.get('price') else None,
                timestamp=datetime.fromtimestamp(exchange_order_response['timestamp'] / 1000, tz=timezone.utc) if exchange_order_response.get('timestamp') else datetime.now(timezone.utc),
                status=exchange_order_response.get('status'),
                filled_amount=float(exchange_order_response.get('filled', 0.0)),
                average_fill_price=float(exchange_order_response.get('average')) if exchange_order_response.get('average') else None,
                exchange_id=str(exchange_order_response.get('id'))
            )
            return fetched_order
        except ccxt.OrderNotFound:
            logger.warning(f"Order {order_id} not found on {self.exchange_id}.")
        except Exception as e:
            logger.error(f"Error fetching order {order_id} on {self.exchange_id}: {e}", exc_info=True)
        return None

    async def fetch_open_orders(self, symbol: Optional[str] = None, since: Optional[datetime] = None, limit: Optional[int] = None) -> List[Order]: # Changed since to datetime
        open_orders_list = []
        if not self.exchange.has['fetchOpenOrders']:
            logger.warning(f"{self.exchange_id} does not support fetching open orders.")
            return open_orders_list

        try:
            since_timestamp_ms = int(since.timestamp() * 1000) if since else None
            raw_orders = await self.exchange.fetch_open_orders(symbol, since_timestamp_ms, limit)
            for ex_order in raw_orders:
                order = Order(
                    id=str(ex_order.get('id')),
                    symbol=ex_order.get('symbol'),
                    type=OrderType(ex_order.get('type').lower()),
                    side=OrderSide(ex_order.get('side').lower()),
                    amount=float(ex_order.get('amount')),
                    price=float(ex_order['price']) if ex_order.get('price') else None,
                    timestamp=datetime.fromtimestamp(ex_order['timestamp'] / 1000, tz=timezone.utc) if ex_order.get('timestamp') else datetime.now(timezone.utc),
                    status=ex_order.get('status', 'open'),
                    filled_amount=float(ex_order.get('filled', 0.0)),
                    average_fill_price=float(ex_order.get('average')) if ex_order.get('average') else None,
                    exchange_id=str(ex_order.get('id'))
                )
                open_orders_list.append(order)
            logger.info(f"Fetched {len(open_orders_list)} open orders for symbol {symbol if symbol else 'all'} on {self.exchange_id}.")
        except Exception as e:
            logger.error(f"Error fetching open orders on {self.exchange_id}: {e}", exc_info=True)
        return open_orders_list

    async def close(self):
        try:
            if hasattr(self.exchange, 'close') and callable(self.exchange.close):
                await self.exchange.close()
                logger.info(f"CCXT exchange connection for {self.exchange_id} closed.")
        except Exception as e:
            logger.error(f"Error closing CCXT exchange connection for {self.exchange_id}: {e}", exc_info=True)

# Example Usage (run within an asyncio event loop):
async def main_exchange_api_example():
    if not settings:
        print("Settings could not be loaded. Exiting example.")
        return

    exchange_api = ExchangeAPI() # Uses exchange_id from settings
    balance = await exchange_api.fetch_balance()
    if balance:
        logger.info(f"Free USD Balance: {balance.get('USD', {}).get('free', 'N/A')}")
        logger.info(f"Free {settings.default_symbol.split('/')[0]} Balance: {balance.get(settings.default_symbol.split('/')[0], {}).get('free', 'N/A')}")

    # target_symbol = settings.default_symbol
    # order_to_place = await exchange_api.create_order(
    #     symbol=target_symbol,
    #     order_type=OrderType.LIMIT,
    #     side=OrderSide.BUY,
    #     amount=0.0001,
    #     price=15000.0
    # )
    # if order_to_place:
    #     logger.info(f"Practice order placed/simulated: ID {order_to_place.id}, Status {order_to_place.status}")
    # else:
    #     logger.warning("Practice order placement failed or was not attempted.")

    await exchange_api.close()
</code>

kamikaze_komodo/exchange_interaction/__init__.py:
<code>
# kamikaze_komodo/exchange_interaction/__init__.py
# This file makes the 'exchange_interaction' directory a Python package.
</code>

kamikaze_komodo/ml_models/__init__.py:
<code>
# kamikaze_komodo/ml_models/__init__.py
# This file makes the 'ml_models' directory a Python package.
</code>

kamikaze_komodo/ml_models/inference_pipelines/lightgbm_inference.py:
<code>
# kamikaze_komodo/ml_models/inference_pipelines/lightgbm_inference.py
import pandas as pd
from typing import Optional, Union
import os
import numpy as np 
from kamikaze_komodo.ml_models.price_forecasting.lightgbm_forecaster import LightGBMForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings
logger = get_logger(__name__)
class LightGBMInference:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LightGBM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        
        model_params_config = settings.get_strategy_params(model_config_section) 
        
        _model_base_path = model_params_config.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = model_params_config.get('modelfilename', f"lgbm_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        script_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) 
        
        if not os.path.isabs(_model_base_path):
            self.model_load_path_dir = os.path.join(script_dir, _model_base_path)
        else:
            self.model_load_path_dir = _model_base_path
            
        self.model_full_load_path = os.path.join(self.model_load_path_dir, _model_filename)
        self.forecaster = LightGBMForecaster(model_path=self.model_full_load_path, params=model_params_config)
        if self.forecaster.model is None:
            logger.warning(f"LightGBMInference: Model could not be loaded from {self.model_full_load_path}. Predictions will not be available.")
    def get_prediction(self, current_data_history: pd.DataFrame) -> Optional[float]:
        """
        Gets a single prediction based on the current data history.
        Assumes current_data_history has enough data to form features for the last point.
        """
        if self.forecaster.model is None:
            logger.warning("No model loaded, cannot get prediction.")
            return None
        if current_data_history.empty:
            logger.warning("Data history is empty, cannot get prediction.")
            return None
        # The LightGBMForecaster's predict method expects a DataFrame, even for a single prediction.
        # It will use its create_features method, which typically uses lags.
        # So, pass the recent history. `predict` will internally call `create_features`.
        
        # Determine feature columns to use for prediction
        feature_cols_to_pass = None
        # Check if 'feature_columns' is in the forecaster's parameters (e.g., from config)
        feature_cols_str_from_params = self.forecaster.params.get('feature_columns')
        if isinstance(feature_cols_str_from_params, str) and feature_cols_str_from_params:
            feature_cols_to_pass = [col.strip() for col in feature_cols_str_from_params.split(',')]
        elif isinstance(feature_cols_str_from_params, list):
             feature_cols_to_pass = feature_cols_str_from_params
        # If not in params, `LightGBMForecaster.predict` will use `self.trained_feature_columns_` if available.
        # Or, `feature_columns_to_use` can be None if the model should use its internal defaults/trained features.
        prediction_output = self.forecaster.predict(current_data_history, feature_columns_to_use=feature_cols_to_pass)
        
        if prediction_output is None:
            return None
        
        if isinstance(prediction_output, pd.Series):
            if not prediction_output.empty:
                return prediction_output.iloc[-1] 
            else:
                logger.warning("Prediction series is empty.")
                return None
        elif isinstance(prediction_output, (float, np.float64)):
            return float(prediction_output)
        else:
            logger.warning(f"Unexpected prediction output type: {type(prediction_output)}")
            return None
async def main_inference_example():
    from kamikaze_komodo.data_handling.database_manager import DatabaseManager
    from datetime import datetime, timedelta, timezone
    
    if not settings:
        print("Settings not loaded, cannot run LightGBM inference example.")
        return
    symbol_to_predict = settings.default_symbol
    timeframe_to_predict = settings.default_timeframe
    if not settings.config.has_section("LightGBM_Forecaster"):
        logger.error("Config section [LightGBM_Forecaster] not found. Cannot run inference.")
        return
    inference_engine = LightGBMInference(symbol=symbol_to_predict, timeframe=timeframe_to_predict)
    if inference_engine.forecaster.model is None:
        logger.error("Failed to load model for inference. Exiting example.")
        return
    db_manager = DatabaseManager()
    hist_days = 30 
    start_dt = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_dt = datetime.now(timezone.utc)
    
    bars = db_manager.retrieve_bar_data(symbol_to_predict, timeframe_to_predict, start_dt, end_dt)
    db_manager.close()
    if not bars or len(bars) < 20: 
        logger.error(f"Not enough recent data ({len(bars)} bars) to make a prediction example.")
        return
        
    data_df = pd.DataFrame([b.model_dump() for b in bars])
    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
    data_df.set_index('timestamp', inplace=True)
    data_df.sort_index(inplace=True)
    
    logger.info(f"Making prediction for {symbol_to_predict} ({timeframe_to_predict}) using last {len(data_df)} bars.")
    prediction = inference_engine.get_prediction(data_df)
    if prediction is not None:
        logger.info(f"Prediction for {symbol_to_predict} ({timeframe_to_predict}): {prediction:.6f}")
    else:
        logger.warning(f"Could not get a prediction for {symbol_to_predict} ({timeframe_to_predict}).")
if __name__ == "__main__":
    import asyncio
    asyncio.run(main_inference_example())
</code>

kamikaze_komodo/ml_models/inference_pipelines/xgboost_classifier_inference.py:
<code>
# FILE: kamikaze_komodo/ml_models/inference_pipelines/xgboost_classifier_inference.py
import pandas as pd
from typing import Optional, Dict, Any
import os
import numpy as np
from kamikaze_komodo.ml_models.price_forecasting.xgboost_classifier_forecaster import XGBoostClassifierForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings

logger = get_logger(__name__)

class XGBoostClassifierInference:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "XGBoost_Classifier_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        
        model_params_config = settings.get_strategy_params(model_config_section)
        
        _model_base_path = model_params_config.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = model_params_config.get('modelfilename', f"xgb_classifier_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        script_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) 
        
        if not os.path.isabs(_model_base_path):
            self.model_load_path_dir = os.path.join(script_dir, _model_base_path)
        else:
            self.model_load_path_dir = _model_base_path
            
        self.model_full_load_path = os.path.join(self.model_load_path_dir, _model_filename)
        self.forecaster = XGBoostClassifierForecaster(model_path=self.model_full_load_path, params=model_params_config)
        
        if self.forecaster.model is None:
            logger.warning(f"XGBoostClassifierInference: Model could not be loaded from {self.model_full_load_path}. Predictions will not be available.")

    def get_prediction(self, current_data_history: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        Gets a single classification prediction based on the current data history.
        Returns a dictionary with 'predicted_class', 'confidence', and 'probabilities'.
        """
        if self.forecaster.model is None:
            logger.warning("XGBoost model not loaded, cannot get prediction.")
            return None
        if current_data_history.empty:
            logger.warning("Data history for XGBoost prediction is empty.")
            return None
            
        prediction_output = self.forecaster.predict(current_data_history) # XGBoostClassifierForecaster.predict returns a dict
        
        if prediction_output and isinstance(prediction_output, dict):
            return prediction_output
        else:
            logger.warning(f"Unexpected prediction output type from XGBoost forecaster: {type(prediction_output)}")
            return None

async def main_xgboost_inference_example():
    from kamikaze_komodo.data_handling.database_manager import DatabaseManager
    from datetime import datetime, timedelta, timezone
    
    if not settings:
        print("Settings not loaded, cannot run XGBoost Classifier inference example.")
        return
        
    symbol_to_predict = settings.default_symbol
    timeframe_to_predict = settings.default_timeframe
    
    if not settings.config.has_section("XGBoost_Classifier_Forecaster"):
        logger.error("Config section [XGBoost_Classifier_Forecaster] not found. Cannot run inference example.")
        return
        
    inference_engine = XGBoostClassifierInference(symbol=symbol_to_predict, timeframe=timeframe_to_predict)
    if inference_engine.forecaster.model is None:
        logger.error("Failed to load XGBoost model for inference. Exiting example.")
        return
        
    db_manager = DatabaseManager()
    hist_days = 30 
    start_dt = datetime.now(timezone.utc) - timedelta(days=hist_days)
    end_dt = datetime.now(timezone.utc)
    
    bars = db_manager.retrieve_bar_data(symbol_to_predict, timeframe_to_predict, start_dt, end_dt)
    db_manager.close()
    
    if not bars or len(bars) < 50: # Need enough for feature generation
        logger.error(f"Not enough recent data ({len(bars)} bars) to make an XGBoost prediction example.")
        return
        
    data_df = pd.DataFrame([b.model_dump() for b in bars])
    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
    data_df.set_index('timestamp', inplace=True)
    data_df.sort_index(inplace=True)
    
    logger.info(f"Making XGBoost prediction for {symbol_to_predict} ({timeframe_to_predict}) using last {len(data_df)} bars.")
    prediction_dict = inference_engine.get_prediction(data_df)
    
    if prediction_dict:
        logger.info(f"XGBoost Prediction for {symbol_to_predict} ({timeframe_to_predict}): {prediction_dict}")
    else:
        logger.warning(f"Could not get an XGBoost prediction for {symbol_to_predict} ({timeframe_to_predict}).")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main_xgboost_inference_example())
</code>

kamikaze_komodo/ml_models/inference_pipelines/__init__.py:
<code>
# kamikaze_komodo/ml_models/inference_pipelines/__init__.py
# This file makes the 'inference_pipelines' directory a Python package.
</code>

kamikaze_komodo/ml_models/price_forecasting/base_forecaster.py:
<code>
# kamikaze_komodo/ml_models/price_forecasting/base_forecaster.py
from abc import ABC, abstractmethod
import pandas as pd
from typing import Optional, Dict, Any, Union
from kamikaze_komodo.app_logger import get_logger
logger = get_logger(__name__)
class BasePriceForecaster(ABC):
    """
    Abstract base class for price forecasting models.
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        self.model_path = model_path
        self.params = params if params is not None else {}
        self.model: Any = None
        if model_path:
            self.load_model(model_path)
        logger.info(f"{self.__class__.__name__} initialized with model_path: {model_path}, params: {self.params}")
    @abstractmethod
    def train(self, historical_data: pd.DataFrame, target_column: str = 'close', feature_columns: Optional[list] = None):
        """
        Trains the forecasting model.
        Args:
            historical_data (pd.DataFrame): DataFrame with historical OHLCV and feature data.
            target_column (str): The name of the column to predict.
            feature_columns (Optional[list]): List of column names to be used as features. If None, uses defaults.
        """
        pass
    @abstractmethod
    def predict(self, new_data: pd.DataFrame, feature_columns: Optional[list] = None) -> Union[pd.Series, float, None]:
        """
        Makes predictions on new data.
        Args:
            new_data (pd.DataFrame): DataFrame with the latest data for prediction.
                                     For bar-by-bar, this might be a single row or a lookback window.
            feature_columns (Optional[list]): List of column names to be used as features, must match training.
        Returns:
            Union[pd.Series, float, None]: Predicted value(s) or None if prediction fails.
                                           Could be a series for multi-step or single float for next step.
        """
        pass
    @abstractmethod
    def save_model(self, path: str):
        """
        Saves the trained model to the specified path.
        """
        pass
    @abstractmethod
    def load_model(self, path: str):
        """
        Loads a trained model from the specified path.
        """
        pass
    @abstractmethod
    def create_features(self, data: pd.DataFrame, feature_columns: Optional[list] = None) -> pd.DataFrame:
        """
        Creates features for the model from raw data.
        """
        pass
</code>

kamikaze_komodo/ml_models/price_forecasting/lightgbm_forecaster.py:
<code>
# kamikaze_komodo/ml_models/price_forecasting/lightgbm_forecaster.py
import lightgbm as lgb
import pandas as pd
import numpy as np
import joblib # For saving/loading model
from typing import Optional, Dict, Any, List, Union
from kamikaze_komodo.ml_models.price_forecasting.base_forecaster import BasePriceForecaster
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings as app_settings # Use app_settings to avoid conflict
logger = get_logger(__name__)
class LightGBMForecaster(BasePriceForecaster):
    """
    LightGBM-based price forecaster.
    Predicts price movement (e.g., next bar's close relative to current).
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        super().__init__(model_path, params) # This calls load_model if model_path is provided
        self.default_lgbm_params = {
            'objective': 'regression_l1', # Or 'regression_l2'
            'metric': 'rmse', # Root Mean Squared Error
            'n_estimators': 100,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'n_jobs': -1,
            'seed': 42,
            'boosting_type': 'gbdt',
        }
        # Update with any params passed from config for 'lgbm_params' specifically
        config_lgbm_params = {}
        if self.params: # self.params comes from config section like [LightGBM_Forecaster]
            for key, value in self.params.items():
                if key.startswith('lgbm_params_'): # e.g. lgbm_params_n_estimators
                    param_name = key.replace('lgbm_params_', '').lower()
                    config_lgbm_params[param_name] = value
        
        self.lgbm_params = {**self.default_lgbm_params, **config_lgbm_params}
        # If model_path was passed to super() and model loaded, self.model is set.
        # If model_path is in params (e.g. from config) but not passed directly to init, load it.
        if not self.model and self.model_path: # self.model_path is set by super if path given
             self.load_model(self.model_path)
        elif not self.model and self.params.get('modelfilename'): # Check if model path is in params from config
            # Construct full path if model_path is not set by direct argument to __init__
            # This logic is typically handled by the training/inference pipeline that instantiates this.
            # For direct use, ensure model_path is passed or params correctly configure it.
            pass
    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Creates a standard set of known features from the input data.
        This method generates a superset of features; selection for training/prediction happens elsewhere.
        """
        if data.empty:
            logger.warning("Data for feature creation is empty.")
            return pd.DataFrame()
        df = data.copy() 
        if 'close' not in df.columns:
            logger.error("'close' column not found in data for feature creation.")
            return df 
        for lag in [1, 3, 5, 10]:
            df[f'log_return_lag_{lag}'] = np.log(df['close'] / df['close'].shift(lag))
            df[f'close_change_lag_{lag}'] = df['close'].pct_change(lag)
        
        if 'log_return_lag_1' in df.columns: # Check if base lag feature was created
            df['volatility_5'] = df['log_return_lag_1'].rolling(window=5).std()
            df['volatility_10'] = df['log_return_lag_1'].rolling(window=10).std()
        else:
            df['volatility_5'] = np.nan
            df['volatility_10'] = np.nan
        if all(col in df.columns for col in ['high', 'low', 'close']):
            try:
                import pandas_ta as ta
                df.ta.rsi(close=df['close'], length=14, append=True, col_names=('RSI_14',))
                df.ta.macd(close=df['close'], append=True, col_names=('MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9'))
            except ImportError:
                logger.warning("pandas_ta not installed. Skipping TA features for LightGBM.")
            except Exception as e_ta: 
                logger.warning(f"Error during pandas_ta feature creation: {e_ta}. TA features might be missing or incomplete.")
        else:
            logger.warning("Missing 'high', 'low', or 'close' columns. Skipping TA features.")
        df = df.replace([np.inf, -np.inf], np.nan)
        return df # Return DataFrame with ALL generated features, NaNs from shifts/calcs are expected here
    def train(self, historical_data: pd.DataFrame, target_column: str = 'close_change_lag_1_future', feature_columns_to_use: Optional[List[str]] = None):
        logger.info(f"Starting LightGBM training for target '{target_column}'. Data shape: {historical_data.shape}")
        df = historical_data.copy()
        if target_column == 'close_change_lag_1_future':
            df['target'] = (df['close'].shift(-1) / df['close']) - 1
        elif target_column.startswith('log_return_lag_') and target_column.endswith('_future'):
            try:
                shift_val = int(target_column.split('_')[3])
                df['target'] = np.log(df['close'].shift(-shift_val) / df['close'])
            except Exception:
                logger.error(f"Could not parse shift value from target_column: {target_column}. Using default log return shift -1.")
                df['target'] = np.log(df['close'].shift(-1) / df['close']) # Default to next bar log return
        else:
            if target_column not in df.columns:
                logger.error(f"Target column '{target_column}' not in data or not a recognized dynamic target format.")
                return
            df['target'] = df[target_column]
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(subset=['target'], inplace=True) 
        df_with_all_features = self.create_features(df) 
        
        X_final_features = pd.DataFrame()
        if feature_columns_to_use:
            actual_features_present = [col for col in feature_columns_to_use if col in df_with_all_features.columns]
            missing = set(feature_columns_to_use) - set(actual_features_present)
            if missing:
                logger.warning(f"During training, specified feature_columns not all found/generated: {missing}. Using available: {actual_features_present}")
            if not actual_features_present:
                 logger.error("None of the specified_feature_columns_to_use are present after generation. Cannot train.")
                 return
            X_final_features = df_with_all_features[actual_features_present].copy()
        else:
            # Default: use a predefined list of potentially generated features
            default_feature_set = [
                col for col in df_with_all_features.columns if col.startswith('log_return_lag_') or \
                col.startswith('close_change_lag_') or \
                col.startswith('volatility_') or \
                col in ['RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9']
            ]
            # Filter this default set by what's actually in df_with_all_features
            default_features_present = [col for col in default_feature_set if col in df_with_all_features.columns]
            if not default_features_present:
                logger.error("No default features could be found/generated. Cannot train.")
                return
            X_final_features = df_with_all_features[default_features_present].copy()
        
        X_final_features.dropna(inplace=True) 
        y_train = df.loc[X_final_features.index, 'target'] 
        X_train = X_final_features
        if X_train.empty or y_train.empty:
            logger.error("Feature matrix X_train or target vector y_train is empty after processing. Training cannot proceed.")
            return
        self.model = lgb.LGBMRegressor(**self.lgbm_params)
        logger.info(f"Training LightGBM model with {len(X_train)} samples. Features: {list(X_train.columns)}")
        try:
            self.model.fit(X_train, y_train)
            logger.info("LightGBM model training completed.")
            self.trained_feature_columns_ = list(X_train.columns) # Store actual columns used
        except Exception as e:
            logger.error(f"Error during LightGBM model training: {e}", exc_info=True)
            self.model = None
    def predict(self, new_data: pd.DataFrame, feature_columns_to_use: Optional[list] = None) -> Union[pd.Series, float, None]:
        if self.model is None:
            logger.error("Model not loaded or trained. Cannot make predictions.")
            return None
        
        df_with_all_features = self.create_features(new_data)
        cols_for_prediction = None
        # Priority: 1. Explicitly passed `feature_columns_to_use`, 2. `self.trained_feature_columns_`
        if feature_columns_to_use:
            cols_for_prediction = [col for col in feature_columns_to_use if col in df_with_all_features.columns]
            missing_explicit = set(feature_columns_to_use) - set(cols_for_prediction)
            if missing_explicit: logger.warning(f"Explicitly requested prediction features not found: {missing_explicit}")
        elif hasattr(self, 'trained_feature_columns_') and self.trained_feature_columns_:
            cols_for_prediction = [col for col in self.trained_feature_columns_ if col in df_with_all_features.columns]
            missing_trained = set(self.trained_feature_columns_) - set(cols_for_prediction)
            if missing_trained: logger.warning(f"Features model was trained on are not all available for prediction: {missing_trained}")
        
        if not cols_for_prediction:
            logger.error("No feature columns determined for prediction. Make sure model is trained or features are specified and generatable.")
            return None
        
        X_new = df_with_all_features[cols_for_prediction].copy()
        if X_new.empty:
            logger.warning("Feature matrix X_new is empty after selection. Cannot predict.")
            return None
        
        # Handle potential NaNs in the very last row for prediction
        # LightGBM can handle NaNs internally if not too many, but for the last row it's critical.
        # If X_new is just one row (latest data point), ensure it's complete or handle.
        if len(X_new) == 1 and X_new.isnull().values.any():
            logger.warning(f"Latest data row for prediction contains NaNs in selected features: {X_new[X_new.isnull().any(axis=1)].columns[X_new.isnull().any(axis=1)[0]]}. Prediction may fail or be zero.")
            # Depending on LightGBM's setup and data, it might predict 0 or error.
            # Option: return None or fill specific ways if this is an issue.
            # For now, let LightGBM try.
        try:
            predictions = self.model.predict(X_new)
            logger.debug(f"Made {len(predictions)} predictions. Latest prediction input shape: {X_new.shape}. Prediction output: {predictions[-1] if len(predictions)>0 else 'N/A'}")
            if len(predictions) == 0: return None
            # We usually want the prediction for the last row of input `new_data`
            return predictions[-1] if isinstance(predictions, np.ndarray) else predictions # if it was a single value already
        except Exception as e:
            logger.error(f"Error during LightGBM prediction: {e}", exc_info=True)
            return None
    def save_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if self.model is None:
            logger.error("No model to save.")
            return
        if not _path:
            logger.error("No path specified for saving the model.")
            return
        try:
            model_and_features = {
                'model': self.model,
                'feature_columns': getattr(self, 'trained_feature_columns_', None)
            }
            joblib.dump(model_and_features, _path)
            logger.info(f"LightGBM model and feature columns saved to {_path}")
        except Exception as e:
            logger.error(f"Error saving LightGBM model to {_path}: {e}", exc_info=True)
    def load_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if not _path:
            # This case might happen if model_path is None and params don't specify it.
            # Initialization of LightGBMForecaster should handle this (e.g. by not setting self.model)
            logger.debug("No path specified for loading the model during load_model call.")
            return
        try:
            model_and_features = joblib.load(_path)
            self.model = model_and_features['model']
            self.trained_feature_columns_ = model_and_features.get('feature_columns') 
            self.model_path = _path 
            logger.info(f"LightGBM model and feature columns loaded from {_path}. Trained features: {self.trained_feature_columns_}")
        except FileNotFoundError:
            logger.error(f"LightGBM model file not found at {_path}.")
            self.model = None
            self.trained_feature_columns_ = None
        except Exception as e:
            logger.error(f"Error loading LightGBM model from {_path}: {e}", exc_info=True)
            self.model = None
            self.trained_feature_columns_ = None
</code>

kamikaze_komodo/ml_models/price_forecasting/xgboost_classifier_forecaster.py:
<code>
# FILE: kamikaze_komodo/ml_models/price_forecasting/xgboost_classifier_forecaster.py
import xgboost as xgb
import pandas as pd
import numpy as np
import joblib
from typing import Optional, Dict, Any, List, Tuple
from sklearn.preprocessing import LabelEncoder

from kamikaze_komodo.ml_models.price_forecasting.base_forecaster import BasePriceForecaster
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class XGBoostClassifierForecaster(BasePriceForecaster):
    """
    XGBoost-based classifier for price movement prediction (UP, DOWN, SIDEWAYS).
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        # FIX: Initialize attributes BEFORE calling super().__init__
        # This prevents the super constructor's call to load_model from being overwritten.
        self.label_encoder = LabelEncoder()
        self.trained_feature_columns_: Optional[List[str]] = None
        
        super().__init__(model_path, params) # Handles loading model if path provided
        
        self.default_xgb_params = {
            'objective': 'multi:softprob', # For multiclass classification, outputs probabilities
            'eval_metric': 'mlogloss', # Multiclass logloss
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 3,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'use_label_encoder': False, # Deprecated, handle encoding manually if needed
            'seed': 42,
        }
        config_xgb_params = {k.replace('xgb_params_', ''): v for k, v in self.params.items() if k.startswith('xgb_params_')}
        self.xgb_params = {**self.default_xgb_params, **config_xgb_params}
        
        self.num_class = int(self.params.get('num_classes', 3)) # UP, DOWN, SIDEWAYS
        self.xgb_params['num_class'] = self.num_class
        
        if not self.model and self.model_path:
            self.load_model(self.model_path)

    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        if data.empty: 
            return pd.DataFrame()
        df = data.copy()
        if 'close' not in df.columns:
            logger.error("'close' column missing for feature creation.")
            return df

        for lag in [1, 2, 3, 5, 10, 20]:
            df[f'log_return_lag_{lag}'] = np.log(df['close'] / df['close'].shift(lag))
            df[f'close_change_lag_{lag}'] = df['close'].pct_change(lag)

        if 'log_return_lag_1' in df.columns:
            df['volatility_5'] = df['log_return_lag_1'].rolling(window=5).std()
            df['volatility_10'] = df['log_return_lag_1'].rolling(window=10).std()
            df['volatility_20'] = df['log_return_lag_1'].rolling(window=20).std()

        if all(col in df.columns for col in ['high', 'low', 'close']):
            try:
                import pandas_ta as ta
                df.ta.rsi(length=14, append=True, col_names=('RSI_14',))
                df.ta.macd(append=True, col_names=('MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9'))
                df.ta.atr(length=14, append=True, col_names=('ATR_14',))
            except ImportError: 
                logger.warning("pandas_ta not installed. Skipping TA features for XGBoost.")
            except Exception as e: 
                logger.warning(f"Error creating TA features: {e}")
        
        df = df.replace([np.inf, -np.inf], np.nan)
        return df

    def _define_target(self, data: pd.DataFrame, thresholds: Optional[Tuple[float, float]] = (-0.001, 0.001)) -> pd.Series:
        """Defines target classes: 0 (UP), 1 (DOWN), 2 (SIDEWAYS)."""
        future_returns = data['close'].pct_change(1).shift(-1) # Next bar's return
        if thresholds is None: 
            thresholds = (-0.001, 0.001) # Default if not provided
        lower_thresh, upper_thresh = thresholds

        target = pd.Series(2, index=data.index) # Default to SIDEWAYS
        target[future_returns > upper_thresh] = 0 # UP
        target[future_returns < lower_thresh] = 1 # DOWN
        return target.astype(int)

    def train(self, historical_data: pd.DataFrame, target_definition: str = 'next_bar_direction', feature_columns: Optional[list] = None):
        logger.info(f"Starting XGBoost Classifier training. Data shape: {historical_data.shape}")
        df = historical_data.copy()

        return_thresholds_str = self.params.get('returnthresholds_percent', "-0.001,0.001")
        try:
            thresholds_list = [float(x.strip()) for x in return_thresholds_str.split(',')]
            if len(thresholds_list) != 2: 
                raise ValueError("ReturnThresholds_Percent must be two comma-separated floats.")
            return_thresholds = tuple(thresholds_list)
        except Exception as e:
            logger.warning(f"Invalid ReturnThresholds_Percent '{return_thresholds_str}', using defaults (-0.001, 0.001). Error: {e}")
            return_thresholds = (-0.001, 0.001)

        if target_definition == 'next_bar_direction':
            df['target'] = self._define_target(df, return_thresholds)
        else:
            logger.error(f"Unsupported target_definition: {target_definition}")
            return

        df.dropna(subset=['target'], inplace=True)
        df_with_features = self.create_features(df)

        if feature_columns:
            actual_features = [col for col in feature_columns if col in df_with_features.columns]
        else:
            default_feature_set = [
                col for col in df_with_features.columns if 
                col.startswith('log_return_lag_') or
                col.startswith('close_change_lag_') or
                col.startswith('volatility_') or
                col in ['RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'ATR_14']
            ]
            actual_features = [col for col in default_feature_set if col in df_with_features.columns]


        if not actual_features:
            logger.error("No features selected for training XGBoost.")
            return

        X = df_with_features[actual_features].copy()
        y = df.loc[X.index, 'target'].astype(int)

        logger.debug(f"XGBoost - Shape of df_with_features: {df_with_features.shape}")
        logger.debug(f"XGBoost - Features selected for X: {actual_features}")
        logger.debug(f"XGBoost - Sample of X before dropna (head):\n{X.head()}")
        logger.debug(f"XGBoost - NaN counts in X before dropna:\n{X.isnull().sum().sort_values(ascending=False)}")
        logger.debug(f"XGBoost - Shape of X before dropna: {X.shape}")

        X.dropna(inplace=True) 

        logger.debug(f"XGBoost - Shape of X after dropna: {X.shape}")
        logger.debug(f"XGBoost - Shape of y before aligning with X: {y.shape}")

        y = y.loc[X.index] 

        logger.debug(f"XGBoost - Shape of y after aligning with X: {y.shape}")
        if not X.empty:
            logger.debug(f"XGBoost - Sample of X after dropna & alignment (head):\n{X.head()}")
        if not y.empty:
            logger.debug(f"XGBoost - Sample of y after dropna & alignment (head):\n{y.head()}")
            logger.debug(f"XGBoost - Value counts of y: \n{y.value_counts(dropna=False)}")


        if X.empty or y.empty:
            logger.error("Feature matrix X or target vector y is empty after processing. Training cannot proceed.")
            if X.empty:
                logger.error(f"XGBoost - X is empty. Columns previously in X (before dropna): {str(actual_features)}")
                logger.error(f"XGBoost - df_with_features had columns: {list(df_with_features.columns)}")
            if y.empty:
                logger.error("XGBoost - y is empty.")
            return

        self.label_encoder.fit(y) # Fit encoder on the integer labels (0, 1, 2)
        y_encoded = self.label_encoder.transform(y)

        self.model = xgb.XGBClassifier(**self.xgb_params)
        logger.info(f"Training XGBoostClassifier with {len(X)} samples. Features: {actual_features}")
        try:
            self.model.fit(X, y_encoded)
            self.trained_feature_columns_ = list(X.columns)
            logger.info("XGBoostClassifier training completed.")
        except Exception as e:
            logger.error(f"Error during XGBoostClassifier training: {e}", exc_info=True)
            self.model = None

    def predict(self, new_data: pd.DataFrame, feature_columns: Optional[list] = None) -> Optional[Dict[str, Any]]:
        if self.model is None:
            logger.error("XGBoost model not loaded/trained. Cannot predict.")
            return None

        df_with_features = self.create_features(new_data)
        cols_for_pred = feature_columns if feature_columns else self.trained_feature_columns_
        if not cols_for_pred:
            logger.error("No feature columns determined for XGBoost prediction.")
            return None

        # Ensure all required columns are present, even if with NaNs, before selection
        missing_cols = set(cols_for_pred) - set(df_with_features.columns)
        if missing_cols:
            logger.warning(f"Columns required for prediction are missing from generated features: {missing_cols}")
            return None

        X_new = df_with_features[cols_for_pred].copy()
        if X_new.empty:
            logger.warning("Feature matrix for prediction is empty.")
            return None

        if X_new.iloc[-1].isnull().any():
            logger.warning(f"Last row for XGBoost prediction contains NaNs in features: {X_new.columns[X_new.iloc[-1].isnull()].tolist()}. Prediction might be unreliable.")

        try:
            # Predict probabilities for the last row
            last_row_features = X_new.iloc[[-1]] # Keep as DataFrame
            probabilities = self.model.predict_proba(last_row_features)[0]
            predicted_class_encoded = np.argmax(probabilities)
            predicted_class_label = self.label_encoder.inverse_transform([predicted_class_encoded])[0] # Original label (0, 1, 2)
            confidence = probabilities[predicted_class_encoded]
        
            return {
                "predicted_class": int(predicted_class_label), # 0:UP, 1:DOWN, 2:SIDEWAYS
                "confidence": float(confidence),
                "probabilities": [float(p) for p in probabilities] # Prob for each class
            }
        except Exception as e:
            logger.error(f"Error during XGBoostClassifier prediction: {e}", exc_info=True)
            return None

    def save_model(self, path: str):
        if self.model is None:
            logger.error("No XGBoost model to save.")
            return
        try:
            model_data = {
                'model': self.model,
                'label_encoder': self.label_encoder,
                'feature_columns': self.trained_feature_columns_
            }
            joblib.dump(model_data, path)
            logger.info(f"XGBoostClassifier model saved to {path}")
        except Exception as e:
            logger.error(f"Error saving XGBoostClassifier model to {path}: {e}", exc_info=True)

    def load_model(self, path: str):
        try:
            model_data = joblib.load(path)
            self.model = model_data['model']
            self.label_encoder = model_data['label_encoder']
            self.trained_feature_columns_ = model_data.get('feature_columns')
            self.model_path = path
            logger.info(f"XGBoostClassifier model loaded from {path}. Trained features: {self.trained_feature_columns_}")
        except FileNotFoundError:
            logger.error(f"XGBoostClassifier model file not found at {path}.")
            self.model = None
        except Exception as e:
            logger.error(f"Error loading XGBoostClassifier model from {path}: {e}", exc_info=True)
            self.model = None
</code>

kamikaze_komodo/ml_models/price_forecasting/__init__.py:
<code>
# kamikaze_komodo/ml_models/price_forecasting/__init__.py
# This file makes the 'price_forecasting' directory a Python package.
</code>

kamikaze_komodo/ml_models/regime_detection/kmeans_regime_model.py:
<code>
# kamikaze_komodo/ml_models/regime_detection/kmeans_regime_model.py
import pandas as pd
import numpy as np
import joblib
import os
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from typing import Optional, Dict, Any, List

from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from datetime import datetime, timedelta, timezone

logger = get_logger(__name__)

class KMeansRegimeModel:
    """
    Identifies market regimes using K-Means clustering on specified features.
    """
    def __init__(self, model_path: Optional[str] = None, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        self.model_path = model_path
        
        self.n_clusters = int(self.params.get('num_clusters', 3))
        # Features string from config, e.g., "volatility_20d,atr_14d_percentage"
        features_str = self.params.get('featuresforclustering', 'volatility_20d,atr_14d_percentage')
        self.features_for_clustering = [f.strip() for f in features_str.split(',')]

        self.model: Optional[KMeans] = None
        self.scaler: Optional[StandardScaler] = None
        self.cluster_centers_: Optional[np.ndarray] = None # To store cluster centers post-training for interpretation

        if model_path:
            self.load_model(model_path)
        logger.info(f"KMeansRegimeModel initialized. Clusters: {self.n_clusters}, Features: {self.features_for_clustering}, Model Path: {model_path}")

    def _calculate_feature_volatility_X_day(self, data: pd.DataFrame, window: int = 20) -> pd.Series:
        """Calculates X-day rolling volatility of log returns."""
        if 'close' not in data.columns or len(data) < window:
            return pd.Series(np.nan, index=data.index)
        log_returns = np.log(data['close'] / data['close'].shift(1))
        return log_returns.rolling(window=window).std() * np.sqrt(window) # Annualize for context if daily, or use raw

    def _calculate_feature_atr_X_day_percentage(self, data: pd.DataFrame, window: int = 14) -> pd.Series:
        """Calculates X-day ATR as a percentage of closing price."""
        if not all(col in data.columns for col in ['high', 'low', 'close']) or len(data) < window:
            return pd.Series(np.nan, index=data.index)
        try:
            import pandas_ta as ta
            atr = ta.atr(high=data['high'], low=data['low'], close=data['close'], length=window)
            if atr is None or data['close'].rolling(window=window).min().eq(0).any(): # Avoid division by zero
                 return pd.Series(np.nan, index=data.index)
            atr_percentage = (atr / data['close']) * 100
            return atr_percentage
        except ImportError:
            logger.warning("pandas_ta not found for ATR calculation in KMeansRegimeModel.")
            return pd.Series(np.nan, index=data.index)
        except Exception as e:
            logger.error(f"Error calculating ATR%: {e}")
            return pd.Series(np.nan, index=data.index)


    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        if data.empty:
            return pd.DataFrame()
        df = data.copy()
        
        generated_features = pd.DataFrame(index=df.index)

        for feature_name in self.features_for_clustering:
            if feature_name.startswith('volatility_') and feature_name.endswith('d'):
                try:
                    window = int(feature_name.split('_')[1][:-1])
                    generated_features[feature_name] = self._calculate_feature_volatility_X_day(df, window)
                except ValueError:
                    logger.warning(f"Could not parse window for volatility feature: {feature_name}")
            elif feature_name.startswith('atr_') and feature_name.endswith('d_percentage'):
                try:
                    window = int(feature_name.split('_')[1][:-1]) # atr_14d -> 14
                    generated_features[feature_name] = self._calculate_feature_atr_X_day_percentage(df, window)
                except ValueError:
                     logger.warning(f"Could not parse window for ATR feature: {feature_name}")
            else:
                logger.warning(f"Unsupported feature definition for Kmeans clustering: {feature_name}")
        
        generated_features.dropna(inplace=True)
        return generated_features

    def train(self, historical_data: pd.DataFrame):
        logger.info(f"Starting KMeans Regime Model training. Data shape: {historical_data.shape}")
        feature_df = self.create_features(historical_data)

        if feature_df.empty or len(feature_df) < self.n_clusters:
            logger.error("Not enough data points after feature creation to train KMeans model.")
            return

        self.scaler = StandardScaler()
        scaled_features = self.scaler.fit_transform(feature_df)

        self.model = KMeans(n_clusters=self.n_clusters, random_state=42, n_init='auto')
        try:
            self.model.fit(scaled_features)
            self.cluster_centers_ = self.scaler.inverse_transform(self.model.cluster_centers_) # Store unscaled centers
            logger.info(f"KMeans Regime Model training completed. Inertia: {self.model.inertia_:.2f}")
            logger.info(f"Unscaled Cluster Centers:\n{self.cluster_centers_}")
            # Interpret clusters (e.g., by examining center values for volatility, atr_percentage)
            # For example, cluster with highest volatility could be "high volatility regime"
        except Exception as e:
            logger.error(f"Error during KMeans model training: {e}", exc_info=True)
            self.model = None
            self.scaler = None

    def predict(self, new_data: pd.DataFrame) -> Optional[int]:
        if self.model is None or self.scaler is None:
            logger.error("KMeans model or scaler not trained/loaded. Cannot predict regime.")
            return None
        
        feature_df = self.create_features(new_data)
        if feature_df.empty:
            logger.warning("No features could be created from new_data for KMeans prediction.")
            return None

        # We need to predict for the last row of feature_df
        last_features = feature_df.iloc[[-1]]
        if last_features.isnull().values.any():
            logger.warning(f"Latest features for KMeans prediction contain NaNs: {last_features}. Cannot predict.")
            return None

        scaled_features = self.scaler.transform(last_features)
        try:
            regime = self.model.predict(scaled_features)[0]
            logger.debug(f"Predicted regime for latest data: {regime}")
            return int(regime)
        except Exception as e:
            logger.error(f"Error during KMeans regime prediction: {e}", exc_info=True)
            return None

    def save_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if self.model is None or self.scaler is None:
            logger.error("No KMeans model or scaler to save.")
            return
        if not _path:
            logger.error("No path specified for saving KMeans model.")
            return
        try:
            model_data = {
                'model': self.model,
                'scaler': self.scaler,
                'features_for_clustering': self.features_for_clustering,
                'n_clusters': self.n_clusters,
                'cluster_centers_': self.cluster_centers_
            }
            joblib.dump(model_data, _path)
            logger.info(f"KMeans Regime model saved to {_path}")
        except Exception as e:
            logger.error(f"Error saving KMeans model to {_path}: {e}", exc_info=True)

    def load_model(self, path: Optional[str] = None):
        _path = path or self.model_path
        if not _path:
            logger.debug("No path specified for loading KMeans model.")
            return
        try:
            model_data = joblib.load(_path)
            self.model = model_data['model']
            self.scaler = model_data['scaler']
            self.features_for_clustering = model_data.get('features_for_clustering', self.features_for_clustering)
            self.n_clusters = model_data.get('n_clusters', self.n_clusters)
            self.cluster_centers_ = model_data.get('cluster_centers_')
            self.model_path = _path
            logger.info(f"KMeans Regime model loaded from {_path}. Clusters: {self.n_clusters}, Features: {self.features_for_clustering}")
            if self.cluster_centers_ is not None:
                 logger.info(f"Loaded Unscaled Cluster Centers:\n{self.cluster_centers_}")
        except FileNotFoundError:
            logger.error(f"KMeans model file not found at {_path}.")
            self.model = None
            self.scaler = None
        except Exception as e:
            logger.error(f"Error loading KMeans model from {_path}: {e}", exc_info=True)
            self.model = None
            self.scaler = None

async def main_kmeans_regime_example():
    if not settings:
        print("Settings not loaded, cannot run KMeans Regime model example.")
        return

    symbol = settings.default_symbol
    timeframe = settings.default_timeframe
    config_section = "KMeans_Regime_Model" # Must match config.ini section name

    if not settings.config.has_section(config_section):
        logger.error(f"Config section [{config_section}] not found. Cannot run KMeans Regime example.")
        return

    model_params = settings.get_strategy_params(config_section)
    _model_base_path = model_params.get('modelsavepath', 'kamkaze_komodo/ml_models/trained_models/regime')
    _model_filename = model_params.get('modelfilename', f"kmeans_regime_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
    
    script_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) 
    if not os.path.isabs(_model_base_path):
        model_save_path_dir = os.path.join(script_dir, _model_base_path)
    else:
        model_save_path_dir = _model_base_path
    if not os.path.exists(model_save_path_dir):
        os.makedirs(model_save_path_dir, exist_ok=True)
    model_full_path = os.path.join(model_save_path_dir, _model_filename)

    # --- Training ---
    logger.info("--- KMeans Regime Model Training Example ---")
    regime_model_trainer = KMeansRegimeModel(params=model_params)
    
    db_manager = DatabaseManager()
    data_fetcher = DataFetcher()
    training_days = int(model_params.get('trainingdayshistory', 1095))
    start_dt = datetime.now(timezone.utc) - timedelta(days=training_days)
    end_dt = datetime.now(timezone.utc)
    
    bars = db_manager.retrieve_bar_data(symbol, timeframe, start_dt, end_dt)
    if not bars or len(bars) < 100: # Need substantial data for regime features
        logger.info(f"Fetching fresh data for KMeans training for {symbol}...")
        bars = await data_fetcher.fetch_historical_data_for_period(symbol, timeframe, start_dt, end_dt)
        if bars: db_manager.store_bar_data(bars)
    await data_fetcher.close()
    db_manager.close()

    if not bars or len(bars) < 100:
        logger.error(f"Not enough data ({len(bars)} bars) for KMeans training.")
        return

    training_df = pd.DataFrame([b.model_dump() for b in bars])
    training_df['timestamp'] = pd.to_datetime(training_df['timestamp'])
    training_df.set_index('timestamp', inplace=True)
    training_df.sort_index(inplace=True)

    regime_model_trainer.train(training_df)
    if regime_model_trainer.model:
        regime_model_trainer.save_model(model_full_path)

    # --- Prediction ---
    logger.info("--- KMeans Regime Model Prediction Example ---")
    if not os.path.exists(model_full_path):
        logger.error("Trained KMeans model not found. Skipping prediction example.")
        return

    regime_model_predictor = KMeansRegimeModel(model_path=model_full_path, params=model_params)
    if regime_model_predictor.model is None:
        logger.error("Could not load KMeans model for prediction.")
        return

    # Use the last N bars of training_df for prediction example (or fetch fresh small segment)
    if len(training_df) > 50:
        prediction_data_segment = training_df.tail(50) # Use recent history to get features for the last point
        predicted_regime = regime_model_predictor.predict(prediction_data_segment)
        if predicted_regime is not None:
            logger.info(f"Predicted regime for {symbol} ({timeframe}) on latest data: {predicted_regime}")
        else:
            logger.warning(f"Could not get KMeans regime prediction for {symbol} ({timeframe}).")
    else:
        logger.warning("Not enough data in training_df to run prediction example.")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main_kmeans_regime_example())
</code>

kamikaze_komodo/ml_models/regime_detection/__init__.py:
<code>
# kamikaze_komodo/ml_models/regime_detection/__init__.py
# This file makes the 'regime_detection' directory a Python package.
</code>

kamikaze_komodo/ml_models/training_pipelines/kmeans_regime_pipeline.py:
<code>
# kamikaze_komodo/ml_models/training_pipelines/kmeans_regime_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os

from kamikaze_komodo.ml_models.regime_detection.kmeans_regime_model import KMeansRegimeModel
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings

logger = get_logger(__name__)

class KMeansRegimeTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "KMeans_Regime_Model"):
        if not settings:
            logger.critical("Settings not loaded. KMeansRegimeTrainingPipeline cannot be initialized.")
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        if not self.model_params:
            logger.warning(f"No parameters found for config section [{model_config_section}]. Using defaults for KMeansRegimeModel if any.")
            self.model_params = {} # Ensure it's a dict
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models/regime')
        _model_filename = self.model_params.get('modelfilename', f"kmeans_regime_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        # Determine project root based on this file's location to ensure correct relative pathing
        # This file is in: kamikaze_komodo/ml_models/training_pipelines/kmeans_regime_pipeline.py
        # Project root is three levels up from this file's directory
        current_file_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_file_dir)))
        
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(project_root, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path
            
        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            logger.info(f"Created directory for trained KMeans regime models: {self.model_save_path_dir}")
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        # Pass the specific model parameters and the constructed model_full_save_path to KMeansRegimeModel
        self.regime_model = KMeansRegimeModel(model_path=None, params=self.model_params) # Don't load, we are training
        logger.info(f"KMeansRegimeTrainingPipeline initialized for {symbol} ({timeframe}). Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int) -> pd.DataFrame:
        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        logger.info(f"Attempting to retrieve data from DB for KMeans training: {self.symbol} ({self.timeframe}) from {start_date} to {end_date}")
        historical_bars = db_manager.retrieve_bar_data(self.symbol, self.timeframe, start_date, end_date)
        
        min_bars_for_features = int(self.model_params.get('minbarsfortraining', 100)) 
        # Consider typical feature lookback for K-Means features (e.g., 20-day vol needs at least 20 bars)
        # If self.regime_model.features_for_clustering includes 'volatility_X_day', min_bars_for_features should be > X

        if not historical_bars or len(historical_bars) < min_bars_for_features:
            logger.info(f"Insufficient data in DB ({len(historical_bars)} bars found, need {min_bars_for_features}). Fetching fresh data for KMeans training...")
            historical_bars = await data_fetcher.fetch_historical_data_for_period(self.symbol, self.timeframe, start_date, end_date)
            if historical_bars:
                db_manager.store_bar_data(historical_bars)
            else:
                logger.error("Failed to fetch training data for KMeans.")
                await data_fetcher.close()
                db_manager.close()
                return pd.DataFrame()
        
        await data_fetcher.close()
        db_manager.close()

        if not historical_bars or len(historical_bars) < min_bars_for_features:
            logger.error(f"Still not enough data ({len(historical_bars)} bars) for KMeans training after fetch attempt. Need {min_bars_for_features}.")
            return pd.DataFrame()
            
        data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
        if data_df.empty:
            logger.error("DataFrame is empty after converting BarData list.")
            return pd.DataFrame()
            
        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
        data_df.set_index('timestamp', inplace=True)
        data_df.sort_index(inplace=True)
        
        logger.info(f"Fetched {len(data_df)} bars for KMeans training {self.symbol} ({self.timeframe}).")
        return data_df

    async def run_training(self):
        """
        Fetches data, trains the KMeans regime model, and saves it.
        """
        days_history = int(self.model_params.get('trainingdayshistory', 1095)) # Default to ~3 years of data
        if days_history <=0:
            logger.error(f"TrainingDaysHistory ({days_history}) must be positive. Cannot run training.")
            return

        historical_df = await self.fetch_training_data(days_history=days_history)

        if historical_df.empty:
            logger.error("Cannot run KMeans training, no historical data was retrieved or processed.")
            return

        logger.info(f"Starting KMeans regime model training using {self.regime_model.__class__.__name__}...")
        
        # The KMeansRegimeModel's train method handles feature creation and fitting
        self.regime_model.train(historical_df) 
        
        if self.regime_model.model and self.regime_model.scaler: # Check if model and scaler were successfully created
            self.regime_model.save_model(self.model_full_save_path) # save_model is in KMeansRegimeModel
            logger.info(f"KMeans regime model training completed and model saved to {self.model_full_save_path}.")
        else:
            logger.error("KMeans regime model training did not produce a valid model or scaler. Model not saved.")

# Example of how it might be run (optional, for standalone testing)
# async def main():
#     if not settings:
#         print("Settings not loaded, cannot run KMeans training pipeline example.")
#         return
#     
#     symbol_to_train = settings.default_symbol
#     timeframe_to_train = settings.default_timeframe
#     config_section = "KMeans_Regime_Model"

#     if not settings.config.has_section(config_section):
#         logger.error(f"Config section [{config_section}] not found. Cannot run training pipeline example.")
#         return
#         
#     logger.info(f"Starting KMeans Regime training pipeline example for {symbol_to_train} ({timeframe_to_train})...")
#     pipeline = KMeansRegimeTrainingPipeline(symbol=symbol_to_train, timeframe=timeframe_to_train, model_config_section=config_section)
#     await pipeline.run_training()
#     logger.info(f"KMeans Regime training pipeline example finished for {symbol_to_train} ({timeframe_to_train}).")

# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(main())
</code>

kamikaze_komodo/ml_models/training_pipelines/lightgbm_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/lightgbm_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
from kamikaze_komodo.ml_models.price_forecasting.lightgbm_forecaster import LightGBMForecaster
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class LightGBMTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "LightGBM_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"lgbm_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        # FIX: Use the consistent PROJECT_ROOT from settings.py
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path

        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            logger.info(f"Created directory for trained models: {self.model_save_path_dir}")
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.forecaster = LightGBMForecaster(params=self.model_params)
        logger.info(f"LightGBM Training Pipeline initialized for {symbol} ({timeframe}). Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int = 730) -> pd.DataFrame:
        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        logger.info(f"Attempting to retrieve data from DB for {self.symbol} ({self.timeframe}) from {start_date} to {end_date}")
        historical_bars = db_manager.retrieve_bar_data(self.symbol, self.timeframe, start_date, end_date)
        
        required_bars = self.model_params.get('min_bars_for_training', 200)
        if not historical_bars or len(historical_bars) < required_bars:
            logger.info(f"Insufficient data in DB ({len(historical_bars)} bars found). Fetching fresh data from exchange...")
            historical_bars = await data_fetcher.fetch_historical_data_for_period(self.symbol, self.timeframe, start_date, end_date)
            if historical_bars:
                db_manager.store_bar_data(historical_bars)
            else:
                logger.error("Failed to fetch training data.")
                await data_fetcher.close()
                db_manager.close()
                return pd.DataFrame()
        
        await data_fetcher.close()
        db_manager.close()
        if not historical_bars:
            logger.error("No historical bars available for training.")
            return pd.DataFrame()
        data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
        data_df.set_index('timestamp', inplace=True)
        data_df.sort_index(inplace=True)
        
        logger.info(f"Fetched {len(data_df)} bars for training {self.symbol} ({self.timeframe}).")
        return data_df

    async def run_training(self):
        days_history = int(self.model_params.get('training_days_history', 730))
        historical_df = await self.fetch_training_data(days_history=days_history)
        if historical_df.empty:
            logger.error("Cannot run training, no historical data.")
            return
        target_col_name = self.model_params.get('target_column_name', 'close_change_lag_1_future')
        
        feature_cols_str = self.model_params.get('feature_columns')
        feature_columns = [col.strip() for col in feature_cols_str.split(',')] if feature_cols_str else None
        
        logger.info(f"Starting training with target: '{target_col_name}', features: {feature_columns if feature_columns else 'default in forecaster'}")
        self.forecaster.train(historical_df, target_column=target_col_name, feature_columns_to_use=feature_columns)
        
        if self.forecaster.model:
            self.forecaster.save_model(self.model_full_save_path)
        else:
            logger.error("Training did not produce a model. Model not saved.")
</code>

kamikaze_komodo/ml_models/training_pipelines/xgboost_classifier_pipeline.py:
<code>
# FILE: kamikaze_komodo/ml_models/training_pipelines/xgboost_classifier_pipeline.py
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
from kamikaze_komodo.ml_models.price_forecasting.xgboost_classifier_forecaster import XGBoostClassifierForecaster
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
from kamikaze_komodo.data_handling.database_manager import DatabaseManager
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings, PROJECT_ROOT

logger = get_logger(__name__)

class XGBoostClassifierTrainingPipeline:
    def __init__(self, symbol: str, timeframe: str, model_config_section: str = "XGBoost_Classifier_Forecaster"):
        if not settings:
            raise ValueError("Settings not loaded.")
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_config_section = model_config_section
        
        self.model_params = settings.get_strategy_params(model_config_section)
        
        _model_base_path = self.model_params.get('modelsavepath', 'ml_models/trained_models')
        _model_filename = self.model_params.get('modelfilename', f"xgb_classifier_{symbol.replace('/', '_').lower()}_{timeframe}.joblib")
        
        # FIX: Use the consistent PROJECT_ROOT from settings.py
        if not os.path.isabs(_model_base_path):
            self.model_save_path_dir = os.path.join(PROJECT_ROOT, _model_base_path)
        else:
            self.model_save_path_dir = _model_base_path
            
        if not os.path.exists(self.model_save_path_dir):
            os.makedirs(self.model_save_path_dir, exist_ok=True)
            logger.info(f"Created directory for trained XGBoost models: {self.model_save_path_dir}")
            
        self.model_full_save_path = os.path.join(self.model_save_path_dir, _model_filename)
        
        self.forecaster = XGBoostClassifierForecaster(params=self.model_params)
        logger.info(f"XGBoost Training Pipeline initialized for {symbol} ({timeframe}). Model will be saved to: {self.model_full_save_path}")

    async def fetch_training_data(self, days_history: int = 730) -> pd.DataFrame:
        db_manager = DatabaseManager()
        data_fetcher = DataFetcher()
        
        start_date = datetime.now(timezone.utc) - timedelta(days=days_history)
        end_date = datetime.now(timezone.utc)
        logger.info(f"Attempting to retrieve data from DB for {self.symbol} ({self.timeframe}) from {start_date} to {end_date}")
        historical_bars = db_manager.retrieve_bar_data(self.symbol, self.timeframe, start_date, end_date)
        
        required_bars = int(self.model_params.get('minbarsfortraining', 200))
        if not historical_bars or len(historical_bars) < required_bars:
            logger.info(f"Insufficient data in DB ({len(historical_bars)} bars found). Fetching fresh data for XGBoost training...")
            historical_bars = await data_fetcher.fetch_historical_data_for_period(self.symbol, self.timeframe, start_date, end_date)
            if historical_bars:
                db_manager.store_bar_data(historical_bars)
            else:
                logger.error("Failed to fetch training data for XGBoost.")
                await data_fetcher.close()
                db_manager.close()
                return pd.DataFrame()
        
        await data_fetcher.close()
        db_manager.close()
        if not historical_bars:
            logger.error("No historical bars available for XGBoost training.")
            return pd.DataFrame()
            
        data_df = pd.DataFrame([bar.model_dump() for bar in historical_bars])
        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])
        data_df.set_index('timestamp', inplace=True)
        data_df.sort_index(inplace=True)
        
        logger.info(f"Fetched {len(data_df)} bars for XGBoost training {self.symbol} ({self.timeframe}).")
        return data_df

    async def run_training(self):
        days_history = int(self.model_params.get('trainingdayshistory', 730))
        historical_df = await self.fetch_training_data(days_history=days_history)
        if historical_df.empty:
            logger.error("Cannot run XGBoost training, no historical data.")
            return

        target_def = self.model_params.get('targetdefinition', 'next_bar_direction')
        feature_cols_str = self.model_params.get('feature_columns')
        feature_columns = [col.strip() for col in feature_cols_str.split(',')] if feature_cols_str else None
        
        logger.info(f"Starting XGBoost training with target definition: '{target_def}', features: {feature_columns if feature_columns else 'default in forecaster'}")
        self.forecaster.train(historical_df, target_definition=target_def, feature_columns=feature_columns)
        
        if self.forecaster.model:
            self.forecaster.save_model(self.model_full_save_path)
        else:
            logger.error("XGBoost training did not produce a model. Model not saved.")

async def main_train_xgboost_classifier():
    if not settings:
        print("Settings not loaded, cannot run XGBoost Classifier training example.")
        return
    
    symbol_to_train = settings.default_symbol
    timeframe_to_train = settings.default_timeframe
    
    if not settings.config.has_section("XGBoost_Classifier_Forecaster"):
        logger.error("Config section [XGBoost_Classifier_Forecaster] not found. Cannot run training pipeline.")
        return
        
    logger.info(f"Starting XGBoost Classifier training pipeline for {symbol_to_train} ({timeframe_to_train})...")
    pipeline = XGBoostClassifierTrainingPipeline(symbol=symbol_to_train, timeframe=timeframe_to_train)
    await pipeline.run_training()
    logger.info(f"XGBoost Classifier training pipeline finished for {symbol_to_train} ({timeframe_to_train}).")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main_train_xgboost_classifier())
</code>

kamikaze_komodo/ml_models/training_pipelines/__init__.py:
<code>
# kamikaze_komodo/ml_models/training_pipelines/__init__.py
# This file makes the 'training_pipelines' directory a Python package.
</code>

kamikaze_komodo/orchestration/scheduler.py:
<code>
# kamikaze_komodo/orchestration/scheduler.py

from typing import Optional
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.executors.pool import ThreadPoolExecutor, ProcessPoolExecutor
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings
import os

logger = get_logger(__name__)

class TaskScheduler:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(TaskScheduler, cls).__new__(cls, *args, **kwargs)
        return cls._instance

    def __init__(self, db_path: Optional[str] = "logs/scheduler_jobs.sqlite"):
        if hasattr(self, '_initialized') and self._initialized: # Ensure __init__ runs only once for singleton
            return
        
        if not settings:
            logger.critical("Settings not loaded. TaskScheduler cannot be initialized.")
            raise ValueError("Settings not loaded.")

        self.db_path = db_path
        if self.db_path and not os.path.isabs(self.db_path):
             # Get project root based on this file's location: kamikaze_komodo/orchestration/scheduler.py
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            self.db_path = os.path.join(project_root, self.db_path)
        
        db_dir = os.path.dirname(self.db_path)
        if not os.path.exists(db_dir):
            os.makedirs(db_dir, exist_ok=True)
            logger.info(f"Created directory for scheduler database: {db_dir}")

        jobstores = {
            'default': SQLAlchemyJobStore(url=f'sqlite:///{self.db_path}')
        }
        executors = {
            'default': ThreadPoolExecutor(10), # For I/O bound tasks
            'processpool': ProcessPoolExecutor(3) # For CPU bound tasks
        }
        job_defaults = {
            'coalesce': False, # Run missed jobs if scheduler was down (be careful with this)
            'max_instances': 3 # Max parallel instances of the same job
        }
        
        self.scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            executors=executors,
            job_defaults=job_defaults,
            timezone='UTC' # Explicitly set timezone
        )
        self._initialized = True
        logger.info(f"TaskScheduler initialized with SQLite job store at: {self.db_path}")

    def start(self):
        if not self.scheduler.running:
            try:
                self.scheduler.start()
                logger.info("APScheduler started.")
            except Exception as e:
                logger.error(f"Failed to start APScheduler: {e}", exc_info=True)
        else:
            logger.info("APScheduler is already running.")

    def shutdown(self, wait: bool = True):
        if self.scheduler.running:
            try:
                self.scheduler.shutdown(wait=wait)
                logger.info("APScheduler shut down.")
            except Exception as e:
                logger.error(f"Error shutting down APScheduler: {e}", exc_info=True)

    def add_job(self, func, trigger: str = 'interval', **kwargs):
        """
        Adds a job to the scheduler.
        Args:
            func: The function to execute.
            trigger: The trigger type (e.g., 'interval', 'cron', 'date').
            **kwargs: Arguments for the trigger and job (e.g., minutes=1, id='my_job').
        """
        try:
            job = self.scheduler.add_job(func, trigger, **kwargs)
            logger.info(f"Job '{kwargs.get('id', func.__name__)}' added with trigger: {trigger}, params: {kwargs}")
            return job
        except Exception as e:
            logger.error(f"Failed to add job '{kwargs.get('id', func.__name__)}': {e}", exc_info=True)
            return None

    def remove_job(self, job_id: str):
        try:
            self.scheduler.remove_job(job_id)
            logger.info(f"Job '{job_id}' removed.")
        except Exception as e: # Specific exception: JobLookupError
            logger.warning(f"Failed to remove job '{job_id}': {e}")

# --- Example Scheduled Tasks (Conceptual for Phase 6 "Begin Integration") ---
async def example_data_polling_task():
    logger.info("Scheduler: Running example_data_polling_task...")
    # In a real scenario, this would call a method in DataFetcher or a dedicated data polling module.
    # from kamikaze_komodo.data_handling.data_fetcher import DataFetcher
    # fetcher = DataFetcher()
    # await fetcher.fetch_latest_data_for_active_symbols() # Fictional method
    # await fetcher.close()
    await asyncio.sleep(2) # Simulate work
    logger.info("Scheduler: example_data_polling_task finished.")

async def example_news_scraping_task():
    logger.info("Scheduler: Running example_news_scraping_task...")
    # from kamikaze_komodo.ai_news_analysis_agent_module.news_scraper import NewsScraper
    # scraper = NewsScraper()
    # articles = await scraper.scrape_all(limit_per_source=5, since_hours_rss=12)
    # if articles:
    #     logger.info(f"Scheduler: Scraped {len(articles)} news articles.")
        # Further processing: store, analyze sentiment, etc.
    await asyncio.sleep(5) # Simulate work
    logger.info("Scheduler: example_news_scraping_task finished.")

async def example_model_retraining_check_task():
    logger.info("Scheduler: Running example_model_retraining_check_task...")
    # This task would check conditions for retraining ML models
    # e.g., time since last training, performance degradation, new data volume
    # from kamikaze_komodo.ml_models.training_pipelines.lightgbm_pipeline import LightGBMTrainingPipeline
    # if conditions_met_for_retraining("LightGBM_BTCUSD_1h"):
    #     pipeline = LightGBMTrainingPipeline(symbol="BTC/USD", timeframe="1h")
    #     await pipeline.run_training()
    await asyncio.sleep(3)
    logger.info("Scheduler: example_model_retraining_check_task finished.")


async def main_scheduler_example():
    """Demonstrates basic scheduler setup and job addition."""
    scheduler_manager = TaskScheduler()

    # Add example jobs (these won't run unless scheduler is started and loop runs)
    scheduler_manager.add_job(example_data_polling_task, 'interval', minutes=15, id='data_poll_main')
    scheduler_manager.add_job(example_news_scraping_task, 'cron', hour='*/2', id='news_scrape_bi_hourly') # Every 2 hours
    scheduler_manager.add_job(example_model_retraining_check_task, 'cron', day_of_week='sun', hour='3', minute='0', id='weekly_retrain_check')

    try:
        scheduler_manager.start()
        # Keep the main thread alive to allow scheduler to run, or integrate into main application loop
        # For this example, we'll just let it run for a short period.
        # In a real app, asyncio.get_event_loop().run_forever() or similar would be used.
        if settings and settings.log_level.upper() == "DEBUG": # Only run for a bit in debug
            logger.info("Scheduler example running for 30 seconds (DEBUG mode)...")
            await asyncio.sleep(30)
        else:
            logger.info("Scheduler example configured. In a full app, it would run continuously.")
            # For non-debug, don't block indefinitely here in a simple example.
            # Real app would have its own main loop.
            
    except (KeyboardInterrupt, SystemExit):
        logger.info("Scheduler example interrupted.")
    finally:
        scheduler_manager.shutdown()

if __name__ == "__main__":
    import asyncio
    # This example shows how to set up the scheduler.
    # It's best integrated into the main application's async loop (e.g., in main.py).
    # Run with: python -m kamikaze_komodo.orchestration.scheduler
    
    # To see jobs persist and get reloaded, run, stop, then run again.
    # Check the logs/scheduler_jobs.sqlite file.
    
    # Note: Running this standalone will schedule jobs. If you run it multiple times
    # without clearing the scheduler_jobs.sqlite, it might try to add duplicate jobs
    # if the `replace_existing=True` option is not used in add_job and IDs are the same.
    # The current setup will log errors for duplicate job IDs if they are not replaced.
    asyncio.run(main_scheduler_example())
</code>

kamikaze_komodo/orchestration/__init__.py:
<code>
# kamikaze_komodo/orchestration/__init__.py
# This file makes the 'orchestration' directory a Python package.
</code>

kamikaze_komodo/portfolio_constructor/asset_allocator.py:
<code>
# FILE: kamikaze_komodo/portfolio_constructor/asset_allocator.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
import numpy as np
import pandas as pd
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import squareform
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BaseAssetAllocator(ABC):
    """
    Abstract base class for asset allocation strategies.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def allocate(
        self,
        assets: List[str],
        portfolio_value: float,
        historical_data: Optional[Dict[str, pd.DataFrame]] = None,
        trade_history: Optional[pd.DataFrame] = None
    ) -> Dict[str, float]:
        """
        Determines the target allocation for assets.
        Returns:
            Dict[str, float]: Dictionary mapping asset symbols to target capital allocation.
        """
        pass

class HRPAllocator(BaseAssetAllocator):
    """
    Allocates assets using Hierarchical Risk Parity (HRP) by De Prado.
    Requires historical returns data for all assets in the allocation universe.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.linkage_method = self.params.get('hrp_linkage_method', 'single')
        logger.info(f"HRPAllocator initialized. Linkage method: {self.linkage_method}")

    def _get_cluster_var(self, cov_matrix: pd.DataFrame, cluster_items: List[int]) -> float:
        """Calculates variance of a cluster using inverse-variance weighting."""
        cluster_cov = cov_matrix.iloc[cluster_items, cluster_items]
        ivp = 1. / np.diag(cluster_cov)
        ivp /= ivp.sum()
        cluster_var = np.dot(ivp.T, np.dot(cluster_cov, ivp))
        return cluster_var

    def _get_recursive_bisection(self, sort_ix: List[int], current_weights: np.ndarray, cov_matrix: pd.DataFrame) -> np.ndarray:
        """Performs recursive bisection for HRP weights."""
        if len(sort_ix) == 1:
            return current_weights

        mid_point = len(sort_ix) // 2
        cluster1_items = sort_ix[:mid_point]
        cluster2_items = sort_ix[mid_point:]

        var_cluster1 = self._get_cluster_var(cov_matrix, cluster1_items)
        var_cluster2 = self._get_cluster_var(cov_matrix, cluster2_items)

        alpha = 1 - (var_cluster1 / (var_cluster1 + var_cluster2))

        current_weights[cluster1_items] *= alpha
        current_weights[cluster2_items] *= (1 - alpha)

        current_weights = self._get_recursive_bisection(cluster1_items, current_weights, cov_matrix)
        current_weights = self._get_recursive_bisection(cluster2_items, current_weights, cov_matrix)

        return current_weights

    def allocate(self, assets: List[str], portfolio_value: float, historical_data: Optional[Dict[str, pd.DataFrame]] = None, **kwargs) -> Dict[str, float]:
        if historical_data is None or len(assets) < 2:
            logger.warning("HRP requires historical data for at least 2 assets. Falling back to equal weight.")
            count = len(assets) if assets else 1
            return {asset: portfolio_value / count for asset in assets}

        returns_df = pd.concat([df['close'].pct_change() for df in historical_data.values()], axis=1)
        returns_df.columns = assets
        returns_df.dropna(inplace=True)

        if len(returns_df) < 2:
            logger.warning("Not enough return data for HRP. Falling back to equal weight.")
            return {asset: portfolio_value / len(assets) for asset in assets}

        cov_matrix = returns_df.cov()
        corr_matrix = returns_df.corr()
        
        dist_matrix = np.sqrt((1 - corr_matrix) / 2)
        link = linkage(squareform(dist_matrix), self.linkage_method)
        
        sort_ix = dendrogram(link, no_plot=True)['leaves']
        sorted_assets = corr_matrix.index[sort_ix].tolist()
        
        hrp_weights = pd.Series(1, index=sorted_assets)
        hrp_weights = self._get_recursive_bisection(list(range(len(sorted_assets))), hrp_weights, cov_matrix)
        
        allocations = {asset: portfolio_value * weight for asset, weight in hrp_weights.items()}
        logger.info(f"HRP Allocator target capital allocation: {allocations}")
        return allocations

class OptimalFAllocator(BaseAssetAllocator):
    """
    Allocates capital based on Vince's Optimal f (a Kelly Criterion variant).
    This allocator uses historical trade performance to determine the fraction of capital to allocate.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.default_win_prob = float(self.params.get('optimalf_default_win_probability', 0.51))
        self.default_payoff_ratio = float(self.params.get('optimalf_default_payoff_ratio', 1.1))
        self.kelly_fraction = float(self.params.get('optimalf_kelly_fraction', 0.25))
        self.min_trades_for_stats = int(self.params.get('optimalf_min_trades_for_stats', 25))
        logger.info(f"OptimalFAllocator initialized. Kelly Fraction: {self.kelly_fraction}, Min Trades: {self.min_trades_for_stats}")

    def _calculate_stats_from_history(self, asset_symbol: str, trade_history: pd.DataFrame) -> Optional[Dict[str, float]]:
        asset_trades = trade_history[trade_history['symbol'] == asset_symbol]
        if len(asset_trades) < self.min_trades_for_stats:
            return None

        wins = asset_trades[asset_trades['pnl'] > 0]['pnl']
        losses = asset_trades[asset_trades['pnl'] < 0]['pnl'].abs()

        if len(wins) == 0 or len(losses) == 0 or losses.mean() == 0:
            return None
        
        win_prob = len(wins) / len(asset_trades)
        payoff_ratio = wins.mean() / losses.mean()
        return {"win_probability": win_prob, "payoff_ratio": payoff_ratio}

    def allocate(self, assets: List[str], portfolio_value: float, trade_history: Optional[pd.DataFrame] = None, **kwargs) -> Dict[str, float]:
        allocations: Dict[str, float] = {}
        for asset in assets:
            win_prob = self.default_win_prob
            payoff_ratio = self.default_payoff_ratio

            if trade_history is not None and not trade_history.empty:
                stats = self._calculate_stats_from_history(asset, trade_history)
                if stats:
                    win_prob = stats['win_probability']
                    payoff_ratio = stats['payoff_ratio']

            optimal_f = win_prob - ((1 - win_prob) / payoff_ratio) if payoff_ratio > 0 else -1.0
            
            allocated_capital = 0.0
            if optimal_f > 0:
                fraction_to_invest = optimal_f * self.kelly_fraction
                allocated_capital = portfolio_value * fraction_to_invest
                logger.info(f"Optimal F for {asset}: f*={optimal_f:.4f}. Target capital: ${allocated_capital:.2f}")
            else:
                logger.info(f"Optimal f for {asset} is not positive ({optimal_f:.4f}). No allocation.")
            
            allocations[asset] = allocated_capital
            
        return allocations
</code>

kamikaze_komodo/portfolio_constructor/rebalancer.py:
<code>
# kamikaze_komodo/portfolio_constructor/rebalancer.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from kamikaze_komodo.core.models import PortfolioSnapshot # For current holdings
from kamikaze_komodo.core.enums import OrderSide, OrderType # For generating orders
from kamikaze_komodo.app_logger import get_logger
import pandas as pd

logger = get_logger(__name__)

class BaseRebalancer(ABC):
    """
    Abstract base class for portfolio rebalancing logic.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def needs_rebalancing(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float], # e.g. {'BTC/USD': 0.6, 'ETH/USD': 0.4} as fractions
        asset_prices: Dict[str, float] # Current market prices for assets {'BTC/USD': 50000, ...}
    ) -> bool:
        """
        Determines if the portfolio needs rebalancing based on current state and targets.
        Args:
            current_portfolio (PortfolioSnapshot): The current state of the portfolio (contains positions quantity).
            target_allocations_pct (Dict[str, float]): The desired target allocations as percentages.
            asset_prices (Dict[str, float]): Current market prices of the assets.
        Returns:
            bool: True if rebalancing is needed, False otherwise.
        """
        pass

    @abstractmethod
    def generate_rebalancing_orders(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float],
        asset_prices: Dict[str, float] # Current market prices for assets
    ) -> List[Dict[str, Any]]: # List of order parameters
        """
        Generates orders needed to rebalance the portfolio to target allocations.
        Args:
            current_portfolio (PortfolioSnapshot): The current state of the portfolio.
            target_allocations_pct (Dict[str, float]): The desired target allocations as percentages.
            asset_prices (Dict[str, float]): Current market prices of the assets.
        Returns:
            List[Dict[str, Any]]: A list of order parameters (e.g., for exchange_api.create_order).
        """
        pass

class BasicRebalancer(BaseRebalancer):
    """
    Rebalances the portfolio if the current weight of any asset deviates
    from its target weight by more than a specified threshold.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.deviation_threshold = float(self.params.get('rebalancer_deviationthreshold', 0.05)) # Default 5% deviation
        self.min_order_value_usd = float(self.params.get('rebalancer_min_order_value_usd', 10.0)) # Min order value to execute
        logger.info(f"BasicRebalancer initialized with deviation threshold: {self.deviation_threshold*100}%, Min Order Value: ${self.min_order_value_usd}")

    def _get_current_weights(self, current_portfolio: PortfolioSnapshot, asset_prices: Dict[str, float]) -> Dict[str, float]:
        current_weights: Dict[str, float] = {}
        total_value_from_positions = 0.0
        asset_values : Dict[str, float] = {}

        for asset, quantity in current_portfolio.positions.items():
            if asset in asset_prices and asset_prices[asset] > 0:
                value = quantity * asset_prices[asset]
                asset_values[asset] = value
                total_value_from_positions += value
            else:
                logger.warning(f"Price for asset {asset} not available or zero. Cannot calculate its value for rebalancing.")
                asset_values[asset] = 0.0

        # Effective portfolio value for weight calculation is cash + value of positions
        effective_total_value = current_portfolio.cash_balance_usd + total_value_from_positions
        if effective_total_value <= 0:
            return {asset: 0.0 for asset in current_portfolio.positions.keys()}

        for asset, value in asset_values.items():
            current_weights[asset] = value / effective_total_value

        # Add assets that are in target but not in current holdings (weight 0)
        for asset in asset_prices.keys():
            if asset not in current_weights:
                current_weights[asset] = 0.0
        return current_weights


    def needs_rebalancing(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float],
        asset_prices: Dict[str, float]
    ) -> bool:
        if not target_allocations_pct:
            logger.debug("No target allocations provided, rebalancing not needed by BasicRebalancer.")
            return False

        current_weights = self._get_current_weights(current_portfolio, asset_prices)
        if not current_weights and any(v > 0 for v in target_allocations_pct.values()): # No current holdings but target has allocations
            logger.info("Rebalancing needed: No current holdings, but target allocations exist.")
            return True

        all_assets = set(current_weights.keys()).union(set(target_allocations_pct.keys()))

        for asset in all_assets:
            current_w = current_weights.get(asset, 0.0)
            target_w = target_allocations_pct.get(asset, 0.0)
            if abs(current_w - target_w) > self.deviation_threshold:
                logger.info(f"Rebalancing needed for {asset}. Current weight: {current_w:.4f}, Target: {target_w:.4f}, Deviation: {abs(current_w - target_w):.4f} > {self.deviation_threshold:.4f}")
                return True
        logger.debug("BasicRebalancer: No rebalancing needed based on deviation threshold.")
        return False

    def generate_rebalancing_orders(
        self,
        current_portfolio: PortfolioSnapshot,
        target_allocations_pct: Dict[str, float],
        asset_prices: Dict[str, float]
    ) -> List[Dict[str, Any]]:
        orders: List[Dict[str, Any]] = []
        if current_portfolio.total_value_usd <=0 :
            logger.warning("Portfolio total value is zero or negative. Cannot generate rebalancing orders.")
            return orders

        # Calculate current values of each asset
        current_asset_values: Dict[str, float] = {}
        for asset, quantity in current_portfolio.positions.items():
            if asset in asset_prices and asset_prices[asset] > 0:
                current_asset_values[asset] = quantity * asset_prices[asset]
            else:
                current_asset_values[asset] = 0.0

        # Calculate target values based on total portfolio value
        target_asset_values: Dict[str, float] = {}
        for asset, target_pct in target_allocations_pct.items():
            target_asset_values[asset] = current_portfolio.total_value_usd * target_pct

        all_assets_in_scope = set(current_asset_values.keys()).union(set(target_asset_values.keys()))

        for asset in all_assets_in_scope:
            current_value = current_asset_values.get(asset, 0.0)
            target_value = target_asset_values.get(asset, 0.0)
            price = asset_prices.get(asset)

            if price is None or price <= 0:
                logger.warning(f"Cannot generate order for {asset}: price is missing or invalid ({price}).")
                continue

            value_diff = target_value - current_value
            amount_to_trade = value_diff / price

            if abs(value_diff) < self.min_order_value_usd: # Skip if trade value is too small
                logger.debug(f"Skipping rebalance for {asset}: change in value ({value_diff:.2f}) is less than min_order_value_usd (${self.min_order_value_usd:.2f}).")
                continue

            if abs(amount_to_trade) > 1e-8: # Ensure there's a non-negligible amount to trade
                order_side = OrderSide.BUY if amount_to_trade > 0 else OrderSide.SELL
                orders.append({
                    'symbol': asset,
                    'type': OrderType.MARKET, # Or allow configurable order type
                    'side': order_side,
                    'amount': abs(amount_to_trade)
                })
                logger.info(f"Generated rebalancing order for {asset}: {order_side.value} {abs(amount_to_trade):.6f} units. Target Value: ${target_value:.2f}, Current Value: ${current_value:.2f}")

        # Orders should ideally be prioritized (e.g., sells before buys if cash is needed)
        # This basic implementation doesn't handle that.
        return orders
</code>

kamikaze_komodo/portfolio_constructor/__init__.py:
<code>
# kamikaze_komodo/portfolio_constructor/__init__.py
# This file makes the 'portfolio_constructor' directory a Python package.
</code>

kamikaze_komodo/risk_control_module/position_sizer.py:
<code>
# kamikaze_komodo/risk_control_module/position_sizer.py
from abc import ABC, abstractmethod
from typing import Dict, Optional, Any, Tuple
import numpy as np
from kamikaze_komodo.core.models import BarData # For ATR based sizers potentially
from kamikaze_komodo.app_logger import get_logger
logger = get_logger(__name__)

class BasePositionSizer(ABC):
    """
    Abstract base class for position sizing strategies.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float, # Total equity
        strategy_signal_strength: Optional[float] = None, # e.g. ML confidence
        latest_bar: Optional[BarData] = None, # For ATR or volatility based
        atr_value: Optional[float] = None # Explicit ATR if available
    ) -> Optional[float]: # Returns position size in asset units, or None if no trade
        """
        Calculates the size of the position to take.
        Args:
            symbol (str): The asset symbol.
            current_price (float): The current price of the asset.
            available_capital (float): The cash available for trading. (May not be used by all sizers)
            current_portfolio_value (float): The total current value of the portfolio (equity).
            strategy_signal_strength (Optional[float]): Confidence or strength of the signal.
            latest_bar (Optional[BarData]): Latest bar data for volatility calculation.
            atr_value (Optional[float]): Pre-calculated ATR value.
        Returns:
            Optional[float]: The quantity of the asset to trade. None if cannot size or no trade.
        """
        pass

class FixedFractionalPositionSizer(BasePositionSizer):
    """
    Sizes positions based on a fixed fraction of the total portfolio equity.
    """
    def __init__(self, fraction: float = 0.01, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        # fraction is often sourced from config: FixedFractional_AllocationFraction
        self.fraction_to_allocate = float(self.params.get('fixedfractional_allocationfraction', fraction))
        if not 0 < self.fraction_to_allocate <= 1.0:
            logger.error(f"Fraction must be between 0 (exclusive) and 1 (inclusive). Got {self.fraction_to_allocate}")
            raise ValueError("Fraction must be > 0 and <= 1.")
        logger.info(f"FixedFractionalPositionSizer initialized with fraction: {self.fraction_to_allocate}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float, # Cash
        current_portfolio_value: float, # Equity
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        if current_price <= 0:
            logger.warning(f"Current price for {symbol} is non-positive ({current_price}). Cannot calculate position size.")
            return None
        if current_portfolio_value <= 0:
            logger.warning(f"Current portfolio value is non-positive ({current_portfolio_value}). Cannot calculate position size.")
            return None
        
        capital_to_allocate = current_portfolio_value * self.fraction_to_allocate
        
        if capital_to_allocate > available_capital :
            logger.warning(f"Calculated capital to allocate ({capital_to_allocate:.2f}) for {symbol} exceeds available cash ({available_capital:.2f}). Using available cash.")
            capital_to_allocate = available_capital
        
        if capital_to_allocate <= 1.0: # Minimum capital to allocate (e.g. $1)
            logger.info(f"Not enough capital to allocate for {symbol} based on fixed fraction ({capital_to_allocate:.2f}). Min trade value not met.")
            return None

        position_size = capital_to_allocate / current_price
        logger.info(f"FixedFractional Sizing for {symbol}: Allocating ${capital_to_allocate:.2f} (Equity: ${current_portfolio_value:.2f}, Fraction: {self.fraction_to_allocate}). Position Size: {position_size:.8f} units at ${current_price:.4f}.")
        return position_size

class ATRBasedPositionSizer(BasePositionSizer):
    """
    Sizes positions based on Average True Range (ATR) to normalize risk per trade.
    This implementation assumes you risk a fixed percentage of portfolio equity,
    and the stop loss is N * ATR away.
    """
    def __init__(self, risk_per_trade_fraction: float = 0.01, atr_multiple_for_stop: float = 2.0, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        # Params often from config: ATRBased_RiskPerTradeFraction, ATRBased_ATRMultipleForStop
        self.risk_per_trade_fraction = float(self.params.get('atrbased_riskpertradefraction', risk_per_trade_fraction))
        self.atr_multiple_for_stop = float(self.params.get('atrbased_atrmultipleforstop', atr_multiple_for_stop))
        logger.info(f"ATRBasedPositionSizer initialized with risk_fraction: {self.risk_per_trade_fraction}, atr_multiple: {self.atr_multiple_for_stop}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None # Allow passing pre-calculated ATR
    ) -> Optional[float]:
        
        effective_atr = atr_value
        if effective_atr is None and latest_bar and latest_bar.atr is not None:
            effective_atr = latest_bar.atr
        
        if effective_atr is None or effective_atr <= 1e-8: # Check for very small or zero ATR
            logger.warning(f"ATR value for {symbol} is missing, zero or invalid ({effective_atr}). Cannot size using ATRBasedPositionSizer.")
            return None
        
        if current_price <= 0:
            logger.warning(f"Current price for {symbol} is non-positive ({current_price}). Cannot size position.")
            return None
        if current_portfolio_value <= 0:
            logger.warning(f"Current portfolio value is non-positive ({current_portfolio_value}). Cannot size position.")
            return None

        capital_to_risk = current_portfolio_value * self.risk_per_trade_fraction
        
        stop_distance_per_unit = self.atr_multiple_for_stop * effective_atr
        if stop_distance_per_unit <= 1e-8: # Avoid division by zero or tiny stop
            logger.warning(f"Stop distance per unit is zero or too small for {symbol} (ATR: {effective_atr}, Multiple: {self.atr_multiple_for_stop}). Cannot size position.")
            return None
            
        position_size = capital_to_risk / stop_distance_per_unit
        
        position_cost = position_size * current_price
        if position_cost > available_capital:
            logger.warning(f"Calculated position cost (${position_cost:.2f}) for {symbol} exceeds available cash (${available_capital:.2f}). Reducing size to available cash.")
            position_size = available_capital / current_price 
            if position_size <= 1e-8 : return None # Ensure size is not effectively zero

        logger.info(f"ATRBased Sizing for {symbol}: Risking ${capital_to_risk:.2f} (Equity: ${current_portfolio_value:.2f}). "
                    f"ATR: {effective_atr:.6f}, StopDist: ${stop_distance_per_unit:.4f}. "
                    f"Calculated Size: {position_size:.8f} units at ${current_price:.4f}.")
        return position_size

class PairTradingPositionSizer(BasePositionSizer):
    """
    Sizes positions for a pair trade, aiming for dollar neutrality if configured.
    This sizer would return a tuple or dict with sizes for both legs.
    For now, let's make it return the size for ONE leg, assuming the strategy will call it twice
    or it's used in a context where dollar neutrality is applied to a total pair capital.
    A more advanced version would calculate sizes for both legs simultaneously.
    Simplified: calculate size for one leg based on total capital allocated to the pair.
    """
    def __init__(self, dollar_neutral: bool = True, fraction_of_equity_for_pair: float = 0.1, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.dollar_neutral = str(self.params.get('pairtradingpositionsizer_dollarneutral', dollar_neutral)).lower() == 'true'
        self.fraction_of_equity_for_pair = float(self.params.get('fraction_of_equity_for_pair', fraction_of_equity_for_pair))
        logger.info(f"PairTradingPositionSizer initialized. Dollar Neutral: {self.dollar_neutral}, Fraction for Pair: {self.fraction_of_equity_for_pair}")

    def calculate_size(
        self,
        symbol: str, # Symbol of the leg being sized
        current_price: float,
        available_capital: float, # Overall available cash
        current_portfolio_value: float, # Total equity
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None,
        # Additional params for pair trading
        other_leg_price: Optional[float] = None, # Price of the other asset in the pair
        hedge_ratio: Optional[float] = None # If sizing based on hedge ratio rather than pure dollar neutrality
    ) -> Optional[float]: # Returns size for the given 'symbol' leg
        
        if current_price <= 0: return None
        if current_portfolio_value <=0: return None

        # Capital allocated to the entire pair trade
        total_capital_for_pair_trade = current_portfolio_value * self.fraction_of_equity_for_pair

        if self.dollar_neutral:
            # Each leg gets half of the capital allocated to the pair trade.
            capital_for_this_leg = total_capital_for_pair_trade / 2.0
            
            # Ensure we don't allocate more than available cash for the whole pair (simplistic check)
            if total_capital_for_pair_trade > available_capital:
                logger.warning(f"Total capital for pair trade (${total_capital_for_pair_trade:.2f}) exceeds available cash (${available_capital:.2f}). Reducing allocation.")
                # This reduction needs to be applied carefully; for now, we'll cap this leg's capital based on a proportional reduction.
                # This is a rough adjustment. Proper handling involves checking margin and total cost of both legs.
                reduction_factor = available_capital / total_capital_for_pair_trade if total_capital_for_pair_trade > 0 else 0
                capital_for_this_leg *= reduction_factor

            if capital_for_this_leg <= 1.0: # Min capital for a leg
                logger.info(f"Not enough capital for leg {symbol} in pair trade ({capital_for_this_leg:.2f})")
                return None
            
            position_size = capital_for_this_leg / current_price
            logger.info(f"PairTrading Sizing (Dollar Neutral) for leg {symbol}: Capital for leg ${capital_for_this_leg:.2f}. Size: {position_size:.8f} units.")
            return position_size
        else:
            # Non-dollar neutral (e.g., based on hedge ratio or other logic) - complex, placeholder
            # This might involve the hedge_ratio to determine relative number of units.
            # For now, just implement a simple allocation for the one leg based on total pair capital.
            if total_capital_for_pair_trade > available_capital:
                 total_capital_for_pair_trade = available_capital # Cap at available cash

            if total_capital_for_pair_trade <= 1.0: return None # Min capital for the pair
            
            # This is naive if not dollar neutral and not using hedge ratio correctly.
            # Assume this leg takes its proportional share based on some other factor or fixed fraction.
            # For Phase 6, if not dollar neutral, it's underspecified here.
            # Let's assume it falls back to a simple fraction for this leg for now.
            capital_for_this_leg = total_capital_for_pair_trade # If only one leg is sized by this call.
            position_size = capital_for_this_leg / current_price
            logger.warning(f"PairTrading Sizing (Non-Dollar Neutral) for leg {symbol} is simplified. Allocating full pair capital portion ${capital_for_this_leg:.2f}. Size: {position_size:.8f} units.")
            return position_size


class OptimalFPositionSizer(BasePositionSizer):
    """
    Sizes positions based on Vince's Optimal f (Kelly Criterion variant).
    Requires win probability and payoff ratio, which are hard to estimate robustly.
    This is a placeholder for a more sophisticated implementation.
    """
    def __init__(self, win_probability: float = 0.55, payoff_ratio: float = 1.5, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.win_probability = float(self.params.get('optimalf_win_probability', win_probability))
        self.payoff_ratio = float(self.params.get('optimalf_payoff_ratio', payoff_ratio)) # AvgWin / AvgLoss
        if not (0 <= self.win_probability <= 1): raise ValueError("Win probability must be between 0 and 1.")
        if self.payoff_ratio <= 0: raise ValueError("Payoff ratio must be positive.")
        logger.info(f"OptimalFPositionSizer initialized. Win Prob: {self.win_probability}, Payoff Ratio: {self.payoff_ratio}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        strategy_signal_strength: Optional[float] = None,
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        if current_price <= 0 or current_portfolio_value <= 0: return None

        # Kelly Formula: f* = (W * P - L) / (W * L_ratio) where W=win prob, P=payoff, L=loss prob, L_ratio=payoff ratio
        # Simplified Kelly: f = W - ( (1-W) / R ) where W is win probability, R is payoff_ratio (AvgWin/AvgLoss)
        kelly_f = self.win_probability - ((1 - self.win_probability) / self.payoff_ratio)

        if kelly_f <= 0:
            logger.info(f"Optimal f ({kelly_f:.4f}) is zero or negative for {symbol}. No position taken.")
            return None
        
        # Use a fraction of Kelly (e.g., half Kelly) for risk reduction
        fractional_kelly = self.params.get('optimalf_kelly_fraction', 0.5) * kelly_f
        
        capital_to_allocate = current_portfolio_value * fractional_kelly
        
        if capital_to_allocate > available_capital:
            capital_to_allocate = available_capital
        if capital_to_allocate <= 1.0:
            logger.info(f"Not enough capital for {symbol} after Optimal F ({capital_to_allocate:.2f}).")
            return None

        position_size = capital_to_allocate / current_price
        logger.info(f"OptimalF Sizing for {symbol}: Kelly f*={kelly_f:.4f}, Using {fractional_kelly*100:.2f}% of equity. Allocating ${capital_to_allocate:.2f}. Size: {position_size:.8f} units.")
        return position_size


class MLConfidencePositionSizer(BasePositionSizer):
    """
    Sizes positions based on a base fraction of equity, modulated by ML model confidence.
    """
    def __init__(self, base_fraction: float = 0.05, min_alloc_fraction: float = 0.01, max_alloc_fraction: float = 0.2, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.base_fraction = float(self.params.get('mlconfidence_base_fraction', base_fraction))
        self.min_alloc_fraction = float(self.params.get('mlconfidence_min_alloc_fraction', min_alloc_fraction))
        self.max_alloc_fraction = float(self.params.get('mlconfidence_max_alloc_fraction', max_alloc_fraction))
        logger.info(f"MLConfidencePositionSizer: BaseFraction={self.base_fraction}, MinAlloc={self.min_alloc_fraction}, MaxAlloc={self.max_alloc_fraction}")

    def calculate_size(
        self,
        symbol: str,
        current_price: float,
        available_capital: float,
        current_portfolio_value: float,
        strategy_signal_strength: Optional[float] = None, # This is the ML confidence (0.0 to 1.0)
        latest_bar: Optional[BarData] = None,
        atr_value: Optional[float] = None
    ) -> Optional[float]:
        if current_price <= 0 or current_portfolio_value <= 0: return None
        
        if strategy_signal_strength is None:
            logger.warning("ML model confidence (strategy_signal_strength) not provided. Cannot use MLConfidencePositionSizer. Defaulting to no trade.")
            return None
        
        # Ensure confidence is within [0,1]
        confidence = max(0.0, min(1.0, strategy_signal_strength))
        
        # Modulate base fraction by confidence. Example: If confidence is high, use higher fraction.
        # Linear modulation: alloc_fraction = min_alloc + (max_alloc - min_alloc) * confidence
        # Or, scale base_fraction: alloc_fraction = base_fraction * (0.5 + confidence) # (scales from 0.5*base to 1.5*base)
        # Let's use a simpler direct scaling of base_fraction, capped by min/max.
        # Scaler: 0.5 (low conf) to 1.5 (high conf), centered at 1.0 for conf=0.5
        # confidence_scaler = 0.5 + confidence 
        # effective_fraction = self.base_fraction * confidence_scaler
        
        # More direct: if confidence is low (e.g. <0.5), use min_alloc. If high (e.g. >0.8), use max_alloc. Interpolate.
        # For simplicity, let's make it proportional to confidence, bounded by min/max overall allocation.
        # If confidence is 0.5, use base_fraction. If 1.0, use max_alloc. If 0.0 use min_alloc (or even zero).
        
        if confidence < 0.5: # Lower confidence scales down from base_fraction towards min_alloc_fraction
             # Interpolate between min_alloc_fraction and base_fraction
            # when confidence is 0, use min_alloc_fraction. when confidence is 0.5, use base_fraction.
            # slope = (base_fraction - min_alloc_fraction) / 0.5
            # effective_fraction = min_alloc_fraction + slope * confidence
            # Simpler: if confidence = 0 -> min_alloc, if confidence=0.5 -> base_fraction
            # scale_factor = confidence / 0.5 # confidence from 0 to 0.5 -> scale_factor from 0 to 1
            # effective_fraction = self.min_alloc_fraction + (self.base_fraction - self.min_alloc_fraction) * scale_factor
            # This is still a bit complex. Alternative:
            effective_fraction = self.base_fraction * (confidence * 1.5) # Scales from 0 to 0.75 * base_fraction for conf 0 to 0.5
        else: # Higher confidence scales up from base_fraction towards max_alloc_fraction
            # Interpolate between base_fraction and max_alloc_fraction
            # when confidence is 0.5, use base_fraction. when confidence is 1.0, use max_alloc_fraction
            # slope = (max_alloc_fraction - base_fraction) / 0.5
            # effective_fraction = base_fraction + slope * (confidence - 0.5)
             effective_fraction = self.base_fraction * (0.75 + (confidence-0.5)*1.5) # Scales from 0.75*base to 1.5*base for conf 0.5 to 1.0


        effective_fraction = np.clip(effective_fraction, self.min_alloc_fraction, self.max_alloc_fraction)

        capital_to_allocate = current_portfolio_value * effective_fraction
        
        if capital_to_allocate > available_capital:
            capital_to_allocate = available_capital
        if capital_to_allocate <= 1.0:
            logger.info(f"Not enough capital for {symbol} using ML confidence ({capital_to_allocate:.2f}). Confidence: {confidence:.2f}, Eff.Frac: {effective_fraction:.4f}")
            return None
            
        position_size = capital_to_allocate / current_price
        logger.info(f"MLConfidence Sizing for {symbol}: Confidence={confidence:.2f}, Eff.Alloc.Frac={effective_fraction:.4f}. Allocating ${capital_to_allocate:.2f}. Size: {position_size:.8f} units.")
        return position_size
</code>

kamikaze_komodo/risk_control_module/stop_manager.py:
<code>
# FILE: kamikaze_komodo/risk_control_module/stop_manager.py
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from kamikaze_komodo.core.models import BarData, Trade
from kamikaze_komodo.core.enums import OrderSide
from kamikaze_komodo.app_logger import get_logger
from datetime import timedelta

logger = get_logger(__name__)

class BaseStopManager(ABC):
    """
    Abstract base class for stop-loss and take-profit management.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        self.params = params if params is not None else {}
        logger.info(f"{self.__class__.__name__} initialized with params: {self.params}")

    @abstractmethod
    def check_stop_loss(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        bar_index: int # New param needed for time-based stops
    ) -> Optional[float]: # Returns stop price if triggered, else None
        """
        Checks if the stop-loss condition is met for the current trade.
        """
        pass

    @abstractmethod
    def check_take_profit(
        self,
        current_trade: Trade,
        latest_bar: BarData
    ) -> Optional[float]: # Returns take profit price if triggered, else None
        """
        Checks if the take-profit condition is met for the current trade.
        """
        pass

class PercentageStopManager(BaseStopManager):
    """
    Manages stops based on a fixed percentage from the entry price.
    """
    def __init__(self, stop_loss_pct: Optional[float] = None, take_profit_pct: Optional[float] = None, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.stop_loss_pct = float(self.params.get('percentagestop_losspct', stop_loss_pct if stop_loss_pct is not None else 0))
        self.take_profit_pct = float(self.params.get('percentagestop_takeprofitpct', take_profit_pct if take_profit_pct is not None else 0))

        if self.stop_loss_pct < 0 or self.stop_loss_pct >= 1.0 :
              if self.stop_loss_pct != 0:
                  raise ValueError("stop_loss_pct must be between 0 (inclusive, to disable) and 1 (exclusive).")
        if self.take_profit_pct < 0:
              if self.take_profit_pct != 0:
                  raise ValueError("take_profit_pct must be non-negative (0 to disable).")
        
        self.stop_loss_pct = None if self.stop_loss_pct == 0 else self.stop_loss_pct
        self.take_profit_pct = None if self.take_profit_pct == 0 else self.take_profit_pct
            
        logger.info(f"PercentageStopManager initialized. SL: {self.stop_loss_pct*100 if self.stop_loss_pct else 'N/A'}%, TP: {self.take_profit_pct*100 if self.take_profit_pct else 'N/A'}%")

    def check_stop_loss(self, current_trade: Trade, latest_bar: BarData, bar_index: int) -> Optional[float]:
        if not self.stop_loss_pct or not current_trade or current_trade.entry_price <= 0:
            return None

        if current_trade.side == OrderSide.BUY:
            stop_price = current_trade.entry_price * (1 - self.stop_loss_pct)
            if latest_bar.low <= stop_price:
                logger.info(f"STOP LOSS (BUY) triggered for trade {current_trade.id} ({current_trade.symbol}) at SL price {stop_price:.4f} (Bar Low: {latest_bar.low:.4f}, Entry: {current_trade.entry_price:.4f})")
                return stop_price
        elif current_trade.side == OrderSide.SELL:
            stop_price = current_trade.entry_price * (1 + self.stop_loss_pct)
            if latest_bar.high >= stop_price:
                logger.info(f"STOP LOSS (SELL) triggered for trade {current_trade.id} ({current_trade.symbol}) at SL price {stop_price:.4f} (Bar High: {latest_bar.high:.4f}, Entry: {current_trade.entry_price:.4f})")
                return stop_price
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData) -> Optional[float]:
        if not self.take_profit_pct or not current_trade or current_trade.entry_price <= 0:
            return None

        if current_trade.side == OrderSide.BUY:
            profit_price = current_trade.entry_price * (1 + self.take_profit_pct)
            if latest_bar.high >= profit_price:
                logger.info(f"TAKE PROFIT (BUY) triggered for trade {current_trade.id} ({current_trade.symbol}) at TP price {profit_price:.4f} (Bar High: {latest_bar.high:.4f}, Entry: {current_trade.entry_price:.4f})")
                return profit_price
        elif current_trade.side == OrderSide.SELL:
            profit_price = current_trade.entry_price * (1 - self.take_profit_pct)
            if latest_bar.low <= profit_price:
                logger.info(f"TAKE PROFIT (SELL) triggered for trade {current_trade.id} ({current_trade.symbol}) at TP price {profit_price:.4f} (Bar Low: {latest_bar.low:.4f}, Entry: {current_trade.entry_price:.4f})")
                return profit_price
        return None

class ATRStopManager(BaseStopManager):
    """
    Manages stops based on ATR.
    """
    def __init__(self, atr_multiple: float = 2.0, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.atr_multiple = float(self.params.get('atrstop_atrmultiple', atr_multiple))
        logger.info(f"ATRStopManager initialized with ATR multiple: {self.atr_multiple}")

    def check_stop_loss(self, current_trade: Trade, latest_bar: BarData, bar_index: int) -> Optional[float]:
        atr_at_entry = current_trade.custom_fields.get("atr_at_entry") if hasattr(current_trade, 'custom_fields') and current_trade.custom_fields else None
        
        if atr_at_entry is None or atr_at_entry <= 1e-8:
            if latest_bar.atr is not None and latest_bar.atr > 1e-8:
                logger.debug(f"ATR at entry not available for trade {current_trade.id}. Using latest_bar.atr ({latest_bar.atr:.6f}) for ATR stop check.")
                atr_at_entry = latest_bar.atr
            else:
                logger.debug(f"ATR value not available or invalid for trade {current_trade.id}. Cannot apply ATR stop. ATR at entry: {atr_at_entry}, Latest bar ATR: {latest_bar.atr}")
                return None
        
        stop_distance = self.atr_multiple * atr_at_entry
        if current_trade.side == OrderSide.BUY:
            stop_price = current_trade.entry_price - stop_distance
            if latest_bar.low <= stop_price:
                logger.info(f"ATR STOP LOSS (BUY) for {current_trade.symbol} at {stop_price:.4f} (Entry: {current_trade.entry_price:.4f}, ATR Used: {atr_at_entry:.6f}, BarLow: {latest_bar.low:.4f})")
                return stop_price
        elif current_trade.side == OrderSide.SELL:
            stop_price = current_trade.entry_price + stop_distance
            if latest_bar.high >= stop_price:
                logger.info(f"ATR STOP LOSS (SELL) for {current_trade.symbol} at {stop_price:.4f} (Entry: {current_trade.entry_price:.4f}, ATR Used: {atr_at_entry:.6f}, BarHigh: {latest_bar.high:.4f})")
                return stop_price
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData) -> Optional[float]:
        return None

class TripleBarrierStopManager(BaseStopManager):
    """
    Implements De Prado's Triple-Barrier Method.
    Sets a stop-loss, a take-profit, and a time-based exit.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.tp_multiple = float(self.params.get('triplebarrier_tp_multiple', 1.5))  # Take profit as a multiple of risk (ATR)
        self.sl_multiple = float(self.params.get('triplebarrier_sl_multiple', 1.0))  # Stop loss as a multiple of risk (ATR)
        self.time_limit_bars = int(self.params.get('triplebarrier_time_limit_bars', 10)) # Time limit in number of bars
        logger.info(f"TripleBarrierStopManager initialized. TP Multiple: {self.tp_multiple}, SL Multiple: {self.sl_multiple}, Time Limit: {self.time_limit_bars} bars.")

    def _get_risk_and_barriers(self, trade: Trade, bar: BarData) -> Optional[Dict[str, float]]:
        """Calculates the risk (e.g., ATR at entry) and derives the barriers."""
        risk_per_unit = trade.custom_fields.get("atr_at_entry")
        if risk_per_unit is None or risk_per_unit <= 1e-8:
            logger.warning(f"ATR at entry not found for trade {trade.id}. Cannot apply Triple Barrier.")
            return None

        entry_price = trade.entry_price
        if trade.side == OrderSide.BUY:
            sl_price = entry_price - (self.sl_multiple * risk_per_unit)
            tp_price = entry_price + (self.tp_multiple * risk_per_unit)
        else: # OrderSide.SELL
            sl_price = entry_price + (self.sl_multiple * risk_per_unit)
            tp_price = entry_price - (self.tp_multiple * risk_per_unit)

        return {"sl_price": sl_price, "tp_price": tp_price}

    def check_stop_loss(self, current_trade: Trade, latest_bar: BarData, bar_index: int) -> Optional[float]:
        barriers = self._get_risk_and_barriers(current_trade, latest_bar)
        if barriers is None:
            return None

        sl_price = barriers['sl_price']
        
        # 1. Price-based stop loss
        if current_trade.side == OrderSide.BUY and latest_bar.low <= sl_price:
            logger.info(f"Triple-Barrier SL (BUY) triggered for trade {current_trade.id} at {sl_price:.4f}")
            return sl_price
        if current_trade.side == OrderSide.SELL and latest_bar.high >= sl_price:
            logger.info(f"Triple-Barrier SL (SELL) triggered for trade {current_trade.id} at {sl_price:.4f}")
            return sl_price
        
        # 2. Time-based stop (vertical barrier)
        entry_bar_index = current_trade.custom_fields.get("entry_bar_index")
        if entry_bar_index is not None:
            bars_held = bar_index - entry_bar_index
            if bars_held >= self.time_limit_bars:
                logger.info(f"Triple-Barrier TIME LIMIT reached for trade {current_trade.id} after {bars_held} bars.")
                return latest_bar.close # Exit at the current closing price
        
        return None

    def check_take_profit(self, current_trade: Trade, latest_bar: BarData) -> Optional[float]:
        barriers = self._get_risk_and_barriers(current_trade, latest_bar)
        if barriers is None:
            return None

        tp_price = barriers['tp_price']
        
        if current_trade.side == OrderSide.BUY and latest_bar.high >= tp_price:
            logger.info(f"Triple-Barrier TP (BUY) triggered for trade {current_trade.id} at {tp_price:.4f}")
            return tp_price
        if current_trade.side == OrderSide.SELL and latest_bar.low <= tp_price:
            logger.info(f"Triple-Barrier TP (SELL) triggered for trade {current_trade.id} at {tp_price:.4f}")
            return tp_price
        
        return None
</code>

kamikaze_komodo/risk_control_module/volatility_band_stop_manager.py:
<code>
# kamikaze_komodo/risk_control_module/volatility_band_stop_manager.py
from typing import Optional, Dict, Any
import pandas as pd
import pandas_ta as ta
from kamikaze_komodo.risk_control_module.stop_manager import BaseStopManager
from kamikaze_komodo.core.models import BarData, Trade
from kamikaze_komodo.core.enums import OrderSide
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class VolatilityBandStopManager(BaseStopManager):
    """
    Manages stops based on volatility bands like Bollinger Bands or Keltner Channels.
    Can be used for trailing stops along the bands.
    """
    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(params)
        self.band_type = self.params.get('volatilitybandstop_band_type', 'bollinger').lower() # 'bollinger' or 'keltner'
        
        # Bollinger Band params
        self.bb_period = int(self.params.get('volatilitybandstop_bb_period', 20))
        self.bb_std_dev = float(self.params.get('volatilitybandstop_bb_stddev', 2.0))
        
        # Keltner Channel params (if used)
        self.kc_period = int(self.params.get('volatilitybandstop_kc_period', 20)) # EMA period
        self.kc_atr_period = int(self.params.get('volatilitybandstop_kc_atr_period', 10))
        self.kc_atr_multiplier = float(self.params.get('volatilitybandstop_kc_atr_multiplier', 1.5))

        self.trail_type = self.params.get('volatilitybandstop_trailtype', 'none').lower() # e.g., 'trailing_bb_upper', 'trailing_bb_lower', 'none'
        
        # Store current stop levels if trailing
        self.current_trailing_stop_price: Optional[float] = None

        logger.info(f"VolatilityBandStopManager initialized. Band: {self.band_type}, Trail: {self.trail_type}")

    def _calculate_bands(self, data_history: pd.DataFrame) -> pd.DataFrame:
        df = data_history.copy()
        if df.empty or len(df) < max(self.bb_period, self.kc_period, self.kc_atr_period):
            return df # Not enough data

        if self.band_type == 'bollinger':
            if 'close' in df.columns and len(df) >= self.bb_period:
                try:
                    bbands = ta.bbands(df['close'], length=self.bb_period, std=self.bb_std_dev)
                    if bbands is not None and not bbands.empty:
                        # pandas_ta typically names columns like BBL_20_2.0, BBM_20_2.0, BBU_20_2.0
                        df['band_lower'] = bbands[f'BBL_{self.bb_period}_{self.bb_std_dev:.1f}']
                        df['band_middle'] = bbands[f'BBM_{self.bb_period}_{self.bb_std_dev:.1f}']
                        df['band_upper'] = bbands[f'BBU_{self.bb_period}_{self.bb_std_dev:.1f}']
                except Exception as e:
                    logger.error(f"Error calculating Bollinger Bands for VolStopManager: {e}")
                    df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
            else:
                 df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA

        elif self.band_type == 'keltner':
            if all(c in df.columns for c in ['high', 'low', 'close']) and len(df) >= max(self.kc_period, self.kc_atr_period):
                try:
                    kc = ta.kc(df['high'], df['low'], df['close'], length=self.kc_period, atr_length=self.kc_atr_period, mamode="EMA", multiplier=self.kc_atr_multiplier)
                    if kc is not None and not kc.empty:
                        # Column names from pandas_ta for Keltner might be like KCLer_20_10_1.5, KCMer_20_10_1.5, KCUer_20_10_1.5
                        # Need to verify exact names or use generic ones if possible. Let's assume standard:
                        df['band_lower'] = kc.iloc[:,0] # Lower band often first column
                        df['band_middle'] = kc.iloc[:,1] # Middle band
                        df['band_upper'] = kc.iloc[:,2] # Upper band
                except Exception as e:
                    logger.error(f"Error calculating Keltner Channels for VolStopManager: {e}")
                    df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
            else:
                 df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
        else:
            logger.warning(f"Unsupported band_type: {self.band_type} in VolatilityBandStopManager.")
            df['band_lower'] = df['band_middle'] = df['band_upper'] = pd.NA
        return df

    def check_stop_loss(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        data_history_for_bands: Optional[pd.DataFrame] = None # Pass full history for band calculation
    ) -> Optional[float]:
        if not data_history_for_bands or data_history_for_bands.empty:
            logger.warning("Data history for bands not provided to VolatilityBandStopManager.")
            return None

        df_with_bands = self._calculate_bands(data_history_for_bands)
        if df_with_bands.empty or 'band_lower' not in df_with_bands.columns or 'band_upper' not in df_with_bands.columns:
            logger.debug("Bands not available for stop loss check.")
            return None
        
        latest_band_lower = df_with_bands['band_lower'].iloc[-1]
        latest_band_upper = df_with_bands['band_upper'].iloc[-1]

        if pd.isna(latest_band_lower) or pd.isna(latest_band_upper):
            logger.debug("Latest band values are NaN.")
            return None

        stop_price = None

        if self.trail_type == 'none': # Fixed stop based on band at entry (requires band_at_entry)
            # This simple version will use current bands as stop.
            # For entry-based band stop, band value at entry should be stored in Trade.custom_fields
            if current_trade.side == OrderSide.BUY:
                stop_price = latest_band_lower # Simplistic: stop at current lower band
                if latest_bar.low <= stop_price:
                    logger.info(f"VOL_BAND STOP (BUY, fixed on current) for {current_trade.symbol} at {stop_price:.4f}")
                    return stop_price
            elif current_trade.side == OrderSide.SELL:
                stop_price = latest_band_upper # Simplistic: stop at current upper band
                if latest_bar.high >= stop_price:
                    logger.info(f"VOL_BAND STOP (SELL, fixed on current) for {current_trade.symbol} at {stop_price:.4f}")
                    return stop_price
        else: # Trailing stop logic
            if current_trade.side == OrderSide.BUY:
                # Trail stop along the lower band (or middle band)
                potential_stop = latest_band_lower # Default to lower band for long
                if self.trail_type == 'trailing_bb_middle' or self.trail_type == 'trailing_kc_middle':
                     if 'band_middle' in df_with_bands.columns and pd.notna(df_with_bands['band_middle'].iloc[-1]):
                        potential_stop = df_with_bands['band_middle'].iloc[-1]
                
                if self.current_trailing_stop_price is None or potential_stop > self.current_trailing_stop_price:
                    self.current_trailing_stop_price = potential_stop
                
                if self.current_trailing_stop_price and latest_bar.low <= self.current_trailing_stop_price:
                    logger.info(f"VOL_BAND TRAILING STOP (BUY) for {current_trade.symbol} at {self.current_trailing_stop_price:.4f}")
                    stop_price_to_return = self.current_trailing_stop_price
                    self.current_trailing_stop_price = None # Reset after hit
                    return stop_price_to_return

            elif current_trade.side == OrderSide.SELL:
                # Trail stop along the upper band (or middle band)
                potential_stop = latest_band_upper # Default to upper band for short
                if self.trail_type == 'trailing_bb_middle' or self.trail_type == 'trailing_kc_middle':
                     if 'band_middle' in df_with_bands.columns and pd.notna(df_with_bands['band_middle'].iloc[-1]):
                        potential_stop = df_with_bands['band_middle'].iloc[-1]

                if self.current_trailing_stop_price is None or potential_stop < self.current_trailing_stop_price:
                    self.current_trailing_stop_price = potential_stop

                if self.current_trailing_stop_price and latest_bar.high >= self.current_trailing_stop_price:
                    logger.info(f"VOL_BAND TRAILING STOP (SELL) for {current_trade.symbol} at {self.current_trailing_stop_price:.4f}")
                    stop_price_to_return = self.current_trailing_stop_price
                    self.current_trailing_stop_price = None # Reset after hit
                    return stop_price_to_return
        return None

    def check_take_profit(
        self,
        current_trade: Trade,
        latest_bar: BarData,
        # data_history_for_bands: Optional[pd.DataFrame] = None # If TP uses bands
    ) -> Optional[float]:
        # Volatility bands are typically used for stops or dynamic exits, not fixed TP.
        # Could implement TP if price touches opposite band, e.g.
        # For now, this manager focuses on stop-loss.
        # Reset trailing stop if trade is closed by other means (e.g. strategy signal)
        if self.current_trailing_stop_price is not None and current_trade.exit_timestamp is not None:
             self.current_trailing_stop_price = None
        return None

    def reset_trailing_stop(self):
        """Called when a new trade is initiated or an old one is closed by other means."""
        self.current_trailing_stop_price = None
</code>

kamikaze_komodo/risk_control_module/__init__.py:
<code>
# kamikaze_komodo/risk_control_module/__init__.py
# This file makes the 'risk_control_module' directory a Python package.
logger_name = "KamikazeKomodo.risk_control_module" # Satisfy linter
</code>

kamikaze_komodo/strategy_framework/base_strategy.py:
<code>
# kamikaze_komodo/strategy_framework/base_strategy.py
# Updated to include optional sentiment_score in on_bar_data
# Updated update_data_history for new BarData fields
# Phase 6: Added market_regime to BarData and data_history.
# Phase 6: Added enable_shorting parameter.
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Union # Added List, Union
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
import pandas as pd
from pydantic import BaseModel # Ensure pydantic.BaseModel is imported



logger = get_logger(__name__)

class SignalCommand(BaseModel):
    signal_type: SignalType
    symbol: str 
    price: Optional[float] = None
    # Add amount if strategy determines it, otherwise position sizer will.
    # amount: Optional[float] = None 
    related_bar_data: Optional[BarData] = None
    # For pair trades, might include specific instructions for each leg
    custom_params: Optional[Dict[str, Any]] = None
    
class BaseStrategy(ABC):
    """
    Abstract base class for all trading strategies.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        self.symbol = symbol # Primary symbol for single-asset strategies or one leg of a pair
        self.timeframe = timeframe
        self.params = params if params is not None else {}
        self.current_position_status: Optional[SignalType] = None # Tracks if currently LONG, SHORT or None (no position)
        
        # Phase 6: Enable shorting based on strategy parameters
        self.enable_shorting: bool = self.params.get('enableshorting', False) # Default to False if not specified
        if isinstance(self.enable_shorting, str): # Handle string 'True'/'False' from config
            self.enable_shorting = self.enable_shorting.lower() == 'true'

        # Initialize data_history with potential columns including new ones from BarData
        self.data_history = pd.DataFrame(columns=[
            'open', 'high', 'low', 'close', 'volume', 
            'atr', 'sentiment_score', 
            'prediction_value', 'prediction_confidence', # New Phase 5 fields
            'market_regime' # New Phase 6 field
        ])
        logger.info(f"Initialized BaseStrategy '{self.name}' for {symbol} ({timeframe}) with params: {self.params}. Shorting enabled: {self.enable_shorting}")

    @abstractmethod
    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        """
        Generates trading signals based on the provided historical data.
        This method is typically called once during backtesting setup or for historical analysis.
        Args:
            data (pd.DataFrame): DataFrame with historical OHLCV data, indexed by timestamp.
                                 Expected columns: 'open', 'high', 'low', 'close', 'volume'.
                                 May also contain 'atr', 'sentiment_score', 'prediction_value', 
                                 'prediction_confidence', 'market_regime'.
            sentiment_series (Optional[pd.Series]): Series with historical sentiment scores, indexed by timestamp.
        Returns:
            pd.Series: A Pandas Series indexed by timestamp, containing SignalType values.
        """
        pass

    @abstractmethod
    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        """
        Processes a new bar of data and decides on a trading action.
        This method is typically called for each new data point in a live or simulated environment.
        Can return a single SignalType or a list of SignalCommands for multi-leg strategies.
        Args:
            bar_data (BarData): The new BarData object, potentially with .atr or .sentiment_score,
                                .prediction_value, .prediction_confidence, .market_regime populated.
            sentiment_score (Optional[float]): External sentiment score for the current bar.
                                               (Note: bar_data.sentiment_score might also be used if populated by engine)
            market_regime_data (Optional[Any]): External market regime data for current bar.
                                                (Note: bar_data.market_regime might also be used if populated by engine)
        Returns:
            Union[Optional[SignalType], List[SignalCommand]]: 
                - A single signal (LONG, SHORT, HOLD, CLOSE_LONG, CLOSE_SHORT) or None if no action.
                - A list of SignalCommand objects for multi-leg trades (e.g., pair trading).
        """
        pass
        
    def update_data_history(self, new_bar_data: BarData):
        """Appends new bar data to the internal history including ATR, sentiment, prediction, and regime fields if available."""
        new_row_data = {
            'open': new_bar_data.open, 'high': new_bar_data.high,
            'low': new_bar_data.low, 'close': new_bar_data.close,
            'volume': new_bar_data.volume,
            'atr': new_bar_data.atr if hasattr(new_bar_data, 'atr') else None,
            'sentiment_score': new_bar_data.sentiment_score if hasattr(new_bar_data, 'sentiment_score') else None,
            'prediction_value': new_bar_data.prediction_value if hasattr(new_bar_data, 'prediction_value') else None,
            'prediction_confidence': new_bar_data.prediction_confidence if hasattr(new_bar_data, 'prediction_confidence') else None,
            'market_regime': new_bar_data.market_regime if hasattr(new_bar_data, 'market_regime') else None, # Phase 6
        }
            
        new_row = pd.DataFrame([new_row_data], index=[new_bar_data.timestamp])
        
        # Ensure columns match, adding missing ones with NaN if this is the first row with new columns
        if not self.data_history.empty:
            for col in new_row.columns:
                if col not in self.data_history.columns:
                    self.data_history[col] = pd.NA 
            for col in self.data_history.columns:
                if col not in new_row.columns:
                    new_row[col] = pd.NA
        self.data_history = pd.concat([self.data_history, new_row[self.data_history.columns.union(new_row.columns)]]) # Ensure all columns are preserved

        # Optional: Keep only a certain number of recent rows to manage memory
        # max_history_length = self.params.get('max_history_length', 1000) # Example: 1000 bars
        # if len(self.data_history) > max_history_length:
        #     self.data_history = self.data_history.iloc[-max_history_length:]

    def get_parameters(self) -> Dict[str, Any]:
        return self.params

    def set_parameters(self, params: Dict[str, Any]):
        self.params.update(params)
        # Update shorting capability if specified in new params
        if 'enableshorting' in self.params:
            self.enable_shorting = str(self.params['enableshorting']).lower() == 'true'
        logger.info(f"Strategy {self.__class__.__name__} parameters updated: {self.params}. Shorting enabled: {self.enable_shorting}")

    @property
    def name(self) -> str:
        return self.__class__.__name__

# Add Pydantic BaseModel for SignalCommand if not already defined elsewhere (e.g., in core.models if broadly used)
# For now, defining it here for clarity in BaseStrategy context.

</code>

kamikaze_komodo/strategy_framework/strategy_manager.py:
<code>
# kamikaze_komodo/strategy_framework/strategy_manager.py
from typing import List, Dict, Any
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.app_logger import get_logger
logger = get_logger(__name__)
class StrategyManager:
    """
    Manages the loading, initialization, and execution of trading strategies.
    """
    def __init__(self):
        self.strategies: List[BaseStrategy] = []
        logger.info("StrategyManager initialized.")
    def add_strategy(self, strategy: BaseStrategy):
        """Adds a strategy instance to the manager."""
        if not isinstance(strategy, BaseStrategy):
            logger.error("Attempted to add an invalid strategy object.")
            raise ValueError("Strategy must be an instance of BaseStrategy.")
        
        self.strategies.append(strategy)
        logger.info(f"Strategy '{strategy.name}' for {strategy.symbol} ({strategy.timeframe}) added to StrategyManager.")
    def remove_strategy(self, strategy_name: str, symbol: str, timeframe: str):
        """Removes a strategy by its name, symbol, and timeframe."""
        initial_count = len(self.strategies)
        self.strategies = [
            s for s in self.strategies 
            if not (s.name == strategy_name and s.symbol == symbol and s.timeframe == timeframe)
        ]
        if len(self.strategies) < initial_count:
            logger.info(f"Strategy '{strategy_name}' for {symbol} ({timeframe}) removed.")
        else:
            logger.warning(f"Strategy '{strategy_name}' for {symbol} ({timeframe}) not found for removal.")
    def load_strategies_from_config(self, config: Dict[str, Any]):
        """
        Loads strategies based on a configuration dictionary.
        This is a placeholder for a more dynamic loading mechanism.
        For now, strategies are added manually or via specific calls.
        """
        # Example:
        # for strategy_config in config.get('strategies', []):
        #     strategy_class = resolve_strategy_class(strategy_config['name']) # Utility to get class from name
        #     params = strategy_config.get('params', {})
        #     symbol = strategy_config.get('symbol')
        #     timeframe = strategy_config.get('timeframe')
        #     if strategy_class and symbol and timeframe:
        #         self.add_strategy(strategy_class(symbol, timeframe, params))
        logger.warning("load_strategies_from_config is a placeholder and not fully implemented.")
        pass
    def on_bar_data_all(self, bar_data: BarData) -> Dict[str, SignalType]:
        """
        Distributes new bar data to all relevant strategies and collects signals.
        A strategy is relevant if the bar_data.symbol and bar_data.timeframe match.
        Returns:
            Dict[str, SignalType]: A dictionary where keys are strategy identifiers
                                   (e.g., "EWMACStrategy_BTC/USD_1h") and values are signals.
        """
        signals_from_strategies: Dict[str, SignalType] = {}
        for strategy in self.strategies:
            if strategy.symbol == bar_data.symbol and strategy.timeframe == bar_data.timeframe:
                signal = strategy.on_bar_data(bar_data)
                if signal: # Only record actual signals, not None or HOLD if not meaningful here
                    strategy_id = f"{strategy.name}_{strategy.symbol.replace('/', '')}_{strategy.timeframe}"
                    signals_from_strategies[strategy_id] = signal
                    logger.debug(f"Signal from {strategy_id}: {signal.name}")
        return signals_from_strategies
    def get_all_strategies(self) -> List[BaseStrategy]:
        return self.strategies
# Example Usage (Conceptual)
if __name__ == '__main__':
    from kamikaze_komodo.strategy_framework.strategies.ewmac import EWMACStrategy
    from kamikaze_komodo.config.settings import settings # Assuming settings are loaded
    if settings:
        manager = StrategyManager()
        
        # Create and add a strategy instance
        ewmac_params = {
            'short_window': settings.ewmac_short_window,
            'long_window': settings.ewmac_long_window
        }
        ewmac_btc_1h = EWMACStrategy(symbol="BTC/USD", timeframe="1h", params=ewmac_params)
        manager.add_strategy(ewmac_btc_1h)
        # Simulate receiving bar data
        # In a real system, this BarData would come from DataFetcher
        from datetime import datetime, timezone
        example_bar = BarData(
            timestamp=datetime.now(timezone.utc),
            open=40000, high=40500, low=39800, close=40200, volume=100,
            symbol="BTC/USD", timeframe="1h"
        )
        # To actually get a signal, the strategy needs historical data first.
        # This is a simplified call. `ewmac_btc_1h.update_data_history(bar)` would need to be called many times first.
        # For a single bar without history, it will likely return HOLD or an error if not enough data.
        # signals = manager.on_bar_data_all(example_bar)
        # logger.info(f"Signals received: {signals}")
        logger.info("StrategyManager example completed. For meaningful signals, strategies need historical data.")
    else:
        logger.error("Settings not loaded, cannot run StrategyManager example.")
</code>

kamikaze_komodo/strategy_framework/__init__.py:
<code>
# kamikaze_komodo/strategy_framework/__init__.py
# This file makes the 'strategy_framework' directory a Python package.
</code>

kamikaze_komodo/strategy_framework/strategies/bollinger_band_breakout_strategy.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/bollinger_band_breakout_strategy.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class BollingerBandBreakoutStrategy(BaseStrategy):
    """
    Implements a Bollinger Band Breakout strategy.
    Enters on price breakouts from Bollinger Bands, potentially filtered by volume or momentum.
    Adaptable for long and short positions.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        self.bb_period = int(self.params.get('bb_period', 20))
        self.bb_std_dev = float(self.params.get('bb_std_dev', 2.0))
        self.atr_period = int(self.params.get('atr_period', 14)) # For ATR-based filters or stops

        # Optional filters
        self.volume_filter_enabled = str(self.params.get('volume_filter_enabled', 'false')).lower() == 'true'
        self.volume_sma_period = int(self.params.get('volume_sma_period', 20))
        self.volume_factor_above_sma = float(self.params.get('volume_factor_above_sma', 1.5))
        self.min_breakout_atr_multiple = float(self.params.get('min_breakout_atr_multiple', 0.0)) # 0 means no filter

        logger.info(
            f"Initialized BollingerBandBreakoutStrategy for {symbol} ({timeframe}) "
            f"with BB Period: {self.bb_period}, StdDev: {self.bb_std_dev}, ATR Period: {self.atr_period}. "
            f"Volume Filter: {self.volume_filter_enabled} (SMA {self.volume_sma_period}, Factor {self.volume_factor_above_sma}). "
            f"Min Breakout ATR Multiple: {self.min_breakout_atr_multiple}. Shorting Enabled: {self.enable_shorting}"
        )

    def _calculate_indicators(self, data_df: pd.DataFrame) -> pd.DataFrame:
        if data_df.empty: return data_df
        df = data_df.copy()

        if 'close' not in df.columns or len(df) < self.bb_period:
            return df

        # Bollinger Bands
        try:
            bbands = ta.bbands(df['close'], length=self.bb_period, std=self.bb_std_dev)
            if bbands is not None and not bbands.empty:
                df['bb_lower'] = bbands[f'BBL_{self.bb_period}_{self.bb_std_dev:.1f}'] # pandas_ta uses .1f for std in col name
                df['bb_middle'] = bbands[f'BBM_{self.bb_period}_{self.bb_std_dev:.1f}']
                df['bb_upper'] = bbands[f'BBU_{self.bb_period}_{self.bb_std_dev:.1f}']
            else:
                df['bb_lower'] = pd.NA
                df['bb_middle'] = pd.NA
                df['bb_upper'] = pd.NA
        except Exception as e:
            logger.error(f"Error calculating Bollinger Bands: {e}")
            df['bb_lower'] = pd.NA
            df['bb_middle'] = pd.NA
            df['bb_upper'] = pd.NA


        # ATR
        if all(col in df.columns for col in ['high', 'low', 'close']):
            if len(df) >= self.atr_period:
                df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)
            else:
                df['atr'] = pd.NA
        else:
            df['atr'] = pd.NA
        
        # Volume SMA (if filter enabled)
        if self.volume_filter_enabled and 'volume' in df.columns:
            if len(df) >= self.volume_sma_period:
                df['volume_sma'] = ta.sma(df['volume'], length=self.volume_sma_period)
            else:
                df['volume_sma'] = pd.NA
        elif 'volume' not in df.columns and self.volume_filter_enabled:
            logger.warning("Volume column not found for volume filter in BollingerBandBreakoutStrategy.")
            df['volume_sma'] = pd.NA

        return df

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        if data.empty or len(data) < self.bb_period:
            logger.warning(f"Not enough historical data for Bollinger Band Breakout signals. Need {self.bb_period}, got {len(data)}.")
            return pd.Series(index=data.index, dtype='object')

        df_processed = self._calculate_indicators(data)

        if 'bb_upper' not in df_processed.columns or 'bb_lower' not in df_processed.columns:
            logger.error("Bollinger Band columns not found after calculation. Cannot generate signals.")
            return pd.Series(index=data.index, dtype='object')

        signals = pd.Series(index=df_processed.index, dtype='object').fillna(SignalType.HOLD)
        current_pos_state: Optional[SignalType] = None

        for i in range(1, len(df_processed)): # Start from 1 to check previous bar conditions
            close = df_processed['close'].iloc[i]
            prev_close = df_processed['close'].iloc[i-1]
            bb_upper = df_processed['bb_upper'].iloc[i]
            prev_bb_upper = df_processed['bb_upper'].iloc[i-1] if i > 0 and 'bb_upper' in df_processed.columns and pd.notna(df_processed['bb_upper'].iloc[i-1]) else bb_upper

            bb_lower = df_processed['bb_lower'].iloc[i]
            prev_bb_lower = df_processed['bb_lower'].iloc[i-1] if i > 0 and 'bb_lower' in df_processed.columns and pd.notna(df_processed['bb_lower'].iloc[i-1]) else bb_lower
            
            atr = df_processed['atr'].iloc[i] if 'atr' in df_processed.columns and pd.notna(df_processed['atr'].iloc[i]) else None

            if pd.isna(close) or pd.isna(prev_close) or pd.isna(bb_upper) or pd.isna(bb_lower) or pd.isna(prev_bb_upper) or pd.isna(prev_bb_lower):
                continue

            # Volume Filter Check
            volume_condition_met = True
            if self.volume_filter_enabled and 'volume' in df_processed.columns and 'volume_sma' in df_processed.columns:
                current_volume = df_processed['volume'].iloc[i]
                volume_sma = df_processed['volume_sma'].iloc[i]
                if pd.notna(current_volume) and pd.notna(volume_sma) and volume_sma > 0: # ensure sma is not zero
                    if current_volume < volume_sma * self.volume_factor_above_sma:
                        volume_condition_met = False
                # else: volume_condition_met remains True if data insufficient for filter
            
            # Breakout Candle Size Filter
            breakout_candle_filter_met = True
            if self.min_breakout_atr_multiple > 0 and atr is not None and atr > 0:
                candle_range = abs(df_processed['high'].iloc[i] - df_processed['low'].iloc[i])
                if candle_range < self.min_breakout_atr_multiple * atr:
                    breakout_candle_filter_met = False


            # Long entry: Close breaks above upper band
            long_breakout = close > bb_upper and prev_close <= prev_bb_upper
            
            # Short entry: Close breaks below lower band
            short_breakout = close < bb_lower and prev_close >= prev_bb_lower

            if current_pos_state == SignalType.LONG:
                # Exit Long: Price closes back inside, e.g., below upper band or hits middle band
                if close < bb_upper: # Simple exit: re-enters band
                    signals.iloc[i] = SignalType.CLOSE_LONG
                    current_pos_state = None
            elif current_pos_state == SignalType.SHORT:
                 # Exit Short: Price closes back inside, e.g., above lower band or hits middle band
                if close > bb_lower: # Simple exit: re-enters band
                    signals.iloc[i] = SignalType.CLOSE_SHORT
                    current_pos_state = None
            else: # No position
                if long_breakout and volume_condition_met and breakout_candle_filter_met:
                    signals.iloc[i] = SignalType.LONG
                    current_pos_state = SignalType.LONG
                elif short_breakout and self.enable_shorting and volume_condition_met and breakout_candle_filter_met:
                    signals.iloc[i] = SignalType.SHORT
                    current_pos_state = SignalType.SHORT
        
        logger.info(f"Generated Bollinger Band Breakout signals. Longs: {signals.eq(SignalType.LONG).sum()}, Shorts: {signals.eq(SignalType.SHORT).sum()}, CloseLongs: {signals.eq(SignalType.CLOSE_LONG).sum()}, CloseShorts: {signals.eq(SignalType.CLOSE_SHORT).sum()}")
        return signals

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(bar_data)
        if len(self.data_history) < self.bb_period + 1: # Need enough data for BBands and prev close check
            return SignalType.HOLD

        df_with_indicators = self._calculate_indicators(self.data_history)
        
        if 'atr' in df_with_indicators.columns and pd.notna(df_with_indicators['atr'].iloc[-1]):
            bar_data.atr = df_with_indicators['atr'].iloc[-1]

        if df_with_indicators.empty or 'bb_upper' not in df_with_indicators.columns or \
           'bb_lower' not in df_with_indicators.columns or len(df_with_indicators) < 2:
            return SignalType.HOLD

        latest_close = df_with_indicators['close'].iloc[-1]
        prev_close = df_with_indicators['close'].iloc[-2]
        latest_bb_upper = df_with_indicators['bb_upper'].iloc[-1]
        prev_bb_upper = df_with_indicators['bb_upper'].iloc[-2] if pd.notna(df_with_indicators['bb_upper'].iloc[-2]) else latest_bb_upper
        latest_bb_lower = df_with_indicators['bb_lower'].iloc[-1]
        prev_bb_lower = df_with_indicators['bb_lower'].iloc[-2] if pd.notna(df_with_indicators['bb_lower'].iloc[-2]) else latest_bb_lower
        latest_atr = df_with_indicators['atr'].iloc[-1] if 'atr' in df_with_indicators.columns and pd.notna(df_with_indicators['atr'].iloc[-1]) else None

        if pd.isna(latest_close) or pd.isna(prev_close) or \
           pd.isna(latest_bb_upper) or pd.isna(prev_bb_upper) or \
           pd.isna(latest_bb_lower) or pd.isna(prev_bb_lower):
            return SignalType.HOLD

        signal_to_return = SignalType.HOLD

        # Volume Filter Check
        volume_condition_met = True
        if self.volume_filter_enabled and 'volume' in df_with_indicators.columns and 'volume_sma' in df_with_indicators.columns:
            current_volume = df_with_indicators['volume'].iloc[-1]
            volume_sma = df_with_indicators['volume_sma'].iloc[-1]
            if pd.notna(current_volume) and pd.notna(volume_sma) and volume_sma > 0:
                if current_volume < volume_sma * self.volume_factor_above_sma:
                    volume_condition_met = False
            # else: volume condition remains true if data insufficient

        # Breakout Candle Size Filter
        breakout_candle_filter_met = True
        if self.min_breakout_atr_multiple > 0 and latest_atr is not None and latest_atr > 0:
            candle_range = abs(df_with_indicators['high'].iloc[-1] - df_with_indicators['low'].iloc[-1])
            if candle_range < self.min_breakout_atr_multiple * latest_atr:
                breakout_candle_filter_met = False

        # Long entry: Close breaks above upper band (current close vs current upper, prev close vs prev upper)
        long_breakout_condition = latest_close > latest_bb_upper and prev_close <= prev_bb_upper
        
        # Short entry: Close breaks below lower band
        short_breakout_condition = latest_close < latest_bb_lower and prev_close >= prev_bb_lower

        if self.current_position_status == SignalType.LONG:
            # Exit Long: Price closes back inside (e.g., below current upper band)
            if latest_close < latest_bb_upper: 
                signal_to_return = SignalType.CLOSE_LONG
                self.current_position_status = None
                logger.info(f"{bar_data.timestamp} - Bollinger CLOSE_LONG for {self.symbol}. Close: {latest_close:.2f}, BB_Upper: {latest_bb_upper:.2f}")
        elif self.current_position_status == SignalType.SHORT:
            # Exit Short: Price closes back inside (e.g., above current lower band)
            if latest_close > latest_bb_lower:
                signal_to_return = SignalType.CLOSE_SHORT
                self.current_position_status = None
                logger.info(f"{bar_data.timestamp} - Bollinger CLOSE_SHORT for {self.symbol}. Close: {latest_close:.2f}, BB_Lower: {latest_bb_lower:.2f}")
        else: # No current position
            if long_breakout_condition and volume_condition_met and breakout_candle_filter_met:
                signal_to_return = SignalType.LONG
                self.current_position_status = SignalType.LONG
                logger.info(f"{bar_data.timestamp} - Bollinger LONG for {self.symbol}. Close: {latest_close:.2f}, BB_Upper: {latest_bb_upper:.2f}. Vol Met: {volume_condition_met}, Candle Met: {breakout_candle_filter_met}. ATR: {bar_data.atr if bar_data.atr else 'N/A'}")
            elif short_breakout_condition and self.enable_shorting and volume_condition_met and breakout_candle_filter_met:
                signal_to_return = SignalType.SHORT
                self.current_position_status = SignalType.SHORT
                logger.info(f"{bar_data.timestamp} - Bollinger SHORT for {self.symbol}. Close: {latest_close:.2f}, BB_Lower: {latest_bb_lower:.2f}. Vol Met: {volume_condition_met}, Candle Met: {breakout_candle_filter_met}. ATR: {bar_data.atr if bar_data.atr else 'N/A'}")
                
        return signal_to_return
</code>

kamikaze_komodo/strategy_framework/strategies/ehlers_instantaneous_trendline.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/ehlers_instantaneous_trendline.py
import pandas as pd
import pandas_ta as ta # pandas_ta might not have Ehlers' IT directly, may need custom impl.
import numpy as np
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class EhlersInstantaneousTrendlineStrategy(BaseStrategy):
    """
    Implements Ehlers' Instantaneous Trendline strategy.
    This is a simplified version focusing on the trendline crossover.
    The Instantaneous Trendline is a 2-bar lookback Finite Impulse Response (FIR) filter.
    A common way to use it is with a trigger line (e.g., delayed trendline).
    Source: "Rocket Science for Traders" by John Ehlers, Chapter 3.
    Phase 6: Added shorting capability.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        # Alpha for smoothing, if desired (not part of core IT calculation directly but often used with Ehlers' indicators)
        # Default to a common value if not specified, e.g. for smoothing price before IT calc or the IT line itself.
        # However, the core IT is (Close + 2*Close[-1] + Close[-2]) / 4
        self.alpha = float(self.params.get('alpha', 0.07)) # Example if smoothing was used for other Ehlers indicators
        self.it_lag_trigger = int(self.params.get('it_lag_trigger', 1)) # Lag for the trigger line (e.g. IT lagged by 1 bar)
        
        logger.info(
            f"Initialized EhlersInstantaneousTrendlineStrategy for {symbol} ({timeframe}) "
            f"with IT Lag Trigger: {self.it_lag_trigger}. (Alpha: {self.alpha} is for general Ehlers context, not core IT). "
            f"Shorting Enabled: {self.enable_shorting}"
        )
        # self.current_position_status is inherited from BaseStrategy

    def _calculate_indicators(self, data_df: pd.DataFrame) -> pd.DataFrame:
        if data_df.empty: return data_df
        df = data_df.copy()

        if 'close' not in df.columns or len(df) < 3: # Need at least 3 bars for IT calculation
            return df

        # Ehlers' Instantaneous Trendline calculation:
        # IT_t = (Close_t + 2*Close_t-1 + Close_t-2) / 4
        close_prices = df['close']
        it_values = np.full(len(df), np.nan)
        if len(close_prices) >= 3:
            for i in range(2, len(close_prices)):
                it_values[i] = (close_prices.iloc[i] + 2 * close_prices.iloc[i-1] + close_prices.iloc[i-2]) / 4
        
        df['it'] = it_values
        df['it_trigger'] = df['it'].shift(self.it_lag_trigger) # Lagged IT as a trigger

        # ATR for BarData object (optional, but good practice)
        if all(col in df.columns for col in ['high', 'low', 'close']):
                atr_period = int(self.params.get('atr_period', 14))
                if len(df) >= atr_period:
                    df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=atr_period)
                else:
                    df['atr'] = pd.NA
        else:
            df['atr'] = pd.NA
            
        return df

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        if data.empty or len(data) < 3 + self.it_lag_trigger: # Need enough for IT and its lag
            logger.warning(f"Not enough historical data for Ehlers IT signals. Need {3 + self.it_lag_trigger}, got {len(data)}.")
            return pd.Series(index=data.index, dtype='object')

        df_processed = self._calculate_indicators(data)

        if 'it' not in df_processed.columns or 'it_trigger' not in df_processed.columns:
            return pd.Series(index=data.index, dtype='object')

        signals = pd.Series(index=df_processed.index, dtype='object').fillna(SignalType.HOLD)
        current_pos_state: Optional[SignalType] = None

        for i in range(1, len(df_processed)): # Start from 1 to compare with previous
            # Ensure all values are non-NaN for comparison
            if pd.isna(df_processed['it'].iloc[i]) or \
               pd.isna(df_processed['it_trigger'].iloc[i]) or \
               pd.isna(df_processed['it'].iloc[i-1]) or \
               pd.isna(df_processed['it_trigger'].iloc[i-1]):
                signals.iloc[i] = SignalType.HOLD
                continue

            is_bullish_cross = df_processed['it'].iloc[i] > df_processed['it_trigger'].iloc[i] and \
                                 df_processed['it'].iloc[i-1] <= df_processed['it_trigger'].iloc[i-1]
            
            is_bearish_cross = df_processed['it'].iloc[i] < df_processed['it_trigger'].iloc[i] and \
                                 df_processed['it'].iloc[i-1] >= df_processed['it_trigger'].iloc[i-1]

            if current_pos_state == SignalType.LONG:
                if is_bearish_cross:
                    signals.iloc[i] = SignalType.CLOSE_LONG
                    current_pos_state = None
                else:
                    signals.iloc[i] = SignalType.HOLD
            elif current_pos_state == SignalType.SHORT:
                if is_bullish_cross:
                    signals.iloc[i] = SignalType.CLOSE_SHORT
                    current_pos_state = None
                else:
                    signals.iloc[i] = SignalType.HOLD
            else: # No current position
                if is_bullish_cross:
                    signals.iloc[i] = SignalType.LONG
                    current_pos_state = SignalType.LONG
                elif is_bearish_cross and self.enable_shorting:
                    signals.iloc[i] = SignalType.SHORT
                    current_pos_state = SignalType.SHORT
                else:
                    signals.iloc[i] = SignalType.HOLD
        
        logger.info(f"Generated Ehlers IT signals. Longs: {signals.eq(SignalType.LONG).sum()}, Shorts: {signals.eq(SignalType.SHORT).sum()}, CloseLongs: {signals.eq(SignalType.CLOSE_LONG).sum()}, CloseShorts: {signals.eq(SignalType.CLOSE_SHORT).sum()}")
        return signals

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(bar_data)
        if len(self.data_history) < 3 + self.it_lag_trigger: # Need enough data for IT and its lag
            return SignalType.HOLD

        df_with_indicators = self._calculate_indicators(self.data_history)
        
        if 'atr' in df_with_indicators.columns and pd.notna(df_with_indicators['atr'].iloc[-1]):
                bar_data.atr = df_with_indicators['atr'].iloc[-1] # Update BarData with ATR

        if df_with_indicators.empty or 'it' not in df_with_indicators.columns or \
           'it_trigger' not in df_with_indicators.columns or len(df_with_indicators) < 2:
            return SignalType.HOLD # Not enough data or indicator calculation failed

        # Get latest and previous values for IT and its trigger
        latest_it = df_with_indicators['it'].iloc[-1]
        prev_it = df_with_indicators['it'].iloc[-2]
        latest_it_trigger = df_with_indicators['it_trigger'].iloc[-1]
        prev_it_trigger = df_with_indicators['it_trigger'].iloc[-2]

        if pd.isna(latest_it) or pd.isna(prev_it) or \
           pd.isna(latest_it_trigger) or pd.isna(prev_it_trigger):
            return SignalType.HOLD # Not enough data for signal generation

        signal_to_return = SignalType.HOLD

        is_bullish_cross = latest_it > latest_it_trigger and prev_it <= prev_it_trigger
        is_bearish_cross = latest_it < latest_it_trigger and prev_it >= prev_it_trigger

        if self.current_position_status == SignalType.LONG:
            if is_bearish_cross:
                signal_to_return = SignalType.CLOSE_LONG
                self.current_position_status = None
                logger.info(f"{bar_data.timestamp} - Ehlers IT CLOSE_LONG for {self.symbol}. IT: {latest_it:.2f}, Trigger: {latest_it_trigger:.2f}")
        elif self.current_position_status == SignalType.SHORT:
            if is_bullish_cross:
                signal_to_return = SignalType.CLOSE_SHORT
                self.current_position_status = None
                logger.info(f"{bar_data.timestamp} - Ehlers IT CLOSE_SHORT for {self.symbol}. IT: {latest_it:.2f}, Trigger: {latest_it_trigger:.2f}")
        else: # No current position
            if is_bullish_cross:
                signal_to_return = SignalType.LONG
                self.current_position_status = SignalType.LONG
                logger.info(f"{bar_data.timestamp} - Ehlers IT LONG for {self.symbol}. IT: {latest_it:.2f}, Trigger: {latest_it_trigger:.2f}. ATR: {bar_data.atr if bar_data.atr else 'N/A'}")
            elif is_bearish_cross and self.enable_shorting:
                signal_to_return = SignalType.SHORT
                self.current_position_status = SignalType.SHORT
                logger.info(f"{bar_data.timestamp} - Ehlers IT SHORT for {self.symbol}. IT: {latest_it:.2f}, Trigger: {latest_it_trigger:.2f}. ATR: {bar_data.atr if bar_data.atr else 'N/A'}")
            
        return signal_to_return
</code>

kamikaze_komodo/strategy_framework/strategies/ewmac.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/ewmac.py
import pandas as pd
import pandas_ta as ta
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger

logger = get_logger(__name__)

class EWMACStrategy(BaseStrategy):
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        # Access params using lowercase keys, as configparser.items() typically lowercases them
        self.short_window = int(self.params.get('shortwindow', 12)) # Changed to lowercase
        self.long_window = int(self.params.get('longwindow', 26))   # Changed to lowercase
        self.atr_period = int(self.params.get('atr_period', 14)) # Already lowercase in config by convention, but ensure consistency
        
        # Sentiment thresholds are often passed directly into params by main.py, preserving case.
        # If they were meant to be read from strategy's own config section, they'd also be lowercase.
        self.sentiment_filter_long_threshold = self.params.get('sentimentfilter_long_threshold') # ensure lowercase for get
        if isinstance(self.sentiment_filter_long_threshold, str):
            try:
                self.sentiment_filter_long_threshold = None if self.sentiment_filter_long_threshold.lower() == 'none' else float(self.sentiment_filter_long_threshold)
            except ValueError:
                logger.warning(f"Could not parse sentiment_filter_long_threshold '{self.params.get('sentimentfilter_long_threshold')}' to float. Defaulting to None.")
                self.sentiment_filter_long_threshold = None

        self.sentiment_filter_short_threshold = self.params.get('sentimentfilter_short_threshold') # ensure lowercase for get
        if isinstance(self.sentiment_filter_short_threshold, str):
            try:
                self.sentiment_filter_short_threshold = None if self.sentiment_filter_short_threshold.lower() == 'none' else float(self.sentiment_filter_short_threshold)
            except ValueError:
                logger.warning(f"Could not parse sentiment_filter_short_threshold '{self.params.get('sentimentfilter_short_threshold')}' to float. Defaulting to None.")
                self.sentiment_filter_short_threshold = None

        if not isinstance(self.short_window, int) or not isinstance(self.long_window, int):
            raise ValueError("EWMACStrategy: 'short_window' and 'long_window' must be integers.")
        if self.short_window >= self.long_window:
            raise ValueError("EWMACStrategy: 'short_window' must be less than 'long_window'.")

        logger.info(
            f"Initialized EWMACStrategy for {symbol} ({timeframe}) "
            f"with Short EMA: {self.short_window}, Long EMA: {self.long_window}, ATR Period: {self.atr_period}. "
            f"Sentiment Long Thresh: {self.sentiment_filter_long_threshold}, Short Thresh: {self.sentiment_filter_short_threshold}. "
            f"Shorting Enabled: {self.enable_shorting}"
        )
        # self.current_position_status is inherited from BaseStrategy

    def _calculate_indicators(self, data_df: pd.DataFrame) -> pd.DataFrame: 
        if data_df.empty: return data_df
        df = data_df.copy()

        if 'close' not in df.columns or len(df) < self.long_window :
            return df 

        df[f'ema_short'] = ta.ema(df['close'], length=self.short_window)
        df[f'ema_long'] = ta.ema(df['close'], length=self.long_window)

        if all(col in df.columns for col in ['high', 'low', 'close']):
            if len(df) >= self.atr_period:
                df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=self.atr_period)
            else:
                df['atr'] = pd.NA 
        else:
            df['atr'] = pd.NA 
        return df

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        if data.empty or len(data) < self.long_window:
            logger.warning(f"Not enough historical data for EWMAC signals. Need {self.long_window}, got {len(data)}.")
            return pd.Series(index=data.index, dtype='object')

        df_processed = self._calculate_indicators(data)

        if 'ema_short' not in df_processed.columns or 'ema_long' not in df_processed.columns:
            return pd.Series(index=data.index, dtype='object') 

        if sentiment_series is not None and not sentiment_series.empty:
            df_processed = df_processed.join(sentiment_series.rename('sentiment_score'), how='left')
            df_processed['sentiment_score'] = df_processed['sentiment_score'].fillna(0.0) 
        elif 'sentiment_score' not in df_processed.columns: 
            df_processed['sentiment_score'] = 0.0

        signals = pd.Series(index=df_processed.index, dtype='object').fillna(SignalType.HOLD)
        current_pos_state: Optional[SignalType] = None 

        for i in range(1, len(df_processed)): 
            prev_short_ema = df_processed['ema_short'].iloc[i-1]
            curr_short_ema = df_processed['ema_short'].iloc[i]
            prev_long_ema = df_processed['ema_long'].iloc[i-1]
            curr_long_ema = df_processed['ema_long'].iloc[i]
            
            current_sentiment = df_processed['sentiment_score'].iloc[i]

            if pd.isna(curr_short_ema) or pd.isna(curr_long_ema) or pd.isna(prev_short_ema) or pd.isna(prev_long_ema):
                signals.iloc[i] = SignalType.HOLD 
                continue

            is_golden_cross = curr_short_ema > curr_long_ema and prev_short_ema <= prev_long_ema
            is_death_cross = curr_short_ema < curr_long_ema and prev_short_ema >= prev_long_ema

            if current_pos_state == SignalType.LONG:
                if is_death_cross:
                    signals.iloc[i] = SignalType.CLOSE_LONG
                    current_pos_state = None
                else:
                    signals.iloc[i] = SignalType.HOLD
            elif current_pos_state == SignalType.SHORT:
                if is_golden_cross:
                    signals.iloc[i] = SignalType.CLOSE_SHORT
                    current_pos_state = None
                else:
                    signals.iloc[i] = SignalType.HOLD
            else: # No current position
                if is_golden_cross:
                    if self.sentiment_filter_long_threshold is None or current_sentiment >= self.sentiment_filter_long_threshold:
                        signals.iloc[i] = SignalType.LONG
                        current_pos_state = SignalType.LONG
                    else:
                        signals.iloc[i] = SignalType.HOLD 
                elif is_death_cross and self.enable_shorting:
                    if self.sentiment_filter_short_threshold is None or current_sentiment <= self.sentiment_filter_short_threshold:
                        signals.iloc[i] = SignalType.SHORT
                        current_pos_state = SignalType.SHORT
                    else:
                        signals.iloc[i] = SignalType.HOLD
                else: 
                    signals.iloc[i] = SignalType.HOLD
        
        logger.info(f"Generated EWMAC signals (vectorized). Longs: {signals.eq(SignalType.LONG).sum()}, Shorts: {signals.eq(SignalType.SHORT).sum()}, CloseLongs: {signals.eq(SignalType.CLOSE_LONG).sum()}, CloseShorts: {signals.eq(SignalType.CLOSE_SHORT).sum()}")
        return signals

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(bar_data) 
        if len(self.data_history) < self.long_window + 1: 
            return SignalType.HOLD

        df_with_indicators = self._calculate_indicators(self.data_history)
        
        if 'atr' in df_with_indicators.columns and pd.notna(df_with_indicators['atr'].iloc[-1]):
            bar_data.atr = df_with_indicators['atr'].iloc[-1]
        
        current_sentiment = sentiment_score if sentiment_score is not None else bar_data.sentiment_score
        if current_sentiment is None: current_sentiment = 0.0 

        if df_with_indicators.empty or 'ema_short' not in df_with_indicators.columns or \
           'ema_long' not in df_with_indicators.columns or len(df_with_indicators) < 2:
            return SignalType.HOLD

        latest_ema_short = df_with_indicators['ema_short'].iloc[-1]
        prev_ema_short = df_with_indicators['ema_short'].iloc[-2]
        latest_ema_long = df_with_indicators['ema_long'].iloc[-1]
        prev_ema_long = df_with_indicators['ema_long'].iloc[-2]

        if pd.isna(latest_ema_short) or pd.isna(prev_ema_short) or \
           pd.isna(latest_ema_long) or pd.isna(prev_ema_long):
            return SignalType.HOLD

        signal_to_return = SignalType.HOLD
        is_golden_cross = latest_ema_short > latest_ema_long and prev_ema_short <= prev_ema_long
        is_death_cross = latest_ema_short < latest_ema_long and prev_ema_short >= prev_ema_long

        if self.current_position_status == SignalType.LONG:
            if is_death_cross:
                signal_to_return = SignalType.CLOSE_LONG
                self.current_position_status = None 
                logger.info(f"{bar_data.timestamp} - EWMAC CLOSE_LONG for {self.symbol}. EMA_S: {latest_ema_short:.2f}, EMA_L: {latest_ema_long:.2f}")
        elif self.current_position_status == SignalType.SHORT:
            if is_golden_cross:
                signal_to_return = SignalType.CLOSE_SHORT
                self.current_position_status = None
                logger.info(f"{bar_data.timestamp} - EWMAC CLOSE_SHORT for {self.symbol}. EMA_S: {latest_ema_short:.2f}, EMA_L: {latest_ema_long:.2f}")
        else: # No current position
            if is_golden_cross:
                if self.sentiment_filter_long_threshold is None or current_sentiment >= self.sentiment_filter_long_threshold:
                    signal_to_return = SignalType.LONG
                    self.current_position_status = SignalType.LONG
                    logger.info(f"{bar_data.timestamp} - EWMAC LONG for {self.symbol}. Sent: {current_sentiment:.2f}. EMA_S: {latest_ema_short:.2f}, EMA_L: {latest_ema_long:.2f}. ATR: {bar_data.atr if bar_data.atr else 'N/A'}")
                else:
                    logger.info(f"{bar_data.timestamp} - EWMAC LONG for {self.symbol} SUPPRESSED by sentiment ({current_sentiment:.2f} < {self.sentiment_filter_long_threshold}).")
            elif is_death_cross and self.enable_shorting:
                if self.sentiment_filter_short_threshold is None or current_sentiment <= self.sentiment_filter_short_threshold:
                    signal_to_return = SignalType.SHORT
                    self.current_position_status = SignalType.SHORT
                    logger.info(f"{bar_data.timestamp} - EWMAC SHORT for {self.symbol}. Sent: {current_sentiment:.2f}. EMA_S: {latest_ema_short:.2f}, EMA_L: {latest_ema_long:.2f}. ATR: {bar_data.atr if bar_data.atr else 'N/A'}")
                else:
                    logger.info(f"{bar_data.timestamp} - EWMAC SHORT for {self.symbol} SUPPRESSED by sentiment ({current_sentiment:.2f} > {self.sentiment_filter_short_threshold}).")
        
        return signal_to_return
</code>

kamikaze_komodo/strategy_framework/strategies/ml_forecaster_strategy.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/ml_forecaster_strategy.py
import pandas as pd
import pandas_ta as ta # For ATR calculation
from typing import Dict, Any, Optional, Union, List
import os
import numpy as np
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.config.settings import settings as app_settings # Use app_settings
from kamikaze_komodo.ml_models.inference_pipelines.lightgbm_inference import LightGBMInference # Example
# from kamikaze_komodo.ml_models.inference_pipelines.xgboost_classifier_inference import XGBoostClassifierInference # Phase 6

logger = get_logger(__name__)

class MLForecasterStrategy(BaseStrategy):
    """
    A strategy that uses an ML price forecaster to generate trading signals.
    This example uses the LightGBMInference pipeline.
    Phase 6: Added shorting capability and option for XGBoostClassifier.
    Phase 6: Consumes market_regime from BarData.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params)
        
        # Get model configuration (e.g., path, type) from strategy params or global settings
        self.model_config_section = self.params.get('modelconfigsection', 'LightGBM_Forecaster') # Config section for model
        self.forecaster_type = self.params.get('forecastertype', 'lightgbm') # e.g., lightgbm, lstm, xgboost_classifier
        
        # Thresholds for converting prediction to signal
        self.long_threshold = float(self.params.get('longthreshold', 0.0005)) # e.g., predicted_return > 0.05%
        self.short_threshold = float(self.params.get('shortthreshold', -0.0005)) # e.g., predicted_return < -0.05%
        self.exit_long_threshold = float(self.params.get('exitlongthreshold', 0.0)) # Prediction < this to close long
        self.exit_short_threshold = float(self.params.get('exitshortthreshold', 0.0)) # Prediction > this to close short

        self.min_prediction_confidence = float(self.params.get('minpredictionconfidence', 0.0)) # Optional
        self.inference_engine = None

        # Initialize inference engine based on forecaster_type
        if self.forecaster_type.lower() == 'lightgbm':
            try:
                self.inference_engine = LightGBMInference(symbol, timeframe, model_config_section=self.model_config_section)
                if self.inference_engine.forecaster.model is None:
                    logger.error(f"MLForecasterStrategy: LightGBM model for {symbol} ({timeframe}) could not be loaded. Strategy will not generate signals.")
                    self.inference_engine = None # Disable if model not loaded
            except Exception as e:
                logger.error(f"Failed to initialize LightGBMInference for {symbol} ({timeframe}): {e}", exc_info=True)
                self.inference_engine = None
        elif self.forecaster_type.lower() == 'xgboost_classifier': # Phase 6 Example
            try:
                from kamikaze_komodo.ml_models.inference_pipelines.xgboost_classifier_inference import XGBoostClassifierInference # Attempt import
                self.inference_engine = XGBoostClassifierInference(symbol, timeframe, model_config_section=self.model_config_section)
                if self.inference_engine.forecaster.model is None:
                    logger.error(f"MLForecasterStrategy: XGBoost model for {symbol} ({timeframe}) could not be loaded.")
                    self.inference_engine = None
            except ImportError:
                logger.error(f"XGBoostClassifierInference not found. Please ensure it's implemented for forecaster_type 'xgboost_classifier'.")
                self.inference_engine = None
            except Exception as e:
                logger.error(f"Failed to initialize XGBoostClassifierInference for {symbol} ({timeframe}): {e}", exc_info=True)
                self.inference_engine = None
        else:
            logger.error(f"Unsupported forecaster_type: {self.forecaster_type} for MLForecasterStrategy.")

        logger.info(
            f"Initialized MLForecasterStrategy for {symbol} ({timeframe}) "
            f"using {self.forecaster_type}. Long Threshold: {self.long_threshold}, Short Threshold: {self.short_threshold}. "
            f"Shorting Enabled: {self.enable_shorting}"
        )
        # self.current_position_status is inherited

    def _get_prediction(self, current_history_df: pd.DataFrame) -> Optional[Any]: # Can return float or dict for classification
        """Internal method to get prediction from the inference engine."""
        if self.inference_engine:
            # The inference engine's get_prediction expects a DataFrame of historical data
            # from which features for the *latest point* will be derived.
            return self.inference_engine.get_prediction(current_history_df)
        return None

    def _calculate_indicators(self, data_df: pd.DataFrame) -> pd.DataFrame:
        """
        For MLForecasterStrategy, this might involve adding predictions as a column if doing batch processing.
        Or simply calculating ATR if needed.
        """
        df = data_df.copy()
        
        # ATR for BarData object (optional, but good practice)
        if all(col in df.columns for col in ['high', 'low', 'close']):
                atr_period = int(self.params.get('atr_period', 14)) # Default ATR period
                if len(df) >= atr_period:
                    # Ensure correct inputs for pandas_ta.atr
                    df['atr'] = ta.atr(high=df['high'], low=df['low'], close=df['close'], length=atr_period)
                else:
                    df['atr'] = pd.NA 
        else:
            df['atr'] = pd.NA
        return df

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        """
        Generates signals in batch. For ML, this would involve generating predictions for the whole series.
        """
        if data.empty or self.inference_engine is None:
            logger.warning("Data is empty or no inference engine for MLForecasterStrategy. No signals generated.")
            return pd.Series(index=data.index, dtype='object')

        df_with_indicators = self._calculate_indicators(data) # Adds ATR
        
        predictions_list = []
        prediction_confidences = [] # For classifiers that provide confidence
        min_hist_for_pred = int(self.params.get('min_bars_for_prediction', 50)) # Min bars for features
        
        for i in range(len(df_with_indicators)):
            if i < min_hist_for_pred -1 :
                predictions_list.append(None) 
                prediction_confidences.append(None)
                continue
            
            current_slice = df_with_indicators.iloc[:i+1] # History up to current bar
            pred_output = self._get_prediction(current_slice)

            if isinstance(pred_output, dict): # Classifier output
                predictions_list.append(pred_output.get('predicted_class')) 
                prediction_confidences.append(pred_output.get('confidence'))
            elif isinstance(pred_output, (float, np.floating, int, np.integer)): # Regressor or simple class output
                predictions_list.append(pred_output)
                prediction_confidences.append(None) # Confidence might not be available
            else:
                predictions_list.append(None) 
                prediction_confidences.append(None)
            
        df_with_indicators['prediction'] = predictions_list
        df_with_indicators['prediction_confidence'] = prediction_confidences
        
        if self.forecaster_type.lower() == 'lightgbm': # Assuming regression
            df_with_indicators['prediction'].fillna(0.0, inplace=True) 
        # For classification, None might mean 'hold' or 'no signal'

        signals = pd.Series(index=df_with_indicators.index, dtype='object').fillna(SignalType.HOLD)
        current_pos_state: Optional[SignalType] = None # Simulate state for batch processing

        for i in range(len(df_with_indicators)):
            prediction_value = df_with_indicators['prediction'].iloc[i]
            # prediction_confidence = df_with_indicators['prediction_confidence'].iloc[i] # Not used in this simplified batch logic
            current_sentiment = sentiment_series.iloc[i] if sentiment_series is not None and i < len(sentiment_series) else 0.0
            market_regime = df_with_indicators['market_regime'].iloc[i] if 'market_regime' in df_with_indicators.columns and pd.notna(df_with_indicators['market_regime'].iloc[i]) else None

            long_signal = False
            short_signal = False
            close_long_signal = False
            close_short_signal = False

            if self.forecaster_type.lower() == 'lightgbm': # Regression example
                if prediction_value is not None:
                    if prediction_value > self.long_threshold: long_signal = True
                    if prediction_value < self.short_threshold: short_signal = True
                    if prediction_value < self.exit_long_threshold: close_long_signal = True
                    if prediction_value > self.exit_short_threshold: close_short_signal = True
            elif self.forecaster_type.lower() == 'xgboost_classifier': # Classification example
                # Assuming prediction_value holds the class: 0=UP, 1=DOWN, 2=SIDEWAYS (example)
                if prediction_value == 0: long_signal = True
                if prediction_value == 1: short_signal = True
                if current_pos_state == SignalType.LONG and (prediction_value == 1 or prediction_value == 2) : close_long_signal = True
                if current_pos_state == SignalType.SHORT and (prediction_value == 0 or prediction_value == 2) : close_short_signal = True
            
            # Regime adaptation placeholder
            if market_regime is not None:
                 # Example: if regime is strongly bearish (e.g., regime == 2), suppress longs
                 # if market_regime == 2 and long_signal: long_signal = False 
                 pass


            # Apply signals based on state
            if current_pos_state == SignalType.LONG:
                if close_long_signal:
                    signals.iloc[i] = SignalType.CLOSE_LONG
                    current_pos_state = None
            elif current_pos_state == SignalType.SHORT:
                if close_short_signal:
                    signals.iloc[i] = SignalType.CLOSE_SHORT
                    current_pos_state = None
            else: # No position
                sentiment_ok_for_long = True
                if app_settings.enable_sentiment_analysis and self.params.get('sentimentfilter_long_threshold') is not None: 
                    if current_sentiment < self.params.get('sentimentfilter_long_threshold', -999.0): # Use strategy specific if available
                        sentiment_ok_for_long = False
                
                sentiment_ok_for_short = True
                if app_settings.enable_sentiment_analysis and self.params.get('sentimentfilter_short_threshold') is not None:
                    if current_sentiment > self.params.get('sentimentfilter_short_threshold', 999.0): # Use strategy specific if available
                        sentiment_ok_for_short = False

                if long_signal and sentiment_ok_for_long:
                    signals.iloc[i] = SignalType.LONG
                    current_pos_state = SignalType.LONG
                elif short_signal and self.enable_shorting and sentiment_ok_for_short:
                    signals.iloc[i] = SignalType.SHORT
                    current_pos_state = SignalType.SHORT
        
        logger.info(f"Generated MLForecaster signals (vectorized). Longs: {signals.eq(SignalType.LONG).sum()}, Shorts: {signals.eq(SignalType.SHORT).sum()}, CloseLongs: {signals.eq(SignalType.CLOSE_LONG).sum()}, CloseShorts: {signals.eq(SignalType.CLOSE_SHORT).sum()}")
        return signals

    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        self.update_data_history(bar_data) # Appends current bar_data to self.data_history

        atr_period = int(self.params.get('atr_period', 14))
        required_bars_for_atr = max(20, atr_period) 
        if len(self.data_history) >= required_bars_for_atr:
            history_slice_for_atr = self.data_history.tail(required_bars_for_atr * 2) # More data for stable ATR
            temp_df_for_atr = self._calculate_indicators(history_slice_for_atr) 
            if not temp_df_for_atr.empty and 'atr' in temp_df_for_atr.columns and pd.notna(temp_df_for_atr['atr'].iloc[-1]):
                bar_data.atr = temp_df_for_atr['atr'].iloc[-1]
        elif bar_data.atr is None : 
            bar_data.atr = None
            
        if self.inference_engine is None:
            return SignalType.HOLD 

        min_history_len = int(self.params.get('min_bars_for_prediction', 50)) 
        if len(self.data_history) < min_history_len:
            logger.debug(f"Not enough history ({len(self.data_history)}/{min_history_len}) for ML prediction on {self.symbol}.")
            return SignalType.HOLD

        prediction_output = self._get_prediction(self.data_history) 

        if prediction_output is None:
            logger.debug(f"No prediction received for {self.symbol} at {bar_data.timestamp}.")
            bar_data.prediction_value = None
            bar_data.prediction_confidence = None 
            return SignalType.HOLD
        
        prediction_numeric_value = 0.0 
        predicted_class_label = None # For classifier

        if isinstance(prediction_output, dict): # Classifier output
            # Example: {'predicted_class': 0, 'confidence': 0.85, 'probabilities': [0.85, 0.10, 0.05]}
            # Where 0=UP, 1=DOWN, 2=SIDEWAYS
            predicted_class_label = prediction_output.get('predicted_class')
            bar_data.prediction_confidence = prediction_output.get('confidence')
            # For thresholding, we might use confidence or a primary probability if available
            # If 'predicted_class' is what we act on, store it in prediction_value for consistency in BarData
            bar_data.prediction_value = float(predicted_class_label) if predicted_class_label is not None else None
        elif isinstance(prediction_output, (float, np.floating, int, np.integer)): # Regressor output
            prediction_numeric_value = float(prediction_output)
            bar_data.prediction_value = prediction_numeric_value
            # Confidence might not be directly available for simple regression
            bar_data.prediction_confidence = None # Or set based on other metrics if model provides
        else: 
            logger.warning(f"Unknown prediction output type: {type(prediction_output)}. Cannot process signal.")
            return SignalType.HOLD
        
        # Use market_regime if available on bar_data (populated by engine or other means)
        current_market_regime = bar_data.market_regime

        signal_to_return = SignalType.HOLD
        effective_sentiment = sentiment_score if sentiment_score is not None else (bar_data.sentiment_score if bar_data.sentiment_score is not None else 0.0)
        
        logger.debug(f"{bar_data.timestamp} - {self.symbol}: ML Pred Output={prediction_output}, Sent={effective_sentiment:.2f}, Regime={current_market_regime}")

        long_signal_triggered = False
        short_signal_triggered = False
        close_long_signal_triggered = False
        close_short_signal_triggered = False

        # --- Signal Logic based on forecaster type ---
        if self.forecaster_type.lower() == 'lightgbm': # Regression
            if prediction_numeric_value > self.long_threshold: long_signal_triggered = True
            if prediction_numeric_value < self.short_threshold: short_signal_triggered = True
            if prediction_numeric_value < self.exit_long_threshold: close_long_signal_triggered = True
            if prediction_numeric_value > self.exit_short_threshold: close_short_signal_triggered = True
        elif self.forecaster_type.lower() == 'xgboost_classifier': # Classification
            # Example: 0=UP, 1=DOWN, 2=SIDEWAYS (as defined in XGBoostClassifierForecaster)
            if predicted_class_label == 0: long_signal_triggered = True
            if predicted_class_label == 1: short_signal_triggered = True
            if self.current_position_status == SignalType.LONG and (predicted_class_label == 1 or predicted_class_label == 2):
                close_long_signal_triggered = True
            if self.current_position_status == SignalType.SHORT and (predicted_class_label == 0 or predicted_class_label == 2):
                close_short_signal_triggered = True
        
        # Regime-based adaptation (example)
        if current_market_regime is not None:
            # Example: if current_market_regime indicates very high volatility (e.g., regime 2), reduce exposure or avoid trades
            # if current_market_regime == 2: # Assuming 2 is 'high volatility / avoid'
            # long_signal_triggered = False
            # short_signal_triggered = False
            # logger.info(f"ML signals suppressed due to market regime {current_market_regime}")
            pass # Implement specific regime logic here

        # Check min prediction confidence if available
        if bar_data.prediction_confidence is not None and bar_data.prediction_confidence < self.min_prediction_confidence:
            logger.info(f"{bar_data.timestamp} - ML signal for {self.symbol} ({'LONG' if long_signal_triggered else 'SHORT' if short_signal_triggered else 'N/A'}) suppressed due to low confidence ({bar_data.prediction_confidence:.2f} < {self.min_prediction_confidence}).")
            long_signal_triggered = False
            short_signal_triggered = False


        # --- Position Management ---
        if self.current_position_status == SignalType.LONG:
            if close_long_signal_triggered:
                signal_to_return = SignalType.CLOSE_LONG
                self.current_position_status = None
                logger.info(f"{bar_data.timestamp} - MLForecaster CLOSE_LONG for {self.symbol}. Prediction: {prediction_output}")
        elif self.current_position_status == SignalType.SHORT:
            if close_short_signal_triggered:
                signal_to_return = SignalType.CLOSE_SHORT
                self.current_position_status = None
                logger.info(f"{bar_data.timestamp} - MLForecaster CLOSE_SHORT for {self.symbol}. Prediction: {prediction_output}")
        else: # No current position
            if long_signal_triggered:
                sentiment_ok_for_long = True
                # Use strategy-specific sentiment thresholds if they exist in self.params, otherwise global from app_settings
                strat_sentiment_long_thresh = self.params.get('sentimentfilter_long_threshold', app_settings.sentiment_filter_threshold_long if app_settings else None)
                if app_settings and app_settings.enable_sentiment_analysis and strat_sentiment_long_thresh is not None:
                    if effective_sentiment < strat_sentiment_long_thresh:
                        logger.info(f"{bar_data.timestamp} - MLForecaster LONG for {self.symbol} SUPPRESSED by sentiment ({effective_sentiment:.2f} < {strat_sentiment_long_thresh}). Prediction: {prediction_output}")
                        sentiment_ok_for_long = False
                
                if sentiment_ok_for_long:
                    signal_to_return = SignalType.LONG
                    self.current_position_status = SignalType.LONG
                    logger.info(f"{bar_data.timestamp} - MLForecaster LONG for {self.symbol}. Prediction: {prediction_output}. ATR: {bar_data.atr if bar_data.atr is not None else 'N/A'}")

            elif short_signal_triggered and self.enable_shorting:
                sentiment_ok_for_short = True
                strat_sentiment_short_thresh = self.params.get('sentimentfilter_short_threshold', app_settings.sentiment_filter_threshold_short if app_settings else None)
                if app_settings and app_settings.enable_sentiment_analysis and strat_sentiment_short_thresh is not None:
                    if effective_sentiment > strat_sentiment_short_thresh:
                        logger.info(f"{bar_data.timestamp} - MLForecaster SHORT for {self.symbol} SUPPRESSED by sentiment ({effective_sentiment:.2f} > {strat_sentiment_short_thresh}). Prediction: {prediction_output}")
                        sentiment_ok_for_short = False

                if sentiment_ok_for_short:
                    signal_to_return = SignalType.SHORT
                    self.current_position_status = SignalType.SHORT
                    logger.info(f"{bar_data.timestamp} - MLForecaster SHORT for {self.symbol}. Prediction: {prediction_output}. ATR: {bar_data.atr if bar_data.atr is not None else 'N/A'}")
        
        return signal_to_return
</code>

kamikaze_komodo/strategy_framework/strategies/pair_trading_strategy.py:
<code>
# FILE: kamikaze_komodo/strategy_framework/strategies/pair_trading_strategy.py
import pandas as pd
import pandas_ta as ta
import statsmodels.api as sm # For cointegration test
from statsmodels.tsa.stattools import adfuller # For stationarity test on spread
from typing import Dict, Any, Optional, Union, List
from kamikaze_komodo.strategy_framework.base_strategy import BaseStrategy, SignalCommand
from kamikaze_komodo.core.enums import SignalType, OrderSide
from kamikaze_komodo.core.models import BarData
from kamikaze_komodo.app_logger import get_logger
from kamikaze_komodo.data_handling.data_fetcher import DataFetcher # For fetching secondary asset data
from kamikaze_komodo.config.settings import settings as app_settings
from datetime import datetime, timedelta, timezone

logger = get_logger(__name__)

class PairTradingStrategy(BaseStrategy):
    """
    Implements a Pair Trading strategy based on cointegration.
    The 'symbol' parameter in __init__ will be considered asset1.
    Asset2 symbol must be provided in params.
    """
    def __init__(self, symbol: str, timeframe: str, params: Optional[Dict[str, Any]] = None):
        super().__init__(symbol, timeframe, params) # symbol is asset1
        
        self.asset1_symbol = symbol
        self.asset2_symbol = self.params.get('asset2_symbol')
        if not self.asset2_symbol:
            raise ValueError("PairTradingStrategy requires 'asset2_symbol' in params.")

        self.cointegration_lookback_days = int(self.params.get('cointegration_lookback_days', 90))
        self.cointegration_test_pvalue_threshold = float(self.params.get('cointegration_test_pvalue_threshold', 0.05))
        self.spread_zscore_entry_threshold = float(self.params.get('spread_zscore_entry_threshold', 2.0))
        self.spread_zscore_exit_threshold = float(self.params.get('spread_zscore_exit_threshold', 0.5))
        self.spread_calculation_window = int(self.params.get('spread_calculation_window', 20)) # For MA and StdDev of spread

        self.data_history_asset2 = pd.DataFrame(columns=['open', 'high', 'low', 'close', 'volume', 'atr'])
        self.is_cointegrated = False
        self.hedge_ratio: Optional[float] = None # From cointegration regression

        # Internal state for an active pair trade
        self.active_pair_trade_leg1_symbol: Optional[str] = None
        self.active_pair_trade_leg2_symbol: Optional[str] = None
        self.active_pair_trade_direction: Optional[str] = None # "long_spread" or "short_spread"

        logger.info(
            f"Initialized PairTradingStrategy for {self.asset1_symbol} / {self.asset2_symbol} ({timeframe}) "
            f"Cointegration Lookback: {self.cointegration_lookback_days} days, p-value: {self.cointegration_test_pvalue_threshold}. "
            f"Z-Score Entry: {self.spread_zscore_entry_threshold}, Exit: {self.spread_zscore_exit_threshold}. "
            f"Spread Window: {self.spread_calculation_window}. Shorting Enabled: {self.enable_shorting}"
        )
        # Note: self.enable_shorting must be true for pair trading to function correctly.
        if not self.enable_shorting:
            logger.warning(f"PairTradingStrategy for {self.asset1_symbol}/{self.asset2_symbol} has enable_shorting=False. This strategy requires shorting for one leg.")


    async def initialize_strategy_data(self, historical_data_asset1: pd.DataFrame, historical_data_asset2: pd.DataFrame):
        """
        Checks for cointegration using pre-fetched historical data.
        This method is called once at the start by the runner.
        """
        if historical_data_asset1.empty or historical_data_asset2.empty:
            logger.error("PairTradingStrategy received empty historical data for one or both assets.")
            self.is_cointegrated = False
            return

        # Store full history for later indicator calculation on individual assets
        self.data_history = historical_data_asset1.copy()
        self.data_history_asset2 = historical_data_asset2.copy()
        
        # FIX: Merge based on the index since 'timestamp' was set as the index.
        merged_df = pd.merge(
            self.data_history[['close']], 
            self.data_history_asset2[['close']], 
            left_index=True, 
            right_index=True, 
            how='inner', 
            suffixes=('_asset1', '_asset2')
        )
        merged_df.dropna(inplace=True)

        if len(merged_df) < self.spread_calculation_window * 2: # Need enough data
            logger.warning(f"Not enough synchronized historical data for {self.asset1_symbol} and {self.asset2_symbol} for cointegration analysis (found {len(merged_df)} bars).")
            self.is_cointegrated = False
            return
        
        # Check for cointegration using Engle-Granger
        close_asset1 = merged_df['close_asset1']
        close_asset2 = merged_df['close_asset2']

        # OLS regression: asset1 = hedge_ratio * asset2 + const
        model = sm.OLS(close_asset1, sm.add_constant(close_asset2, prepend=True))
        results = model.fit()
        self.hedge_ratio = results.params[1]

        spread = close_asset1 - self.hedge_ratio * close_asset2
        
        # ADF test on the spread to check for stationarity
        adf_result = adfuller(spread.dropna())
        p_value = adf_result[1]

        if p_value < self.cointegration_test_pvalue_threshold:
            self.is_cointegrated = True
            logger.info(f"Pair {self.asset1_symbol}/{self.asset2_symbol} IS cointegrated. ADF p-value: {p_value:.4f}, Hedge Ratio: {self.hedge_ratio:.4f}")
        else:
            self.is_cointegrated = False
            self.hedge_ratio = None # Reset if not cointegrated
            logger.info(f"Pair {self.asset1_symbol}/{self.asset2_symbol} is NOT cointegrated. ADF p-value: {p_value:.4f}")
        
        return


    def _calculate_spread_zscore(self) -> Optional[float]:
        """Calculates the Z-score of the current spread."""
        if self.hedge_ratio is None or len(self.data_history) < self.spread_calculation_window or len(self.data_history_asset2) < self.spread_calculation_window:
            return None
            
        # Ensure both histories have the latest timestamp available
        last_ts_asset1 = self.data_history.index[-1]
        if last_ts_asset1 not in self.data_history_asset2.index:
            logger.debug(f"Latest timestamp {last_ts_asset1} for {self.asset1_symbol} not in {self.asset2_symbol} history. Cannot calculate current spread.")
            return None
            
        close1 = self.data_history['close'].loc[last_ts_asset1]
        close2 = self.data_history_asset2['close'].loc[last_ts_asset1]

        if pd.isna(close1) or pd.isna(close2): return None

        # Calculate historical spread for Z-score
        hist_close1 = self.data_history['close']
        hist_close2 = self.data_history_asset2['close']
        
        merged_closes = pd.merge(hist_close1.rename('c1'), hist_close2.rename('c2'), left_index=True, right_index=True, how='inner')
        if len(merged_closes) < self.spread_calculation_window: return None

        historical_spread = merged_closes['c1'] - self.hedge_ratio * merged_closes['c2']
        
        if len(historical_spread) < self.spread_calculation_window:
            return None
            
        spread_mean = historical_spread.rolling(window=self.spread_calculation_window).mean().iloc[-1]
        spread_std = historical_spread.rolling(window=self.spread_calculation_window).std().iloc[-1]

        if pd.isna(spread_mean) or pd.isna(spread_std) or spread_std == 0:
            return None
            
        current_spread = close1 - self.hedge_ratio * close2
        z_score = (current_spread - spread_mean) / spread_std
        logger.debug(f"Current Spread for {self.asset1_symbol}/{self.asset2_symbol}: {current_spread:.4f}, Mean: {spread_mean:.4f}, Std: {spread_std:.4f}, Z-Score: {z_score:.2f}")
        return z_score

    def generate_signals(self, data: pd.DataFrame, sentiment_series: Optional[pd.Series] = None) -> pd.Series:
        # Pair trading decisions are made bar-by-bar based on spread Z-score.
        # This batch method is less suitable. Primary logic will be in on_bar_data.
        logger.warning("generate_signals is not the primary method for PairTradingStrategy; logic is in on_bar_data.")
        return pd.Series(index=data.index, dtype='object').fillna(SignalType.HOLD)


    def on_bar_data(self, bar_data: BarData, sentiment_score: Optional[float] = None, market_regime_data: Optional[Any] = None) -> Union[Optional[SignalType], List[SignalCommand]]:
        # This strategy relies on the backtest engine to manage and align data for both assets.
        # It updates its internal history for asset1 (self.symbol) when its bar data arrives.
        # The backtest engine must provide the history for asset2 via its `data_feed_df_pair_asset2` parameter.
        if bar_data.symbol == self.asset1_symbol:
            self.update_data_history(bar_data)
        else:
            # This strategy instance should only process data for its primary symbol.
            # The engine is responsible for passing the right data.
            return SignalType.HOLD

        if not self.is_cointegrated or self.hedge_ratio is None:
            return SignalType.HOLD

        current_z_score = self._calculate_spread_zscore()
        if current_z_score is None:
            return SignalType.HOLD

        signals_to_execute: List[SignalCommand] = []

        # Exit logic first
        if self.current_position_status == SignalType.LONG: # Means we are long the spread (Long Asset1, Short Asset2)
            if current_z_score >= self.spread_zscore_exit_threshold:
                logger.info(f"Exiting LONG SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                # Backtest engine needs to get price for asset2 to close
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_LONG, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_SHORT, symbol=self.asset2_symbol))
                self.current_position_status = None
                return signals_to_execute

        elif self.current_position_status == SignalType.SHORT: # Means we are short the spread (Short Asset1, Long Asset2)
            if current_z_score <= -self.spread_zscore_exit_threshold:
                logger.info(f"Exiting SHORT SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_SHORT, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.CLOSE_LONG, symbol=self.asset2_symbol))
                self.current_position_status = None
                return signals_to_execute
        
        # Entry logic if no active pair trade
        if self.current_position_status is None:
            # Entry condition: Long the spread (Asset1 Long, Asset2 Short) if Z-score is very low
            if current_z_score < -self.spread_zscore_entry_threshold:
                logger.info(f"Entering LONG SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                signals_to_execute.append(SignalCommand(signal_type=SignalType.LONG, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.SHORT, symbol=self.asset2_symbol))
                self.current_position_status = SignalType.LONG # Representing "long the spread"
                return signals_to_execute

            # Entry condition: Short the spread (Asset1 Short, Asset2 Long) if Z-score is very high
            elif current_z_score > self.spread_zscore_entry_threshold:
                logger.info(f"Entering SHORT SPREAD trade for {self.asset1_symbol}/{self.asset2_symbol}. Z-Score: {current_z_score:.2f}")
                signals_to_execute.append(SignalCommand(signal_type=SignalType.SHORT, symbol=self.asset1_symbol))
                signals_to_execute.append(SignalCommand(signal_type=SignalType.LONG, symbol=self.asset2_symbol))
                self.current_position_status = SignalType.SHORT # Representing "short the spread"
                return signals_to_execute

        return SignalType.HOLD
</code>

kamikaze_komodo/strategy_framework/strategies/__init__.py:
<code>
# kamikaze_komodo/strategy_framework/strategies/__init__.py
# This file makes the 'strategies' directory a Python package.
</code>

